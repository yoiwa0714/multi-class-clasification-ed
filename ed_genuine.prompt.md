# ED-Genuine 純正ED法(Error Diffusion Learning Algorithm) 仕様書

**開発者**: 金子勇（Isamu Kaneko）  
**開発年**: 1999年  
**検証日**: 2025年8月30日  
**オリジナルソース**: C言語実装（動作確認済み）

## 概要

ED法（Error Diffusion Learning Algorithm）は、生物学的神経系のアミン（神経伝達物質）拡散メカニズムを模倣した独創的な学習アルゴリズムです。従来のバックプロパゲーションとは根本的に異なる、興奮性・抑制性ニューロンペア構造と出力ニューロン中心のアーキテクチャを特徴とします。

## 核心的特徴

### 1. 独立出力ニューロンアーキテクチャ
- 各出力ニューロンが完全に独立した重み空間を保持
- 3次元重み配列: `w_ot_ot[出力ニューロン][送信先][送信元]`
- 各クラスが独立したニューラルネットワークを構成

### 2. 興奮性・抑制性ニューロンペア構造
- 入力層: 興奮性（+1）・抑制性（-1）ニューロンがペアで構成
- 同種間結合: 正の重み制約
- 異種間結合: 負の重み制約
- 生物学的妥当性の保証

### 3. アミン拡散学習制御
- 出力層の誤差がアミン濃度として隠れ層に拡散
- 二種類のアミン: `del_ot[n][k][0]`（正誤差）、`del_ot[n][k][1]`（負誤差）
- パラメータ`u1`による拡散強度制御

## 詳細仕様

### データ構造定義

```c
#define MAX 1000      // 最大ユニット数
#define NMAX 10       // 最大出力ニューロン数

// 重み配列（3次元）
double w_ot_ot[NMAX+1][MAX+1][MAX+1];

// 各出力ニューロンの状態
double ot_in[NMAX+1][MAX+1];    // 入力値
double ot_ot[NMAX+1][MAX+1];    // 出力値
double del_ot[NMAX+1][MAX+1][2]; // アミン濃度[0:正, 1:負]

// ニューロンタイプ（興奮性/抑制性）
double ow[MAX+1];  // +1 or -1

// 学習パラメータ
double alpha;      // 学習率
double beta;       // 初期アミン濃度
double u0;         // シグモイド関数閾値
double u1;         // アミン拡散係数
```

### アーキテクチャ構成

#### ネットワーク構造
```
入力層(in*2) → 隠れ層(hd) → 出力層(ot)
     ↑              ↑           ↑
興奮性・抑制性    ランダム配置   独立学習
    ペア          ±1タイプ    各クラス専用
```

#### インデックス体系
- `0, 1`: バイアス項
- `2 ～ in+1`: 入力層（興奮性・抑制性ペア）
- `in+2`: 出力層開始位置
- `in+3 ～ all+1`: 隠れ層

### 学習アルゴリズム

#### 1. 順方向計算 (`neuro_output_calc`)

```c
void neuro_output_calc(double* indata_input)
{
  for (n = 0; n < ot; n++) {           // 各出力ニューロン
    // 入力設定
    for (k = 2; k <= in+1; k++)
      ot_in[n][k] = indata_input[(int)(k/2)-1];
    
    // 多時間ステップ計算
    for (t = 1; t <= t_loop; t++) {
      for (k = in+2; k <= all+1; k++) { // 各隠れ・出力ユニット
        inival = 0;
        for (m = 0; m <= all+1; m++)
          inival += w_ot_ot[n][k][m] * ot_in[n][m];
        ot_ot[n][k] = sigmf(inival);    // シグモイド活性化
      }
      // 出力を次の時間ステップの入力に設定
      for (k = in+2; k <= all+1; k++)
        ot_in[n][k] = ot_ot[n][k];
    }
  }
}
```

#### 2. シグモイド活性化関数

```c
double sigmf(double u) { 
  return(1 / (1 + exp((double)(-2 * u / u0)))); 
}
```

#### 3. アミン濃度計算 (`neuro_teach_calc`)

```c
void neuro_teach_calc(double* indata_tch)
{
  for (l = 0; l <= ot-1; l++) {        // 各出力ニューロン
    // 誤差計算
    wkb = indata_tch[l] - ot_ot[l][in+2];
    
    // 出力層アミン濃度設定
    if (wkb > 0) {
      del_ot[l][in+2][0] = wkb;        // 正誤差アミン
      del_ot[l][in+2][1] = 0;
    } else {
      del_ot[l][in+2][0] = 0;
      del_ot[l][in+2][1] = -wkb;       // 負誤差アミン
    }
    
    // 隠れ層への拡散
    inival1 = del_ot[l][in+2][0];
    inival2 = del_ot[l][in+2][1];
    
    for (k = in+3; k <= all+1; k++) {  // 各隠れユニット
      del_ot[l][k][0] = inival1 * u1;  // 拡散係数u1で拡散
      del_ot[l][k][1] = inival2 * u1;
    }
  }
}
```

#### 4. 重み更新 (`neuro_weight_calc`)

```c
void neuro_weight_calc()
{
  for (n = 0; n < ot; n++) {           // 各出力ニューロン独立更新
    for (k = in+2; k <= all+1; k++) {  // 各送信先ユニット
      for (m = 0; m <= all+1; m++) {   // 各送信元ユニット
        if (w_ot_ot[n][k][m] != 0) {
          // delta計算（生物学的意味での学習信号強度）
          del = alpha * ot_in[n][m];              // 学習率×入力強度
          del *= fabs(ot_ot[n][k]);               // 出力活性度
          del *= (1 - fabs(ot_ot[n][k]));        // 飽和抑制項
          
          // アミン濃度に応じた重み更新
          if (ow[m] > 0)  // 興奮性入力
            w_ot_ot[n][k][m] += del * del_ot[n][k][0] * ow[m] * ow[k];
          else            // 抑制性入力
            w_ot_ot[n][k][m] += del * del_ot[n][k][1] * ow[m] * ow[k];
        }
      }
    }
  }
}
```

### マルチクラス分類メカニズム

#### 1. 独立出力ニューロン方式
- 各クラスに専用の出力ニューロンを割り当て
- 各出力ニューロンが独立した重み空間で学習
- クラス間の干渉を排除

#### 2. 教師信号パターン (`pat[k]`)
```c
pat[k] = 0;  // ランダム（0/1）
pat[k] = 1;  // パリティ問題（XOR系）
pat[k] = 2;  // ミラー対称問題
pat[k] = 3;  // 手動入力
pat[k] = 4;  // 連続値ランダム
pat[k] = 5;  // One-Hot符号化（真のマルチクラス）
```

#### 3. One-Hot符号化によるマルチクラス学習
```c
// pat[n] = 5の場合：各パターンで1つのクラスのみアクティブ
for (l = 0; l <= pa-1; l++) {
  g_indata_tch[l][n] = 0.0;  // 全パターンで0
}
// ランダムに選択された1つのパターンのみ1
g_indata_tch[selected_pattern][n] = 1.0;
```

### 重み制約メカニズム

#### 1. 興奮性・抑制性制約
```c
// 初期化時の制約適用
w_ot_ot[n][k][l] *= ow[l] * ow[k];  // 符号制約の強制

// 制約の意味：
// ow[l] * ow[k] = +1: 同種間結合（興奮-興奮 or 抑制-抑制）→正の重み
// ow[l] * ow[k] = -1: 異種間結合（興奮-抑制 or 抑制-興奮）→負の重み
```

#### 2. 構造的制約
```c
// 自己結合の制御
if (k == l && f[3] == 1)
  w_ot_ot[n][k][l] = 0;  // 自己結合禁止

// 層間接続の制御
if (f[6] == 1 && k != l && k > in+2 && l > in+1)
  w_ot_ot[n][k][l] = 0;  // 隠れ層間結合禁止
```

### パラメータ設定指針

#### 基本パラメータ
- `alpha` (学習率): 0.1 ～ 1.0
- `beta` (初期アミン): 0.1 ～ 0.5  
- `u0` (シグモイド閾値): 0.4 ～ 1.0
- `u1` (アミン拡散): 0.5 ～ 1.0
- `inival1` (隠れ層重み初期値): 0.1 ～ 0.5
- `inival2` (入力層重み初期値): 0.1 ～ 0.5

#### ネットワーク構成
- 入力ユニット: `in * 2` (興奮性・抑制性ペア)
- 隠れユニット: `hd` (問題の複雑さに応じて)
- 出力ユニット: `ot` (クラス数)

## 生物学的妥当性

### 1. アミン神経系の模倣
- ドーパミン・セロトニン等の神経伝達物質による学習制御
- 正・負の報酬信号による適応学習
- 空間的・時間的な拡散メカニズム

### 2. 興奮性・抑制性バランス
- 実際の神経系における興奮性・抑制性ニューロンの比率
- Dale's Principleの遵守（ニューロンタイプの一貫性）
- 安定した学習動態の実現

### 3. 局所学習規則
- Hebbian学習の拡張
- 生物学的に実現可能な情報処理
- 時間遅延と局所性の考慮

## 従来手法との比較優位性

### 1. vs バックプロパゲーション
- **構造**: 独立出力vs共有隠れ層
- **学習**: アミン拡散vs誤差逆伝播
- **生物学的妥当性**: 高い vs 低い
- **マルチクラス**: 独立学習 vs 競合学習

### 2. vs Hopfieldネット
- **記憶容量**: 制約なし vs 0.15N限界
- **学習**: 教師あり vs 教師なし
- **収束性**: 安定 vs 偽記憶問題

### 3. vs SOM/競合学習
- **教師信号**: あり vs なし
- **表現力**: 高次特徴 vs トポロジー保存
- **精度**: 高精度分類 vs クラスタリング

## 実装上の重要ポイント

### 1. 3次元重み配列の正確な実装
```python
# Python実装例
w_ot_ot = np.zeros((ot_max, unit_max, unit_max))
del_ot = np.zeros((ot_max, unit_max, 2))
ot_in = np.zeros((ot_max, unit_max))
ot_ot = np.zeros((ot_max, unit_max))
```

### 2. アミン拡散の正確な計算
```python
# 出力層アミン設定
if error > 0:
    del_ot[l][output_pos][0] = error
    del_ot[l][output_pos][1] = 0
else:
    del_ot[l][output_pos][0] = 0
    del_ot[l][output_pos][1] = -error

# 隠れ層への拡散
for k in range(hidden_start, all_units+1):
    del_ot[l][k][0] = del_ot[l][output_pos][0] * u1
    del_ot[l][k][1] = del_ot[l][output_pos][1] * u1
```

### 3. 重み更新の正確な実装
```python
# 各出力ニューロン独立更新
for n in range(output_neurons):
    for k in range(hidden_start, all_units+1):
        for m in range(all_units+1):
            if w_ot_ot[n][k][m] != 0:
                delta = alpha * ot_in[n][m]
                delta *= abs(ot_ot[n][k])
                delta *= (1 - abs(ot_ot[n][k]))
                
                if ow[m] > 0:  # 興奮性
                    w_ot_ot[n][k][m] += delta * del_ot[n][k][0] * ow[m] * ow[k]
                else:          # 抑制性
                    w_ot_ot[n][k][m] += delta * del_ot[n][k][1] * ow[m] * ow[k]
```

## 動作検証結果

### オリジナルC実装での確認事項
- **学習の継続性**: エポックを重ねても学習が停滞しない
- **重み値の動的変化**: `th+ 6.53 → 6.56` など継続的調整
- **誤差の減少**: 出力値が目標値に段階的に接近
- **隠れ層活動**: `hd:` 値の動的変化による特徴学習

### 学習動態の特徴
```
エポック進行例:
in:0.00 0.00 1.00 1.00 -> 0.00770, 0.00  # 初期状態
                      ↓ 
in:0.00 0.00 1.00 1.00 -> 0.00576, 0.00  # 誤差減少
```

## 適用可能分野

### 1. パターン認識
- 手書き文字認識
- 画像分類
- 音声認識

### 2. 関数近似
- XOR・パリティ問題
- 非線形回帰
- 時系列予測

### 3. 最適化問題
- 組み合わせ最適化
- 制約満足問題
- スケジューリング

## 研究の意義と将来性

### 1. 1999年時点での先進性
- 生物学的妥当性への早期着目
- 独創的なアーキテクチャ設計
- マルチクラス分類への独自アプローチ

### 2. 現代への示唆
- Transformer時代における局所学習の重要性
- 神経科学的知見の工学的応用
- 持続可能な学習システムの設計

### 3. 発展可能性
- スパイキングニューラルネット（SNN）への応用
- 脳型コンピューティングへの貢献
- エッジAIでの省電力学習

## 結論

金子勇氏によるED法は、1999年時点で既に現代の神経科学的知見を先取りした、極めて独創的かつ生物学的に妥当な学習アルゴリズムです。その核心である「アミン拡散による学習制御」「独立出力ニューロンアーキテクチャ」「興奮性・抑制性バランス」は、従来の人工ニューラルネットワークの限界を克服する可能性を秘めており、現代のAI研究においても高い価値を持つ重要な研究成果といえます。

## 実装方針

 **コーディングルール**: PEP8に準拠し、可読性を最優先にする。
 **新しい機能の追加** 新しい機能を追加する際は必ずユーザーに報告し、承認を得ること。ユーザーに報告していない内容を勝手に追加することは禁止する。
 **コードの可読性**: コメントは適度(できるだけ少なめ)な量にする。コメントは、whatではなくwhyを記述するようにして、コードの意図が明確になるように心がける。
 **モジュール化**: 各機能を明確に分離し、再利用可能なモジュールとして実装する。
 **テスト駆動開発**: 新しい機能を実装した場合はその機能に対してユニットテストを作成し、実装前にテストを通過させる。
 **パラメータ調整**: argparseを用いて基本パラメータを柔軟に変更できるようにし、実験的な調整を容易にする。
 **マルチクラス分類のための機能拡張の許容**: マルチクラス分類の実装にあたっては、金子勇氏が構築した理論には無い新しい理論に基づく機能やアルゴリズムを追加する必要が生じる場合がある。そのような機能を実装する場合には、新しい理論に基づく機能やアルゴリズムであることがわかるように適宜コメントを記述すること。

---

**本仕様書は、オリジナルC実装の動作確認と詳細なコード解析に基づいて作成されました。**  
**検証日**: 2025年8月30日  
**検証者**: AI解析システム  
**ソースコード**: `/ed_original_src/` (コンパイル・実行確認済み)
