"""
ED-Genuine MNISTå°‚ç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¯ãƒ©ã‚¹
Isamu Kaneko's Error Diffusion Learning Algorithm implementation
Based on C code pat[5]: One-Hot encoding for multi-class classification

ã€ed_genuine.prompt.md æº–æ‹ å®Ÿè£…ã€‘
-            visualizer = RealtimeLe            # æ··åŒè¡Œåˆ—å¯è¦–åŒ–ï¼ˆè¨“ç·´é–‹å§‹æ™‚ç‚¹ã‹ã‚‰è¡¨ç¤ºï¼‰
            confusion_visualizer = RealtimeConfusionMatrixVisualizer(
                num_classes=10, window_size=(800, 600), save_dir=getattr(self.hyperparams, 'save_fig', None))
            confusion_visualizer.setup_plots()
                          # ğŸ¯ ed_multi.prompt.mdæº–æ‹ ï¼šæ··åŒè¡Œåˆ—è¡¨ç¤ºï¼ˆã‚¨ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—å®Œäº†å¾Œã«ç§»å‹•ï¼‰
                    # ç”»é¢å´©ã‚Œã‚’é˜²ããŸã‚ã€ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ãƒãƒ¼æ›´æ–°å¾Œã«è¡¨ç¤º
                    # if not getattr(self.hyperparams, 'quiet_mode', False) and not enable_visualization and epoch == epochs - 1:
                    #     # æ–‡å­—ãƒ™ãƒ¼ã‚¹è¡¨ç¤ºã§ã®ã¿å®Ÿè¡Œï¼ˆã‚°ãƒ©ãƒ•è¡¨ç¤ºæ™‚ã¯å­¦ç¿’å®Œäº†å¾Œã®ã¿ï¼‰
                    #     results_buffer.display_confusion_matrix_single_epoch('test', epoch) 
            # ğŸ¯ ed_multi.prompt.mdæº–æ‹ ï¼šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒœãƒƒã‚¯ã‚¹è¡¨ç¤ºç”¨ãƒ‡ãƒ¼ã‚¿è¨­å®š
            ed_params = {
                'learning_rate': self.hyperparams.learning_rate,
                'amine': self.hyperparams.amine,
                'diffusion': self.hyperparams.diffusion,
                'sigmoid': self.hyperparams.sigmoid,
                'weight1': self.hyperparams.weight1,
                'weight2': self.hyperparams.weight2
            }
            exec_params = {
                'train_samples': self.hyperparams.train_samples,
                'test_samples': self.hyperparams.test_samples,
                'epochs': self.hyperparams.epochs,
                'hidden': str(self.hyperparams.hidden),
                'batch_size': self.hyperparams.batch_size,
                'seed': getattr(self.hyperparams, 'seed', 'Random')
            }
            # ãƒ‡ãƒãƒƒã‚°: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ç¢ºèª
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚° - EDæ³•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {ed_params}")
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚° - å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {exec_params}")
            confusion_visualizer.set_parameters(ed_params, exec_params)
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³å¯è¦–åŒ–ï¼ˆv0.2.4æ–°æ©Ÿèƒ½ï¼‰
            # ed_multi.prompt.mdæº–æ‹ : --vizã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã¯å­¦ç¿’æ›²ç·šãƒ»æ··åŒè¡Œåˆ—ãƒ»ç²¾åº¦æ¨ç§»ã®ã¿è¡¨ç¤º
            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å¯è¦–åŒ–ã¯ç„¡åŠ¹åŒ–
                'learning_rate': self.hyperparams.learning_rate,
                'amine': self.hyperparams.amine,
                'diffusion': self.hyperparams.diffusion,
                'sigmoid': self.hyperparams.sigmoid,
                'weight1': self.hyperparams.weight1,
                'weight2': self.hyperparams.weight2
            }
            exec_params = {
                'train_samples': self.hyperparams.train_samples,
                'test_samples': self.hyperparams.test_samples,
                'epochs': self.hyperparams.epochs,
                'hidden': str(self.hyperparams.hidden),
                'batch_size': self.hyperparams.batch_size,
                'seed': getattr(self.hyperparams, 'seed', 'Random')
            }
            # ãƒ‡ãƒãƒƒã‚°: ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ç¢ºèª
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚° - EDæ³•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {ed_params}")
            print(f"ğŸ” ãƒ‡ãƒãƒƒã‚° - å®Ÿè¡Œãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {exec_params}")
            confusion_visualizer.set_parameters(ed_params, exec_params)
            
            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³å¯è¦–åŒ–ï¼ˆv0.2.4æ–°æ©Ÿèƒ½ï¼‰
            # ed_multi.prompt.mdæº–æ‹ : --vizã‚ªãƒ—ã‚·ãƒ§ãƒ³ã§ã¯å­¦ç¿’æ›²ç·šãƒ»æ··åŒè¡Œåˆ—ãƒ»ç²¾åº¦æ¨ç§»ã®ã¿è¡¨ç¤º
            # ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³å¯è¦–åŒ–ã¯ç„¡åŠ¹åŒ–ion-MNIST ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå°‚ç”¨æ‹¡å¼µ
- ç‹¬ç«‹å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ 
- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶å¾¡å¯¾å¿œ
- ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–æ©Ÿèƒ½
"""

import numpy as np
import time
import threading
from typing import Optional, Tuple, Dict, Any

# å¤–éƒ¨ä¾å­˜ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
try:
    import matplotlib.pyplot as plt
    from tqdm import tqdm
    HAS_VISUALIZATION = True
except ImportError:
    HAS_VISUALIZATION = False

# ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«å†…ä¾å­˜
from .ed_core import EDGenuine


class EDNetworkMNIST(EDGenuine):
    """
    MNISTç”¨EDæ³•ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ‹¡å¼µã‚¯ãƒ©ã‚¹
    """
    
    def __init__(self, hyperparams):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—å¯¾å¿œï¼‰"""
        super().__init__(hyperparams)
        self.heatmap_callback = None  # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºç”¨ã®ç¾åœ¨ã®ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±
        self.current_sample_info = {
            'epoch': 0,
            'sample_idx': 0,
            'true_label': -1,
            'predicted_label': -1,
            'pattern_idx': 0,
            'input_data': None  # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜ã—ã¦åŒæœŸã‚’ç¢ºä¿
        }
        
        # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ æ›´æ–°é–“éš”ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ•°å˜ä½ï¼‰
        self.heatmap_update_interval = 5  # 5ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯ã«æ›´æ–°ã§ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
    
    def set_heatmap_callback(self, callback):
        """ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šï¼ˆEDHeatmapIntegrationé€£æºç”¨ï¼‰ - ed_multi.prompt.mdæº–æ‹ """
        self.heatmap_callback = callback
        print("âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã«ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯è¨­å®šå®Œäº†")
    
    def update_current_sample_info(self, epoch, sample_idx, true_label, predicted_label=None, pattern_idx=None, input_data=None):
        """ç¾åœ¨ã®ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã‚’æ›´æ–°ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºç”¨ãƒ»å…¥åŠ›ãƒ‡ãƒ¼ã‚¿åŒæœŸå¯¾å¿œï¼‰"""
        self.current_sample_info.update({
            'epoch': epoch,
            'sample_idx': sample_idx,
            'true_label': true_label,
            'predicted_label': predicted_label if predicted_label is not None else -1,
            'pattern_idx': pattern_idx if pattern_idx is not None else sample_idx,
            'input_data': input_data  # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—ã¨ã‚µãƒ–ã‚¿ã‚¤ãƒˆãƒ«åŒæœŸã®ãŸã‚
        })
    
    def get_current_sample_info(self):
        """ç¾åœ¨ã®ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã‚’å–å¾—ï¼ˆãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—è¡¨ç¤ºç”¨ï¼‰"""
        return self.current_sample_info.copy()
    
    def run_classification(self, train_size=None, test_size=None, epochs=None, random_state=42, 
                          enable_visualization=None, use_fashion_mnist=None):
        """
        MNIST/Fashion-MNISTåˆ†é¡å­¦ç¿’ã®å®Ÿè¡Œ (ed_multi.prompt.mdæº–æ‹  - å¤šå±¤å¯¾å¿œç‰ˆ)
        
        Args:
            train_size: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆNoneã®å ´åˆhyperparamsã‹ã‚‰å–å¾—ï¼‰
            test_size: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆNoneã®å ´åˆhyperparamsã‹ã‚‰å–å¾—ï¼‰
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆNoneã®å ´åˆhyperparamsã‹ã‚‰å–å¾—ï¼‰
            random_state: ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰
            enable_visualization: å¯è¦–åŒ–æœ‰åŠ¹/ç„¡åŠ¹ï¼ˆNoneã®å ´åˆhyperparamsã‹ã‚‰å–å¾—ï¼‰
            use_fashion_mnist: Fashion-MNISTä½¿ç”¨ãƒ•ãƒ©ã‚°ï¼ˆNoneã®å ´åˆhyperparamsã‹ã‚‰å–å¾—ï¼‰
        Returns:
            dict: çµæœçµ±è¨ˆ
        """
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰å€¤ã‚’å–å¾—ï¼ˆå®‰å…¨ãªã‚¢ã‚¯ã‚»ã‚¹ï¼‰
        train_size = train_size or getattr(self.hyperparams, 'train_samples', 1000)
        test_size = test_size or getattr(self.hyperparams, 'test_samples', 200)
        epochs = epochs or getattr(self.hyperparams, 'epochs', 10)
        if enable_visualization is None:
            # ğŸ†• --save_figã‚ªãƒ—ã‚·ãƒ§ãƒ³æŒ‡å®šæ™‚ã‚‚å¯è¦–åŒ–ã‚’æœ‰åŠ¹ã«ã™ã‚‹ (ed_multi.prompt.mdæº–æ‹ )
            viz_enabled = getattr(self.hyperparams, 'enable_visualization', False)
            save_fig_enabled = getattr(self.hyperparams, 'save_fig', None) is not None
            fig_path_enabled = getattr(self.hyperparams, 'fig_path', None) is not None
            enable_visualization = viz_enabled or save_fig_enabled or fig_path_enabled
        if use_fashion_mnist is None:
            use_fashion_mnist = getattr(self.hyperparams, 'fashion_mnist', False)

        dataset_name = "Fashion-MNIST" if use_fashion_mnist else "MNIST"
        print("=" * 60)
        print(f"{dataset_name}åˆ†é¡å­¦ç¿’ é–‹å§‹ - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œç‰ˆ")
        print("=" * 60)

        # ğŸ”§ ã€é‡è¦ä¿®æ­£ã€‘ed_multi.prompt.mdæº–æ‹ : ã‚¨ãƒãƒƒã‚¯ç·æ•°ã‚’è€ƒæ…®ã—ãŸé©åˆ‡ãªãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºè¨ˆç®—
        # å„ã‚¨ãƒãƒƒã‚¯ã§ç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€ç·ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã¯ã‚¨ãƒãƒƒã‚¯æ•°Ã—å„ã‚¨ãƒãƒƒã‚¯ã‚µã‚¤ã‚º
        total_train_needed = train_size * epochs
        total_test_needed = test_size * epochs

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ï¼ˆã‚¨ãƒãƒƒã‚¯æ•°ã‚’è€ƒæ…®ã—ãŸç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«å–å¾—ï¼‰
        # ed_multi.prompt.mdæº–æ‹ : è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆä¸¡æ–¹ã§ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
        train_inputs, train_labels, test_inputs, test_labels = self.load_dataset(
            train_size=total_train_needed, test_size=total_test_needed, use_fashion_mnist=use_fashion_mnist, total_epochs=epochs)

        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–ï¼ˆ10ã‚¯ãƒ©ã‚¹åˆ†é¡ç”¨ï¼‰ - å¤šå±¤å¯¾å¿œ
        hidden_layers = getattr(self.hyperparams, 'hidden_layers', [100])

        if len(hidden_layers) == 1:
            # å˜å±¤ã®å ´åˆ
            hidden_size = hidden_layers[0]
            hidden2_size = 0
            print(f"ğŸ“Š å˜å±¤æ§‹æˆ: éš ã‚Œå±¤{hidden_size}ãƒ¦ãƒ‹ãƒƒãƒˆ")
        else:
            # å¤šå±¤ã®å ´åˆ: ç·éš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆæ•°ã‚’è¨ˆç®—
            hidden_size = sum(hidden_layers)
            hidden2_size = 0  # ed_multi.prompt.mdæº–æ‹ ã§ã¯ hidden2_size ã¯ä½¿ç”¨ã—ãªã„
            print(f"ğŸ“Š å¤šå±¤æ§‹æˆ: éš ã‚Œå±¤{'â†’'.join(map(str, hidden_layers))} (ç·è¨ˆ{hidden_size}ãƒ¦ãƒ‹ãƒƒãƒˆ)")

        self.neuro_init(
            input_size=784,  # 28x28 MNIST/Fashion-MNIST
            num_outputs=10,  # 10ã‚¯ãƒ©ã‚¹
            hidden_size=hidden_size,  # å˜å±¤/å¤šå±¤ã«å¿œã˜ãŸéš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆæ•°
            hidden2_size=hidden2_size  # éš ã‚Œå±¤2ã¯ä½¿ç”¨ã—ãªã„
        )

        print(f"\nğŸ“Š ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆ:")
        print(f"  å…¥åŠ›å±¤: 784æ¬¡å…ƒ (28x28ç”»åƒ)")
        print(f"  ä¸­é–“å±¤: {hidden_size}ãƒ¦ãƒ‹ãƒƒãƒˆ {'(å¤šå±¤åˆè¨ˆ)' if len(hidden_layers) > 1 else '(å˜å±¤)'}")
        if len(hidden_layers) > 1:
            print(f"  å¤šå±¤è©³ç´°: {'â†’'.join(map(str, hidden_layers))}")
        print(f"  å‡ºåŠ›å±¤: 10ã‚¯ãƒ©ã‚¹")
        print(f"  ç·ãƒ¦ãƒ‹ãƒƒãƒˆ: {self.total_units}")

        # ğŸ¯ ed_multi.prompt.mdæº–æ‹ : å‹•çš„ãƒ¡ãƒ¢ãƒªç®¡ç†ä¸‹ã§ã¯é…åˆ—ã‚µã‚¤ã‚ºãƒã‚§ãƒƒã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
        # å®Ÿéš›ã®å­¦ç¿’å‡¦ç†ã¯`train_epoch_with_minibatch`ã§å®‰å…¨ã«å®Ÿè¡Œã•ã‚Œã‚‹
        # è­¦å‘Šã‚’å‡ºã•ãšã«å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™
        self.num_patterns = len(train_inputs)

        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®šï¼ˆå‹•çš„ãƒ¡ãƒ¢ãƒªç®¡ç†å¯¾å¿œï¼‰
        for i, inp in enumerate(train_inputs):
            # ğŸ¯ ä¿®æ­£: å‹•çš„ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ä¸‹ã§ã¯ç¯„å›²ãƒã‚§ãƒƒã‚¯ã‚’æœ€é©åŒ–
            if i < len(self.input_data):
                inp_flat = inp.flatten().astype(float)
                for j, val in enumerate(inp_flat):
                    if j < len(self.input_data[i]):
                        self.input_data[i][j] = val

        # æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã‚’è¨­å®šï¼ˆ10ã‚¯ãƒ©ã‚¹åˆ†é¡ï¼‰
        for i, label in enumerate(train_labels):
            # ğŸ¯ ä¿®æ­£: å‹•çš„ãƒ¡ãƒ¢ãƒªç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ä¸‹ã§ã¯ç¯„å›²ãƒã‚§ãƒƒã‚¯ã‚’æœ€é©åŒ–
            if i < len(self.teacher_data):
                # å…¨ã¦ã®ã‚¯ãƒ©ã‚¹å‡ºåŠ›ã‚’0.0ã«åˆæœŸåŒ–
                for c in range(10):
                    if c < len(self.teacher_data[i]):
                        self.teacher_data[i][c] = 0.0
                
                # æ­£è§£ã‚¯ãƒ©ã‚¹ã®ã¿1.0ã«è¨­å®šï¼ˆOne-Hotå½¢å¼ï¼‰
                true_class = int(label)
                if true_class < len(self.teacher_data[i]):
                    self.teacher_data[i][true_class] = 1.0

        print(f"âœ… ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†: è¨“ç·´{len(train_inputs)}ã‚µãƒ³ãƒ—ãƒ«, ãƒ†ã‚¹ãƒˆ{len(test_inputs)}ã‚µãƒ³ãƒ—ãƒ«")

    def _extract_actual_neuron_activities(self):
        """
        EDæ³•ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã‚‰å®Ÿéš›ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ´»å‹•ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
        ed_multi.prompt.mdæº–æ‹ ã®æ­£ç¢ºãªãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³çŠ¶æ…‹å–å¾—
        
        Returns:
            List[np.ndarray]: å„å±¤ã®ç™ºç«ãƒ‡ãƒ¼ã‚¿ [å…¥åŠ›å±¤, éš ã‚Œå±¤, å‡ºåŠ›å±¤]
        """
        try:
            # EDæ³•ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç¢ºèª
            if not hasattr(self, 'ed_genuine') or self.ed_genuine is None:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿
                return [np.zeros(784), np.zeros(128), np.zeros(10)]
            
            # EDæ³•ã®å®Ÿéš›ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³çŠ¶æ…‹é…åˆ—ã‹ã‚‰æŠ½å‡º
            layer_activities = []
            
            # å…¥åŠ›å±¤æ´»å‹•ï¼ˆ784ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼‰
            # output_inputsé…åˆ—ã®æœ€åˆã®å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‹ã‚‰å…¥åŠ›å±¤ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º
            if hasattr(self.ed_genuine, 'output_inputs') and self.ed_genuine.output_inputs is not None:
                # å…¥åŠ›å±¤ã¯2-785ç•ªç›®ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆ0,1ã¯ãƒã‚¤ã‚¢ã‚¹ã€2-785ã¯784å€‹ã®å…¥åŠ›ï¼‰
                input_range_start = 2
                input_range_end = input_range_start + 784
                input_activity = []
                
                # ç¬¬0å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å…¥åŠ›å±¤éƒ¨åˆ†ã‚’æŠ½å‡ºï¼ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¾Œã®å€¤ã‚’ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¤‰æ›ï¼‰
                for i in range(input_range_start, min(input_range_end, self.ed_genuine.output_inputs.shape[1])):
                    neuron_value = self.ed_genuine.output_inputs[0][i] if i < self.ed_genuine.output_inputs.shape[1] else 0.0
                    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å€¤ã‚’ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¤‰æ›ï¼ˆé–¾å€¤0.5ï¼‰
                    firing_rate = 1.0 if neuron_value > 0.5 else 0.0
                    input_activity.append(firing_rate)
                
                # 784å€‹ã«èª¿æ•´
                while len(input_activity) < 784:
                    input_activity.append(0.0)
                input_activity = input_activity[:784]
            else:
                input_activity = np.zeros(784)
            
            layer_activities.append(np.array(input_activity))
            
            # éš ã‚Œå±¤æ´»å‹•ï¼ˆ128ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼‰
            if hasattr(self.ed_genuine, 'output_inputs') and self.ed_genuine.output_inputs is not None:
                # éš ã‚Œå±¤ã¯786-913ç•ªç›®ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆå…¥åŠ›784+ãƒã‚¤ã‚¢ã‚¹2ã®å¾Œï¼‰
                hidden_range_start = 2 + 784  # 786
                hidden_range_end = hidden_range_start + 128  # 914
                hidden_activity = []
                
                for i in range(hidden_range_start, min(hidden_range_end, self.ed_genuine.output_inputs.shape[1])):
                    neuron_value = self.ed_genuine.output_inputs[0][i] if i < self.ed_genuine.output_inputs.shape[1] else 0.0
                    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å€¤ã‚’ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¤‰æ›ï¼ˆéš ã‚Œå±¤ç”¨é–¾å€¤0.3ï¼‰
                    firing_rate = min(1.0, max(0.0, neuron_value)) if neuron_value > 0.3 else 0.0
                    hidden_activity.append(firing_rate)
                
                # 128å€‹ã«èª¿æ•´
                while len(hidden_activity) < 128:
                    hidden_activity.append(0.0)
                hidden_activity = hidden_activity[:128]
            else:
                hidden_activity = np.zeros(128)
                
            layer_activities.append(np.array(hidden_activity))
            
            # å‡ºåŠ›å±¤æ´»å‹•ï¼ˆ10ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ï¼‰
            if hasattr(self.ed_genuine, 'output_outputs') and self.ed_genuine.output_outputs is not None:
                output_activity = []
                
                # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ´»å‹•ã‚’å–å¾—
                for n in range(min(10, self.ed_genuine.output_outputs.shape[0])):
                    # å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æœ€çµ‚å€¤ï¼ˆåˆ†é¡å‡ºåŠ›ï¼‰
                    output_value = self.ed_genuine.output_outputs[n][0] if self.ed_genuine.output_outputs.shape[1] > 0 else 0.0
                    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å€¤ã‚’ç™ºç«ãƒ‘ã‚¿ãƒ¼ãƒ³ã«å¤‰æ›ï¼ˆå‡ºåŠ›å±¤ç”¨é–¾å€¤0.1ï¼‰
                    firing_rate = min(1.0, max(0.0, output_value))
                    output_activity.append(firing_rate)
                
                # 10å€‹ã«èª¿æ•´
                while len(output_activity) < 10:
                    output_activity.append(0.0)
                output_activity = output_activity[:10]
            else:
                output_activity = np.zeros(10)
                
            layer_activities.append(np.array(output_activity))
            
            return layer_activities
            
        except Exception as e:
            print(f"âš ï¸ ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ´»å‹•æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã¯ã‚¼ãƒ­ãƒ™ã‚¯ãƒˆãƒ«ã‚’è¿”ã™
            return [np.zeros(784), np.zeros(128), np.zeros(10)]
    
    def _display_data_usage_statistics(self):
        """å­¦ç¿’å®Œäº†å¾Œã«MNISTãƒ‡ãƒ¼ã‚¿ä½¿ç”¨çµ±è¨ˆã‚’è¡¨ç¤º"""
        try:
            print(f"\n{'='*60}")
            print("ğŸ“Š MNISTãƒ‡ãƒ¼ã‚¿ä½¿ç”¨çµ±è¨ˆ")
            print(f"{'='*60}")
            
            # åŸºæœ¬çš„ãªãƒ‡ãƒ¼ã‚¿æƒ…å ±
            if hasattr(self, 'ed_genuine') and hasattr(self.ed_genuine, 'train_original_indices'):
                train_indices = self.ed_genuine.train_original_indices
                test_indices = self.ed_genuine.test_original_indices
                
                print(f"åŸºæœ¬ãƒ‡ãƒ¼ã‚¿æƒ…å ±:")
                print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°: {len(train_indices)}")
                print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°: {len(test_indices)}")
                print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²: [{train_indices.min()}, {train_indices.max()}]")
                print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²: [{test_indices.min()}, {test_indices.max()}]")
                
                # ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿åˆ†æ
                unique_train = np.unique(train_indices)
                unique_test = np.unique(test_indices)
                print(f"  è¨“ç·´ãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿æ•°: {len(unique_train)}")
                print(f"  ãƒ†ã‚¹ãƒˆãƒ¦ãƒ‹ãƒ¼ã‚¯ãƒ‡ãƒ¼ã‚¿æ•°: {len(unique_test)}")
                
                # é‡è¤‡ä½¿ç”¨ã®ç¢ºèª
                unique_train_indices, train_counts = np.unique(train_indices, return_counts=True)
                duplicates = train_counts[train_counts > 1]
                if len(duplicates) > 0:
                    print(f"  âš ï¸ è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡è¤‡: {len(duplicates)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ãŒè¤‡æ•°å›ä½¿ç”¨")
                    print(f"  æœ€å¤§ä½¿ç”¨å›æ•°: {duplicates.max()}")
                else:
                    print(f"  âœ… è¨“ç·´ãƒ‡ãƒ¼ã‚¿é‡è¤‡ãªã—: å…¨ãƒ‡ãƒ¼ã‚¿ãŒ1å›ãšã¤ä½¿ç”¨")
                
                # è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆé–“ã®é‡è¤‡ç¢ºèª
                overlap = np.intersect1d(train_indices, test_indices)
                if len(overlap) > 0:
                    print(f"  âš ï¸ è­¦å‘Š: è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆé–“ã§{len(overlap)}å€‹ã®ãƒ‡ãƒ¼ã‚¿ãŒé‡è¤‡")
                    print(f"  é‡è¤‡ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä¾‹: {overlap[:5]}")
                else:
                    print(f"  âœ… è¨“ç·´ãƒ»ãƒ†ã‚¹ãƒˆé–“ã®é‡è¤‡ãªã—")
            
            # ãƒŸãƒ‹ãƒãƒƒãƒãƒ­ãƒ¼ãƒ€ãƒ¼ã‹ã‚‰ã®è©³ç´°çµ±è¨ˆ
            if hasattr(self, '_last_train_loader'):
                train_loader = self._last_train_loader
                if train_loader and train_loader.track_usage:
                    usage_stats = train_loader.get_usage_statistics()
                    if usage_stats:
                        print(f"\nâœ… MNISTãƒ‡ãƒ¼ã‚¿ä½¿ç”¨çµ±è¨ˆï¼ˆç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«è¿½è·¡ï¼‰:")
                        print(f"  ç·ä½¿ç”¨ãƒ‡ãƒ¼ã‚¿æ•°: {usage_stats['total_data']}")
                        print(f"  æœ€å¤§ä½¿ç”¨å›æ•°: {usage_stats['max_usage']}")
                        print(f"  æœ€å°ä½¿ç”¨å›æ•°: {usage_stats['min_usage']}")
                        print(f"  å¹³å‡ä½¿ç”¨å›æ•°: {usage_stats['avg_usage']:.2f}")
                        
                        # ä½¿ç”¨å›æ•°åˆ†å¸ƒã‚’è¡¨ç¤º
                        usage_dist = {}
                        for count in usage_stats['usage_distribution'].values():
                            usage_dist[count] = usage_dist.get(count, 0) + 1
                        
                        print(f"  ä½¿ç”¨å›æ•°åˆ†å¸ƒ:")
                        for count, num_data in sorted(usage_dist.items()):
                            print(f"    {count}å›ä½¿ç”¨: {num_data}å€‹ã®ãƒ‡ãƒ¼ã‚¿")
                            
                        # ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨ã®æ¤œè¨¼
                        # ã‚¨ãƒãƒƒã‚¯æ•°ã‚’è€ƒæ…®ã—ãŸæ­£ã—ã„åˆ¤å®š
                        expected_usage_per_data = 1  # ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ãªã‚‰å„ãƒ‡ãƒ¼ã‚¿ã¯1å›ã®ã¿ä½¿ç”¨
                        if usage_stats['max_usage'] == expected_usage_per_data and usage_stats['min_usage'] == expected_usage_per_data:
                            print(f"  âœ… å®Œå…¨ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: å…¨{usage_stats['total_data']}ã‚µãƒ³ãƒ—ãƒ«ãŒ{expected_usage_per_data}å›ãšã¤ä½¿ç”¨")
                        else:
                            # ã‚¨ãƒãƒƒã‚¯æ•°ã¨ä¸€è‡´ã™ã‚‹å ´åˆã¯æ­£å¸¸ï¼ˆã‚¨ãƒãƒƒã‚¯æ¯ã«ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼‰
                            actual_epochs = getattr(self.hyperparams, 'epochs', 1)
                            if usage_stats['max_usage'] == actual_epochs and usage_stats['min_usage'] == actual_epochs:
                                print(f"  âœ… ã‚¨ãƒãƒƒã‚¯åˆ†å‰²ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨: å„ã‚¨ãƒãƒƒã‚¯ã§ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ï¼ˆ{actual_epochs}ã‚¨ãƒãƒƒã‚¯å¯¾å¿œï¼‰")
                            else:
                                print(f"  âš ï¸ ãƒ‡ãƒ¼ã‚¿é‡è¤‡æ¤œå‡º: ä¸€éƒ¨ãƒ‡ãƒ¼ã‚¿ãŒè¤‡æ•°å›ä½¿ç”¨ã•ã‚Œã¦ã„ã¾ã™")
                    else:
                        print(f"\nâš ï¸ MNISTãƒ‡ãƒ¼ã‚¿ä½¿ç”¨çµ±è¨ˆ: çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãªã—")
                else:
                    print(f"\nâš ï¸ MNISTãƒ‡ãƒ¼ã‚¿ä½¿ç”¨çµ±è¨ˆ: è¿½è·¡ç„¡åŠ¹")
            else:
                print(f"\nâš ï¸ MNISTãƒ‡ãƒ¼ã‚¿ä½¿ç”¨çµ±è¨ˆ: ãƒ­ãƒ¼ãƒ€ãƒ¼æƒ…å ±ãªã—")
            
            print(f"{'='*60}")
                
        except Exception as e:
            if not getattr(self.hyperparams, 'quiet_mode', False):
                print(f"âš ï¸ ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨çµ±è¨ˆã®è¡¨ç¤ºä¸­ã«ã‚¨ãƒ©ãƒ¼: {e}")

