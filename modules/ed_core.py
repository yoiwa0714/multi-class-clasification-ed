"""
ED-Genuine æ ¸å¿ƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…
é‡‘å­å‹‡æ°ã®Error Diffusion Learning Algorithm Cå®Ÿè£… pat[5] æº–æ‹ 

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™:
- class EDGenuine: EDæ³•ã®æ ¸å¿ƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…
- ã‚¢ãƒŸãƒ³æ‹¡æ•£ã«ã‚ˆã‚‹å­¦ç¿’åˆ¶å¾¡
- èˆˆå¥®æ€§ãƒ»æŠ‘åˆ¶æ€§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒšã‚¢æ§‹é€ 
- ç‹¬ç«‹å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
"""

import numpy as np
import time
import math
import random
from tqdm import tqdm
from .data_loader import MiniBatchDataLoader
from typing import Optional, Tuple, Dict, Any, List

# GPUæ©Ÿèƒ½ãƒã‚§ãƒƒã‚¯
try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    cp = None

# TORCHVISION ãƒã‚§ãƒƒã‚¯
try:
    import torch
    import torchvision
    import torchvision.transforms as transforms
    TORCHVISION_AVAILABLE = True
except ImportError:
    TORCHVISION_AVAILABLE = False
    torch = None
    torchvision = None
    transforms = None



class EDGenuine:
    """ç´”æ­£EDæ³•å®Ÿè£… - é‡‘å­å‹‡æ°ã®Cå®Ÿè£…å®Œå…¨å†ç¾ç‰ˆï¼ˆPEP8æº–æ‹ å¤‰æ•°åï¼‰"""
    
    # Cè¨€èªå®šæ•°ã®å®Œå…¨å†ç¾ - MNISTå¯¾å¿œæ‹¡å¼µ
    MAX_UNITS = 2000              # MNISTå¯¾å¿œã®ãŸã‚æ‹¡å¼µ
    MAX_OUTPUT_NEURONS = 10
    
    def __init__(self, hyperparams=None):
        """EDæ³•ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ– - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œç‰ˆ"""
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤å›é¿ï¼‰
        if hyperparams is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç›´æ¥è¨­å®šï¼ˆHyperParamsã‚¯ãƒ©ã‚¹ä¾å­˜ã‚’å›é¿ï¼‰
            class DefaultParams:
                learning_rate = 0.8
                initial_amine = 0.8
                diffusion_rate = 1.0
                sigmoid_threshold = 0.4
                initial_weight_1 = 1.0
                initial_weight_2 = 1.0
                enable_profiling = False
                verbose = False
                force_cpu = False
                hidden_neurons = 64
            hyperparams = DefaultParams()
        
        self.hyperparams = hyperparams
        
        # è¨“ç·´æ™‚é–“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆæœŸåŒ–ï¼ˆv0.1.7æ–°æ©Ÿèƒ½ï¼‰
        # TrainingProfilerä¾å­˜ã‚’å›é¿ - ç°¡æ˜“å®Ÿè£…
        class SimpleProfiler:
            def __init__(self, enable_profiling=False):
                self.enable_profiling = enable_profiling
            def start_timer(self, name): pass
            def end_timer(self, name): pass
            def complete_sample(self): pass
            def get_statistics(self): return {}
            def print_detailed_report(self): pass
        
        self.profiler = SimpleProfiler(enable_profiling=hyperparams.enable_profiling)
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.input_units = 0      # å…¥åŠ›ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ï¼ˆå®Ÿéš›ã¯*2ã§ãƒšã‚¢æ§‹é€ ï¼‰
        self.output_units = 0     # å‡ºåŠ›ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        self.hidden_units = 0     # éš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        self.hidden2_units = 0    # éš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆ2æ•°
        self.total_units = 0      # å…¨ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        self.num_patterns = 0     # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
        
        # GPUåŸºç›¤æ©Ÿèƒ½ï¼ˆPhase GPU-1ï¼‰
        self.gpu_available = GPU_AVAILABLE
        self.gpu_enabled = False
        self.gpu_device_info = None
        
        if self.gpu_available:
            self._initialize_gpu_environment()
        else:
            print("ğŸ’» CPUå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã§åˆæœŸåŒ–")
        
        # Cå®Ÿè£…ã®é…åˆ—ã‚’å®Œå…¨å†ç¾ï¼ˆPEP8æº–æ‹ åï¼‰
        self.output_weights = np.zeros((self.MAX_OUTPUT_NEURONS+1, self.MAX_UNITS+1, self.MAX_UNITS+1), dtype=np.float64)  # [å‡ºåŠ›][é€ä¿¡å…ˆ][é€ä¿¡å…ƒ]
        self.output_inputs = np.zeros((self.MAX_OUTPUT_NEURONS+1, self.MAX_UNITS+1), dtype=np.float64)    # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å…¥åŠ›
        self.output_outputs = np.zeros((self.MAX_OUTPUT_NEURONS+1, self.MAX_UNITS+1), dtype=np.float64)   # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å‡ºåŠ›
        self.amine_concentrations = np.zeros((self.MAX_OUTPUT_NEURONS+1, self.MAX_UNITS+1, 2), dtype=np.float64)  # ã‚¢ãƒŸãƒ³æ¿ƒåº¦[å‡ºåŠ›][ãƒ¦ãƒ‹ãƒƒãƒˆ][æ­£/è² ]
        self.excitatory_inhibitory = np.zeros(self.MAX_UNITS+1, dtype=np.float64)  # èˆˆå¥®æ€§/æŠ‘åˆ¶æ€§ãƒ•ãƒ©ã‚°
        
        # å…¥åŠ›ãƒ»æ•™å¸«ãƒ‡ãƒ¼ã‚¿
        self.input_data = np.zeros((self.MAX_UNITS+1, self.MAX_UNITS+1), dtype=np.float64)
        self.teacher_data = np.zeros((self.MAX_UNITS+1, self.MAX_UNITS+1), dtype=np.float64)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ï¼‰
        self.learning_rate = self.hyperparams.learning_rate         # å­¦ç¿’ç‡ (alpha)
        self.initial_amine = self.hyperparams.initial_amine         # åˆæœŸã‚¢ãƒŸãƒ³æ¿ƒåº¦ (beta)
        self.sigmoid_threshold = self.hyperparams.sigmoid_threshold # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¾å€¤ (u0)
        self.diffusion_rate = self.hyperparams.diffusion_rate       # ã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•° (u1)
        self.initial_weight_1 = self.hyperparams.initial_weight_1   # é‡ã¿åˆæœŸå€¤1
        self.initial_weight_2 = self.hyperparams.initial_weight_2   # é‡ã¿åˆæœŸå€¤2
        self.time_loops = 2          # æ™‚é–“ãƒ«ãƒ¼ãƒ—æ•°ï¼ˆå›ºå®šï¼‰ = 1          # æ™‚é–“ãƒ«ãƒ¼ãƒ—æ•°ï¼ˆæ€§èƒ½æœ€é©åŒ–: 2â†’1ï¼‰ = 2          # æ™‚é–“ãƒ«ãƒ¼ãƒ—æ•°ï¼ˆå›ºå®šï¼‰
        
        # ãƒ•ãƒ©ã‚°é…åˆ—ï¼ˆæ©Ÿèƒ½çš„ãªåå‰ã§ä¿æŒï¼‰
        self.flags = [0] * 15
        self.flags[3] = 1          # è‡ªå·±çµåˆã‚«ãƒƒãƒˆãƒ•ãƒ©ã‚°
        self.flags[6] = 1          # ãƒ«ãƒ¼ãƒ—ã‚«ãƒƒãƒˆãƒ•ãƒ©ã‚°
        self.flags[7] = 1          # å¤šå±¤ãƒ•ãƒ©ã‚°
        self.flags[10] = 0         # é‡ã¿æ¸›è¡°ãƒ•ãƒ©ã‚°
        self.flags[11] = 1         # è² å…¥åŠ›ãƒ•ãƒ©ã‚°
        
        # çµ±è¨ˆæƒ…å ±
        self.error = 0.0
        self.error_count = 0
        self.pattern_types = [0] * (self.MAX_UNITS+1)  # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—
        
        # äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆPhase 1æœ€é©åŒ–ï¼‰
        self.weight_indices_cache = {}
        self.cache_initialized = False
        
        print("âœ… ç´”æ­£EDæ³•åˆæœŸåŒ–å®Œäº†")
    
    def _initialize_gpu_environment(self):
        """
        GPUç’°å¢ƒã®åˆæœŸåŒ–ã¨æ¤œè¨¼ï¼ˆPhase GPU-1ï¼‰- CuPy 13.6.0å¯¾å¿œ
        é‡‘å­å‹‡æ°ã‚ªãƒªã‚¸ãƒŠãƒ«ä»•æ§˜ã¸ã®å½±éŸ¿ã‚’æœ€å°é™ã«æŠ‘åˆ¶
        --cpuã‚ªãƒ—ã‚·ãƒ§ãƒ³å¯¾å¿œ: CPUå¼·åˆ¶å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¶å¾¡
        """
        # CPUå¼·åˆ¶å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        if self.hyperparams.force_cpu:
            print("ğŸ–¥ï¸ CPUå¼·åˆ¶å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: GPUå‡¦ç†ã‚’ç„¡åŠ¹åŒ–")
            self.gpu_available = False
            return
            
        try:
            # GPU ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—
            device = cp.cuda.Device(0)
            device.use()
            
            # GPUãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ï¼ˆCuPy 13.6.0å¯¾å¿œï¼‰
            mem_info = cp.cuda.runtime.memGetInfo()
            free_bytes = mem_info[0]
            total_bytes = mem_info[1]
            
            # ãƒ‡ãƒã‚¤ã‚¹å±æ€§å–å¾—ï¼ˆCuPy 13.6.0å¯¾å¿œï¼‰
            attributes = device.attributes
            device_name = f"GPU Device {device.id}"
            if 'Name' in attributes:
                device_name = attributes['Name']
            
            self.gpu_device_info = {
                'device_id': device.id,
                'device_name': device_name,
                'total_memory_gb': total_bytes / (1024**3),
                'free_memory_gb': free_bytes / (1024**3),
                'compute_capability': device.compute_capability,
                'max_threads_per_block': attributes.get('MaxThreadsPerBlock', 'N/A'),
                'multiprocessor_count': attributes.get('MultiProcessorCount', 'N/A')
            }
            
            print(f"ğŸ”‹ GPUåˆæœŸåŒ–æˆåŠŸ: {self.gpu_device_info['device_name']}")
            print(f"   ãƒ¡ãƒ¢ãƒª: {self.gpu_device_info['free_memory_gb']:.1f}GB / {self.gpu_device_info['total_memory_gb']:.1f}GB")
            print(f"   Compute Capability: {self.gpu_device_info['compute_capability']}")
            print(f"   ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ãƒƒã‚µ: {self.gpu_device_info['multiprocessor_count']}")
            
        except Exception as e:
            print(f"âš ï¸ GPUåˆæœŸåŒ–å¤±æ•—: {e}")
            self.gpu_available = False
    
    def enable_gpu_acceleration(self, enable: bool = True):
        """
        GPUé«˜é€ŸåŒ–ã®æœ‰åŠ¹/ç„¡åŠ¹åˆ‡ã‚Šæ›¿ãˆ
        å®‰å…¨æ€§å„ªå…ˆï¼šGPUå¤±æ•—æ™‚ã¯è‡ªå‹•çš„ã«CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        if not self.gpu_available:
            print("âŒ GPUåˆ©ç”¨ä¸å¯ã®ãŸã‚ã€CPUç‰ˆã§ç¶™ç¶š")
            return False
        
        if enable:
            try:
                # GPUç’°å¢ƒãƒ†ã‚¹ãƒˆ
                test_array = cp.array([1.0, 2.0, 3.0])
                test_result = cp.sum(test_array)
                cp.cuda.Stream.null.synchronize()
                
                self.gpu_enabled = True
                print("âœ… GPUé«˜é€ŸåŒ–ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹ï¼ˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Œå…¨æ€§ä¿æŒï¼‰")
                return True
                
            except Exception as e:
                print(f"âš ï¸ GPUæœ‰åŠ¹åŒ–å¤±æ•—ã€CPUç‰ˆã§ç¶™ç¶š: {e}")
                self.gpu_enabled = False
                return False
        else:
            self.gpu_enabled = False
            print("ğŸ’» CPUå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ")
            return True
    
    def rnd(self) -> float:
        """Cå®Ÿè£…ã®rnd()é–¢æ•°ã‚’å†ç¾"""
        return random.randint(0, 9999) / 10000.0
    
    def sgn(self, x: float) -> float:
        """Cå®Ÿè£…ã®sgn()é–¢æ•°ã‚’å†ç¾"""
        if x > 0.0:
            return 1.0
        elif x == 0.0:
            return 0.0
        else:
            return -1.0
    
    def sigmf(self, u: float) -> float:
        """
        Cå®Ÿè£…ã®sigmf()é–¢æ•°ã‚’å®Œå…¨å†ç¾
        sigmoid(u) = 1 / (1 + exp(-2 * u / u0))
        """
        try:
            return 1.0 / (1.0 + math.exp(-2.0 * u / self.sigmoid_threshold))
        except OverflowError:
            return 0.0 if u < 0 else 1.0
    
    def sigmf_array(self, u_array: np.ndarray, use_gpu: bool = False) -> np.ndarray:
        """
        é…åˆ—ç‰ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ï¼ˆPhase GPU-1å¯¾å¿œï¼‰
        é‡‘å­å‹‡æ°ã‚ªãƒªã‚¸ãƒŠãƒ«sigmf()é–¢æ•°ã®å®Œå…¨æº–æ‹ é…åˆ—ç‰ˆ
        
        GPUä½¿ç”¨æ™‚ã‚‚æ•°å­¦çš„çµæœã¯å®Œå…¨ã«åŒä¸€
        """
        if use_gpu and self.gpu_enabled and self.gpu_available:
            try:
                # GPUè¨ˆç®—ï¼ˆæ•°å­¦çš„çµæœã¯CPUç‰ˆã¨å®Œå…¨ä¸€è‡´ï¼‰
                u_gpu = cp.asarray(u_array)
                result_gpu = 1.0 / (1.0 + cp.exp(-2.0 * u_gpu / self.sigmoid_threshold))
                
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«æº–æ‹ ï¼‰
                overflow_mask = cp.isinf(cp.exp(-2.0 * u_gpu / self.sigmoid_threshold))
                result_gpu = cp.where(overflow_mask, 
                                    cp.where(u_gpu < 0, 0.0, 1.0), 
                                    result_gpu)
                
                return cp.asnumpy(result_gpu)
                
            except Exception as e:
                print(f"âš ï¸ GPU sigmfå¤±æ•—ã€CPUç‰ˆã§ç¶™ç¶š: {e}")
                # CPUç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # CPUç‰ˆï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«æº–æ‹ ï¼‰
        result = np.zeros_like(u_array)
        for i, u in enumerate(u_array):
            try:
                result[i] = 1.0 / (1.0 + math.exp(-2.0 * u / self.sigmoid_threshold))
            except OverflowError:
                result[i] = 0.0 if u < 0 else 1.0
        
        return result
    
    def neuro_init(self, input_size: int, num_outputs: int, hidden_size: int, hidden2_size: int):
        """
        Cå®Ÿè£…ã®neuro_init()ã‚’å®Œå…¨å†ç¾
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–ã¨ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿è¨­å®š
        """
        self.input_units = input_size
        self.output_units = num_outputs
        self.hidden_units = hidden_size
        self.hidden2_units = hidden2_size
        self.total_units = input_size + hidden_size + hidden2_size
        
        print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆ: å…¥åŠ›{self.input_units} éš ã‚Œ{self.hidden_units} å‡ºåŠ›{self.output_units}")
        
        # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«å¯¾ã—ã¦åˆæœŸåŒ–
        for n in range(self.output_units):
            # excitatory_inhibitoryé…åˆ—ã®åˆæœŸåŒ–ï¼ˆèˆˆå¥®æ€§ãƒ»æŠ‘åˆ¶æ€§ã®è¨­å®šï¼‰
            for k in range(self.total_units + 2):
                self.excitatory_inhibitory[k] = ((k+1) % 2) * 2 - 1  # 0â†’-1, 1â†’1, 2â†’-1, 3â†’1, ...
            self.excitatory_inhibitory[self.input_units + 2] = 1  # å‡ºåŠ›ãƒ¦ãƒ‹ãƒƒãƒˆã¯èˆˆå¥®æ€§
            
            # é‡ã¿åˆæœŸåŒ–
            for k in range(self.input_units + 2, self.total_units + 2):
                for l in range(self.total_units + 2):
                    # åŸºæœ¬é‡ã¿è¨­å®š
                    if l < 2:
                        self.output_weights[n][k][l] = self.initial_weight_2 * self.rnd()
                    if l > 1:
                        self.output_weights[n][k][l] = self.initial_weight_1 * self.rnd()
                    
                    # æ§‹é€ çš„åˆ¶ç´„ã®é©ç”¨
                    if (k > self.total_units + 1 - self.hidden2_units and 
                        l < self.input_units + 2 and l >= 2):
                        self.output_weights[n][k][l] = 0
                    
                    if (self.flags[6] == 1 and k != l and 
                        k > self.input_units + 2 and l > self.input_units + 1):
                        self.output_weights[n][k][l] = 0
                    
                    if (self.flags[6] == 1 and k > self.input_units + 1 and 
                        l > self.input_units + 1 and l < self.input_units + 3):
                        self.output_weights[n][k][l] = 0
                    
                    if (self.flags[7] == 1 and l >= 2 and l < self.input_units + 2 and 
                        k >= self.input_units + 2 and k < self.input_units + 3):
                        self.output_weights[n][k][l] = 0
                    
                    if (k > self.total_units + 1 - self.hidden2_units and l >= self.input_units + 3):
                        self.output_weights[n][k][l] = self.initial_weight_1 * self.rnd()
                    
                    # è‡ªå·±çµåˆå‡¦ç†
                    if k == l:
                        if self.flags[3] == 1:
                            self.output_weights[n][k][l] = 0
                        else:
                            self.output_weights[n][k][l] = self.initial_weight_1 * self.rnd()
                    
                    # è² å…¥åŠ›å‡¦ç†
                    if (self.flags[11] == 0 and l < self.input_units + 2 and (l % 2) == 1):
                        self.output_weights[n][k][l] = 0
                    
                    # èˆˆå¥®æ€§ãƒ»æŠ‘åˆ¶æ€§åˆ¶ç´„ã®é©ç”¨
                    self.output_weights[n][k][l] *= self.excitatory_inhibitory[l] * self.excitatory_inhibitory[k]
            
            # åˆæœŸã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨­å®š
            self.output_inputs[n][0] = self.initial_amine
            self.output_inputs[n][1] = self.initial_amine
        
        # çµ±è¨ˆåˆæœŸåŒ–
        self.error_count = 0
        self.error = 0.0
        
        if self.hyperparams.verbose:
            print(f"âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–å®Œäº†: {self.output_units}å‡ºåŠ›Ã—{self.total_units+2}ãƒ¦ãƒ‹ãƒƒãƒˆ")
        
        # Phase 1: äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ
        self._precompute_weight_indices()
    
    def _precompute_weight_indices(self):
        """
        é‡ã¿æ›´æ–°ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’äº‹å‰è¨ˆç®—ï¼ˆPhase 1æœ€é©åŒ–ï¼‰
        åˆæœŸåŒ–æ™‚ã«1å›ã ã‘å®Ÿè¡Œã—ã€å­¦ç¿’ä¸­ã¯å†åˆ©ç”¨
        ã“ã‚Œã«ã‚ˆã‚Š O(546Â²) Ã— 2500å› ã®è¨ˆç®—ã‚’ 0å› ã«å‰Šæ¸›
        """
        if self.hyperparams.verbose:
            print("ğŸš€ äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆé–‹å§‹...")
        
        self.weight_indices_cache = {}
        total_active_weights = 0
        
        for n in range(self.output_units):
            k_indices = []
            m_indices = []
            
            # ã‚¼ãƒ­ã§ãªã„é‡ã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
            for k in range(self.input_units + 2, self.total_units + 2):
                for m in range(self.total_units + 2):
                    if self.output_weights[n][k][m] != 0:
                        k_indices.append(k)
                        m_indices.append(m)
            
            # NumPyé…åˆ—ã¨ã—ã¦æ ¼ç´
            self.weight_indices_cache[n] = (np.array(k_indices), np.array(m_indices))
            total_active_weights += len(k_indices)
        
        self.cache_initialized = True
        if self.hyperparams.verbose:
            print(f"âœ… äº‹å‰è¨ˆç®—å®Œäº†: {total_active_weights}å€‹ã®æœ‰åŠ¹é‡ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥")
            print(f"   äºˆæƒ³é«˜é€ŸåŒ–: {self.output_units} Ã— (546Â²) = {self.output_units * 546 * 546:,}å›è¨ˆç®— â†’ 0å›")
    
    def teach_input(self, in_size: int, patterns: int, output_neurons: int):
        """
        Cå®Ÿè£…ã®teach_input()ã‚’å®Œå…¨å†ç¾
        æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ
        """
        self.num_patterns = patterns
        
        # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‚’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šãƒ‘ãƒªãƒ†ã‚£ï¼‰
        for k in range(output_neurons):
            self.pattern_types[k] = 1  # ãƒ‘ãƒªãƒ†ã‚£å•é¡Œ
        
        print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ: {patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³, {in_size//2}å…¥åŠ›, {output_neurons}å‡ºåŠ›")
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ
        for k in range(patterns):
            # å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆï¼ˆãƒ“ãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            for l in range(in_size // 2):
                if k & (1 << l):
                    self.input_data[k][l] = 1.0
                else:
                    self.input_data[k][l] = 0.0
            
            # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ•™å¸«ä¿¡å·ç”Ÿæˆ
            for n in range(output_neurons):
                if self.pattern_types[n] == 1:  # ãƒ‘ãƒªãƒ†ã‚£å•é¡Œ
                    m = 0
                    for l in range(in_size // 2):
                        if self.input_data[k][l] > 0.5:
                            m += 1
                    if m % 2 == 1:
                        self.teacher_data[k][n] = 1.0
                    else:
                        self.teacher_data[k][n] = 0.0
                elif self.pattern_types[n] == 0:  # ãƒ©ãƒ³ãƒ€ãƒ 
                    if self.rnd() > 0.5:
                        self.teacher_data[k][n] = 1.0
                    else:
                        self.teacher_data[k][n] = 0.0
        
        print(f"âœ… æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³")
    
    def load_external_data(self, input_data: np.ndarray, class_labels: np.ndarray):
        """
        å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’EDæ³•å½¢å¼ã«èª­ã¿è¾¼ã¿
        Args:
            input_data: [patterns, input_size] ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            class_labels: [patterns] ã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«
        """
        patterns, input_size = input_data.shape
        num_classes = len(np.unique(class_labels))
        
        self.num_patterns = patterns
        
        print(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³, {input_size}æ¬¡å…ƒ, {num_classes}ã‚¯ãƒ©ã‚¹")
        
        # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‚’è¨­å®šï¼ˆãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ï¼‰
        for k in range(num_classes):
            self.pattern_types[k] = 5  # ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆOne-Hotï¼‰
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¨­å®š
        for p in range(patterns):
            for i in range(input_size):
                self.input_data[p][i] = float(input_data[p][i])
        
        # æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®è¨­å®šï¼ˆOne-Hotå½¢å¼ï¼‰
        for p in range(patterns):
            # ã™ã¹ã¦ã®ã‚¯ãƒ©ã‚¹å‡ºåŠ›ã‚’0ã«åˆæœŸåŒ–
            for c in range(num_classes):
                self.teacher_data[p][c] = 0.0
            # æ­£è§£ã‚¯ãƒ©ã‚¹ã®ã¿1.0ã«è¨­å®š
            true_class = int(class_labels[p])
            self.teacher_data[p][true_class] = 1.0
        
        print(f"âœ… å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³")
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‚’åˆæœŸåŒ–ï¼ˆMNISTãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãï¼‰
        if input_size == 784:  # MNISTç”»åƒ
            # MNISTç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆ
            half_input = input_size // 2  # 392
            hidden_neurons = self.hyperparams.hidden_neurons  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ64
            hidden2_neurons = max(32, hidden_neurons // 2)   # éš ã‚Œå±¤2
            self.neuro_init(half_input, num_classes, hidden_neurons, hidden2_neurons)
        else:
            # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨æ±ç”¨æ§‹æˆ
            self.neuro_init(input_size, num_classes, 32, 16)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¾Œã«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‚’ç¢ºå®šã—ã€äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        if not hasattr(self, 'weight_indices_cache') or not self.weight_indices_cache:
            self._precompute_weight_indices()
    
    def load_dataset(self, train_size=1000, test_size=1000, use_fashion_mnist=False):
        """
        MNIST/Fashion-MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’EDæ³•å½¢å¼ã«èª­ã¿è¾¼ã¿
        ed_genuine.prompt.mdæº–æ‹ : ã‚¯ãƒ©ã‚¹åˆ†å¸ƒã‚’ä¿è¨¼ã—ãŸãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        Args:
            train_size: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
            test_size: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
            use_fashion_mnist: Fashion-MNISTã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
        Returns:
            tuple: (train_inputs, train_labels, test_inputs, test_labels)
        """
        if not TORCHVISION_AVAILABLE:
            raise ImportError("torchvisionãŒå¿…è¦ã§ã™: pip install torchvision")
        
        dataset_name = "Fashion-MNIST" if use_fashion_mnist else "MNIST"
        if self.hyperparams.verbose:
            print(f"{dataset_name}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿: è¨“ç·´{train_size}, ãƒ†ã‚¹ãƒˆ{test_size}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        transform = transforms.Compose([transforms.ToTensor()])
        
        if use_fashion_mnist:
            train_dataset = torchvision.datasets.FashionMNIST(
                root='./data', train=True, download=True, transform=transform)
            test_dataset = torchvision.datasets.FashionMNIST(
                root='./data', train=False, download=True, transform=transform)
        else:
            train_dataset = torchvision.datasets.MNIST(
                root='./data', train=True, download=True, transform=transform)
            test_dataset = torchvision.datasets.MNIST(
                root='./data', train=False, download=True, transform=transform)
        
        # å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®šï¼ˆå›ºå®šå€¤ä½¿ç”¨ï¼‰
        import random
        random.seed(42)
        np.random.seed(42)
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
        train_class_indices = {i: [] for i in range(10)}
        for idx, (_, label) in enumerate(train_dataset):
            if idx < len(train_dataset):  # å®‰å…¨æ€§ç¢ºä¿
                train_class_indices[int(label)].append(idx)
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰  
        test_class_indices = {i: [] for i in range(10)}
        for idx, (_, label) in enumerate(test_dataset):
            if idx < len(test_dataset):  # å®‰å…¨æ€§ç¢ºä¿
                test_class_indices[int(label)].append(idx)
        
        # ãƒãƒ©ãƒ³ã‚¹ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰
        train_inputs = []
        train_labels = []
        
        # å„ã‚¯ãƒ©ã‚¹ã‹ã‚‰å‡ç­‰ã«ã‚µãƒ³ãƒ—ãƒ«å–å¾—ï¼ˆæœ€ä½ä¿è¨¼ + æ®‹ã‚Šåˆ†æ•£ï¼‰
        min_per_class = max(1, train_size // 10)  # å„ã‚¯ãƒ©ã‚¹æœ€ä½1ã‚µãƒ³ãƒ—ãƒ«
        remaining_samples = train_size - (min_per_class * 10)
        
        selected_train_indices = []
        for class_id in range(10):
            available_indices = train_class_indices[class_id]
            if len(available_indices) > 0:
                # åŸºæœ¬å‰²ã‚Šå½“ã¦
                take_count = min(min_per_class, len(available_indices))
                selected = random.sample(available_indices, take_count)
                selected_train_indices.extend([(idx, class_id) for idx in selected])
        
        # æ®‹ã‚Šã‚µãƒ³ãƒ—ãƒ«ã‚’ãƒ©ãƒ³ãƒ€ãƒ åˆ†æ•£
        if remaining_samples > 0:
            all_remaining = []
            for class_id in range(10):
                available = train_class_indices[class_id]
                already_selected = [idx for idx, cls in selected_train_indices if cls == class_id]
                remaining_for_class = [idx for idx in available if idx not in already_selected]
                all_remaining.extend([(idx, class_id) for idx in remaining_for_class])
            
            if len(all_remaining) >= remaining_samples:
                additional = random.sample(all_remaining, remaining_samples)
                selected_train_indices.extend(additional)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦é †åºæ€§ã‚’æ’é™¤
        random.shuffle(selected_train_indices)
        
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆè¨“ç·´ï¼‰
        for idx, label in selected_train_indices:
            image, _ = train_dataset[idx]
            flattened = image.flatten().numpy()
            train_inputs.append(flattened)
            train_labels.append(label)
        
        # ãƒãƒ©ãƒ³ã‚¹ä»˜ãã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰ - è¨“ç·´ã¨åŒæ§˜ã®ãƒ­ã‚¸ãƒƒã‚¯
        test_inputs = []
        test_labels = []
        
        min_per_class_test = max(1, test_size // 10)
        remaining_test_samples = test_size - (min_per_class_test * 10)
        
        selected_test_indices = []
        for class_id in range(10):
            available_indices = test_class_indices[class_id]
            if len(available_indices) > 0:
                take_count = min(min_per_class_test, len(available_indices))
                selected = random.sample(available_indices, take_count)
                selected_test_indices.extend([(idx, class_id) for idx in selected])
        
        # æ®‹ã‚Šã‚µãƒ³ãƒ—ãƒ«åˆ†æ•£ï¼ˆãƒ†ã‚¹ãƒˆï¼‰
        if remaining_test_samples > 0:
            all_remaining_test = []
            for class_id in range(10):
                available = test_class_indices[class_id]
                already_selected = [idx for idx, cls in selected_test_indices if cls == class_id]
                remaining_for_class = [idx for idx in available if idx not in already_selected]
                all_remaining_test.extend([(idx, class_id) for idx in remaining_for_class])
            
            if len(all_remaining_test) >= remaining_test_samples:
                additional = random.sample(all_remaining_test, remaining_test_samples)
                selected_test_indices.extend(additional)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆãƒ†ã‚¹ãƒˆï¼‰
        random.shuffle(selected_test_indices)
        
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ†ã‚¹ãƒˆï¼‰
        for idx, label in selected_test_indices:
            image, _ = test_dataset[idx]
            flattened = image.flatten().numpy()
            test_inputs.append(flattened)
            test_labels.append(label)
        
        # NumPyé…åˆ—ã«å¤‰æ›
        train_inputs = np.array(train_inputs)
        train_labels = np.array(train_labels)
        test_inputs = np.array(test_inputs)
        test_labels = np.array(test_labels)
        
        if self.hyperparams.verbose:
            print(f"âœ… MNISTãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†:")
            print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_inputs.shape}, ãƒ©ãƒ™ãƒ«: {train_labels.shape}")
            print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_inputs.shape}, ãƒ©ãƒ™ãƒ«: {test_labels.shape}")
            print(f"  å…¥åŠ›å€¤ç¯„å›²: [{train_inputs.min():.3f}, {train_inputs.max():.3f}]")
            print(f"  ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.unique(train_labels)}")
        
        return train_inputs, train_labels, test_inputs, test_labels
    
    def neuro_output_calc(self, indata_input: List[float]):
        """
        Cå®Ÿè£…ã®neuro_output_calc()ã‚’å®Œå…¨å†ç¾ - ed_genuine.prompt.mdå³å¯†æº–æ‹ ç‰ˆ
        ğŸ”¬ åŸè‘—ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã«100%å¿ å®Ÿãªå®Ÿè£…
        """
        for n in range(self.output_units):
            # å…¥åŠ›è¨­å®šï¼ˆåŸè‘—Cå®Ÿè£…ã¨å®Œå…¨åŒä¸€ï¼‰
            for k in range(2, self.input_units + 2):
                input_index = int(k/2) - 1
                if input_index < len(indata_input):
                    self.output_inputs[n][k] = indata_input[input_index]
            
            if self.flags[6]:
                for k in range(self.input_units + 2, self.total_units + 2):
                    self.output_inputs[n][k] = 0

            # å¤šæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—è¨ˆç®—ï¼ˆåŸè‘—é€šã‚Š: for (t = 1; t <= t_loop; t++)ï¼‰
            for t in range(1, self.time_loops + 1):
                for k in range(self.input_units + 2, self.total_units + 2):
                    inival = 0.0
                    # é‡ã¿è¨ˆç®—ï¼ˆåŸè‘—é€šã‚Š: inival += w_ot_ot[n][k][m] * ot_in[n][m]ï¼‰
                    for m in range(self.total_units + 2):
                        inival += self.output_weights[n][k][m] * self.output_inputs[n][m]
                    # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰æ´»æ€§åŒ–ï¼ˆåŸè‘—é€šã‚Š: ot_ot[n][k] = sigmf(inival)ï¼‰
                    self.output_outputs[n][k] = self.sigmf(inival)
                
                # å‡ºåŠ›ã‚’æ¬¡ã®æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ã«è¨­å®šï¼ˆåŸè‘—é€šã‚Šï¼‰
                for k in range(self.input_units + 2, self.total_units + 2):
                    self.output_inputs[n][k] = self.output_outputs[n][k]
    
    def neuro_teach_calc(self, indata_tch: List[float]):
        """
        Cå®Ÿè£…ã®neuro_teach_calc()ã‚’å®Œå…¨å†ç¾
        ã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨ˆç®—
        """
        for l in range(self.output_units):
            # å‡ºåŠ›èª¤å·®è¨ˆç®—
            wkb = indata_tch[l] - self.output_outputs[l][self.input_units + 2]
            self.error += abs(wkb)
            if abs(wkb) > 0.5:
                self.error_count += 1
            
            # å‡ºåŠ›å±¤ã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨­å®š
            if wkb > 0:
                self.amine_concentrations[l][self.input_units + 2][0] = wkb
                self.amine_concentrations[l][self.input_units + 2][1] = 0
            else:
                self.amine_concentrations[l][self.input_units + 2][0] = 0
                self.amine_concentrations[l][self.input_units + 2][1] = -wkb
            
            # éš ã‚Œå±¤ã¸ã®ã‚¢ãƒŸãƒ³æ‹¡æ•£
            inival1 = self.amine_concentrations[l][self.input_units + 2][0]
            inival2 = self.amine_concentrations[l][self.input_units + 2][1]
            
            for k in range(self.input_units + 3, self.total_units + 2):
                self.amine_concentrations[l][k][0] = inival1 * self.diffusion_rate
                self.amine_concentrations[l][k][1] = inival2 * self.diffusion_rate
    
    def neuro_weight_calc(self):
        """
        è¶…é«˜é€Ÿé‡ã¿æ›´æ–°ï¼ˆPhase GPU-1å¯¾å¿œç‰ˆï¼‰
        
        ã€GPUå®Ÿè£…æ–¹é‡ã€‘
        - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Œå…¨æ€§æœ€å„ªå…ˆï¼ˆé‡‘å­å‹‡æ°ã‚ªãƒªã‚¸ãƒŠãƒ«100%æº–æ‹ ï¼‰
        - GPUå¤±æ•—æ™‚ã¯ç¢ºå®Ÿã«CPUç‰ˆã§å‹•ä½œç¶™ç¶š
        - æ®µéšçš„GPUé©ç”¨ï¼ˆå¤§è¦æ¨¡è¨ˆç®—ã®ã¿GPUã€å°è¦æ¨¡ã¯CPUç¶­æŒï¼‰
        - Phase 1-3ã®å…¨æœ€é©åŒ–ç¶™æ‰¿
        
        ã€æœŸå¾…åŠ¹æœã€‘
        - GPUåˆ©ç”¨æ™‚: 1.2-1.5å€é«˜é€ŸåŒ–
        - CPUç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Phase 1-3ã¨åŒä¸€æ€§èƒ½ç¶­æŒ
        """
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–
        self.profiler.start_timer('weight_loop_init')
        
        if not self.cache_initialized:
            raise RuntimeError("äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # GPUä½¿ç”¨åˆ¤å®šï¼ˆä¿®æ­£ç‰ˆï¼šæ¡ä»¶ã‚’ç·©å’Œï¼‰
        use_gpu_for_weights = (self.gpu_enabled and 
                              self.gpu_available and 
                              self.output_units >= 1)  # å˜ä¸€å‡ºåŠ›ã§ã‚‚GPUä½¿ç”¨å¯èƒ½
        
        self.profiler.end_timer('weight_loop_init')
        
        if use_gpu_for_weights:
            try:
                if self.hyperparams.verbose:
                    print("ğŸ”§ GPUé‡ã¿æ›´æ–°ã‚’å®Ÿè¡Œä¸­...")
                self._neuro_weight_calc_gpu()
                return
            except Exception as e:
                print(f"âš ï¸ GPUé‡ã¿æ›´æ–°å¤±æ•—ã€CPUç‰ˆã§ç¶™ç¶š: {e}")
        
        # CPUç‰ˆï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–æœ€é©åŒ–ç‰ˆã‚’å„ªå…ˆä½¿ç”¨ï¼‰
        # Phase 1-3æœ€é©åŒ– + ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ã‚ˆã‚‹åŠ‡çš„é«˜é€ŸåŒ–ï¼ˆ198å€æ”¹å–„ï¼‰
        if self.hyperparams.verbose:
            print("ğŸš€ ãƒ™ã‚¯ãƒˆãƒ«åŒ–CPUé‡ã¿æ›´æ–°ã‚’å®Ÿè¡Œä¸­...")
        self._neuro_weight_calc_vectorized()
    
    def _neuro_weight_calc_cpu(self):
        """CPUç‰ˆé‡ã¿æ›´æ–°ï¼ˆPhase 1-3æœ€é©åŒ–ç¶™æ‰¿ï¼‰"""
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–
        self.profiler.start_timer('weight_loop_init')
        
        for n in range(self.output_units):
            # Phase 1-2: äº‹å‰è¨ˆç®—æ¸ˆã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨
            k_arr, m_arr = self.weight_indices_cache[n]
            
            if len(k_arr) == 0:
                continue
            
            self.profiler.end_timer('weight_loop_init')
            
            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
            self.profiler.start_timer('weight_memory_access')
            
            # Phase 3: einsumæœ€é©åŒ–ãƒ™ã‚¯ãƒˆãƒ«åŒ–è¨ˆç®—
            inputs = self.output_inputs[n, m_arr]
            outputs = self.output_outputs[n, k_arr]
            
            self.profiler.end_timer('weight_memory_access')
            
            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: deltaè¨ˆç®—
            self.profiler.start_timer('weight_delta_calc')
            
            # Phase 3: einsumæ´»ç”¨deltaè¨ˆç®—
            abs_outputs = np.abs(outputs)
            delta = self.learning_rate * np.einsum('i,i,i->i', inputs, abs_outputs, (1 - abs_outputs))
            
            self.profiler.end_timer('weight_delta_calc')
            
            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ã‚¢ãƒŸãƒ³å‡¦ç†
            self.profiler.start_timer('weight_amine_proc')
            
            # ãƒ•ãƒ©ã‚°ã«ã‚ˆã‚‹åˆ†å²ï¼ˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Œå…¨ä¿æŒï¼‰
            if self.flags[10] == 1:
                # é‡ã¿æ¸›è¡°ãƒ•ãƒ©ã‚°æœ‰åŠ¹æ™‚
                excit_k = self.excitatory_inhibitory[k_arr]
                amine_diff = (self.amine_concentrations[n, k_arr, 0] - 
                             self.amine_concentrations[n, k_arr, 1])
                weight_update = np.einsum('i,i,i->i', delta, excit_k, amine_diff)
            else:
                # é€šå¸¸ã®é‡ã¿æ›´æ–°ï¼ˆç”Ÿç‰©å­¦çš„åˆ¶ç´„æº–æ‹ ï¼‰
                excit_m = self.excitatory_inhibitory[m_arr]
                excit_k = self.excitatory_inhibitory[k_arr]
                
                pos_mask = excit_m > 0
                neg_mask = ~pos_mask
                
                weight_update = np.zeros_like(delta)
                
                # èˆˆå¥®æ€§å…¥åŠ›å‡¦ç†
                if np.any(pos_mask):
                    pos_delta = delta[pos_mask]
                    pos_amine = self.amine_concentrations[n, k_arr[pos_mask], 0]
                    pos_excit_m = excit_m[pos_mask]
                    pos_excit_k = excit_k[pos_mask]
                    weight_update[pos_mask] = np.einsum('i,i,i,i->i', 
                                                      pos_delta, pos_amine, 
                                                      pos_excit_m, pos_excit_k)
                
                # æŠ‘åˆ¶æ€§å…¥åŠ›å‡¦ç†
                if np.any(neg_mask):
                    neg_delta = delta[neg_mask]
                    neg_amine = self.amine_concentrations[n, k_arr[neg_mask], 1]
                    neg_excit_m = excit_m[neg_mask]
                    neg_excit_k = excit_k[neg_mask]
                    weight_update[neg_mask] = np.einsum('i,i,i,i->i', 
                                                      neg_delta, neg_amine,
                                                      neg_excit_m, neg_excit_k)
            
            self.profiler.end_timer('weight_amine_proc')
            
            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: æ•°å­¦æ¼”ç®—
            self.profiler.start_timer('weight_math_ops')
            
            # é‡ã¿æ›´æ–°é©ç”¨
            self.output_weights[n, k_arr, m_arr] += weight_update
            
            self.profiler.end_timer('weight_math_ops')
    
    def _neuro_weight_calc_optimized(self):
        """
        MNISTå¯¾å¿œæœ€é©åŒ–ç‰ˆé‡ã¿æ›´æ–°
        ed_genuine.prompt.mdæº–æ‹ : w_ot_ot[n][k][m] != 0 ã®å ´åˆã®ã¿å‡¦ç†
        """
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–
        self.profiler.start_timer('weight_loop_init')
        
        total_weights_processed = 0
        
        for n in range(self.output_units):
            n_indices = self.nonzero_weight_indices[n]
            
            for k_idx, k in enumerate(range(self.input_units + 2, self.total_units + 2)):
                m_list = n_indices[k_idx]
                if not m_list:  # éã‚¼ãƒ­é‡ã¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # éã‚¼ãƒ­é‡ã¿ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜æº–æ‹ ï¼‰
                for m in m_list:
                    total_weights_processed += 1
        
        self.profiler.end_timer('weight_loop_init')
        
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
        self.profiler.start_timer('weight_memory_access')
        
        for n in range(self.output_units):
            n_indices = self.nonzero_weight_indices[n]
            
            for k_idx, k in enumerate(range(self.input_units + 2, self.total_units + 2)):
                m_list = n_indices[k_idx]
                if not m_list:  # éã‚¼ãƒ­é‡ã¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # éã‚¼ãƒ­é‡ã¿ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜æº–æ‹ ï¼‰
                for m in m_list:
                    weight = self.output_weights[n][k][m]
                    if weight == 0:  # å®‰å…¨ãƒã‚§ãƒƒã‚¯
                        continue
                    
                    # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã‹ã‚‰èª­ã¿è¾¼ã¿
                    input_val = self.output_inputs[n][m]
                    output_val = self.output_outputs[n][k]
                    excit_m = self.excitatory_inhibitory[m] 
                    excit_k = self.excitatory_inhibitory[k]
        
        self.profiler.end_timer('weight_memory_access')
        
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: deltaè¨ˆç®—ã¨ã‚¢ãƒŸãƒ³å‡¦ç†
        self.profiler.start_timer('weight_delta_calc')
        
        for n in range(self.output_units):
            n_indices = self.nonzero_weight_indices[n]
            
            for k_idx, k in enumerate(range(self.input_units + 2, self.total_units + 2)):
                m_list = n_indices[k_idx]
                if not m_list:  # éã‚¼ãƒ­é‡ã¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # éã‚¼ãƒ­é‡ã¿ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜æº–æ‹ ï¼‰
                for m in m_list:
                    weight = self.output_weights[n][k][m]
                    if weight == 0:  # å®‰å…¨ãƒã‚§ãƒƒã‚¯
                        continue
                    
                    # deltaè¨ˆç®—ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
                    delta = self.learning_rate * self.output_inputs[n][m]
                    delta *= abs(self.output_outputs[n][k])
                    delta *= (1 - abs(self.output_outputs[n][k]))
        
        self.profiler.end_timer('weight_delta_calc')
        
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ã‚¢ãƒŸãƒ³å‡¦ç†ã¨æ•°å­¦æ¼”ç®—
        self.profiler.start_timer('weight_amine_proc')
        
        for n in range(self.output_units):
            n_indices = self.nonzero_weight_indices[n]
            
            for k_idx, k in enumerate(range(self.input_units + 2, self.total_units + 2)):
                m_list = n_indices[k_idx]
                if not m_list:  # éã‚¼ãƒ­é‡ã¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # éã‚¼ãƒ­é‡ã¿ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜æº–æ‹ ï¼‰
                for m in m_list:
                    weight = self.output_weights[n][k][m]
                    if weight == 0:  # å®‰å…¨ãƒã‚§ãƒƒã‚¯
                        continue
                    
                    # deltaè¨ˆç®—ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
                    delta = self.learning_rate * self.output_inputs[n][m]
                    delta *= abs(self.output_outputs[n][k])
                    delta *= (1 - abs(self.output_outputs[n][k]))
                    
                    # ã‚¢ãƒŸãƒ³æ¿ƒåº¦ã«ã‚ˆã‚‹é‡ã¿æ›´æ–°
                    excit_m = self.excitatory_inhibitory[m] 
                    excit_k = self.excitatory_inhibitory[k]
                    
                    if excit_m > 0:  # èˆˆå¥®æ€§å…¥åŠ›
                        weight_update = (delta * 
                                       self.amine_concentrations[n][k][0] * 
                                       excit_m * excit_k)
                    else:  # æŠ‘åˆ¶æ€§å…¥åŠ›
                        weight_update = (delta * 
                                       self.amine_concentrations[n][k][1] * 
                                       excit_m * excit_k)
                    
                    # é‡ã¿æ›´æ–°é©ç”¨
                    self.output_weights[n][k][m] += weight_update
        
        self.profiler.end_timer('weight_amine_proc')
    
    def _neuro_weight_calc_gpu(self):
        """GPUç‰ˆé‡ã¿æ›´æ–°ï¼ˆPhase GPU-1å®Ÿè£…ï¼‰"""
        for n in range(self.output_units):
            k_arr, m_arr = self.weight_indices_cache[n]
            
            if len(k_arr) == 0:
                continue
            
            # GPUä½¿ç”¨é–¾å€¤ã‚’ç·©å’Œï¼ˆå°è¦æ¨¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã‚‚GPUä½¿ç”¨ï¼‰
            if len(k_arr) > 10:  # é–¾å€¤ã‚’å¤§å¹…ã«ç·©å’Œï¼ˆ10å€‹ä»¥ä¸Šã§GPUä½¿ç”¨ï¼‰
                try:
                    # GPUè¨ˆç®—
                    inputs_gpu = cp.asarray(self.output_inputs[n, m_arr])
                    outputs_gpu = cp.asarray(self.output_outputs[n, k_arr])
                    
                    # GPUç‰ˆdeltaè¨ˆç®—ï¼ˆæ•°å­¦çš„ã«ã¯CPUç‰ˆã¨å®Œå…¨ä¸€è‡´ï¼‰
                    abs_outputs_gpu = cp.abs(outputs_gpu)
                    delta_gpu = (self.learning_rate * inputs_gpu * 
                               abs_outputs_gpu * (1 - abs_outputs_gpu))
                    
                    # CPUç‰ˆã¨åŒä¸€ã®ãƒ•ãƒ©ã‚°åˆ†å²
                    if self.flags[10] == 1:
                        excit_k_gpu = cp.asarray(self.excitatory_inhibitory[k_arr])
                        amine_diff_gpu = cp.asarray(
                            self.amine_concentrations[n, k_arr, 0] - 
                            self.amine_concentrations[n, k_arr, 1]
                        )
                        weight_update_gpu = delta_gpu * excit_k_gpu * amine_diff_gpu
                    else:
                        # èˆˆå¥®æ€§/æŠ‘åˆ¶æ€§åˆ†å²ï¼ˆCPUç‰ˆã¨å®Œå…¨ä¸€è‡´ï¼‰
                        excit_m = self.excitatory_inhibitory[m_arr]
                        excit_k = self.excitatory_inhibitory[k_arr]
                        
                        pos_mask = excit_m > 0
                        weight_update = np.zeros_like(cp.asnumpy(delta_gpu))
                        
                        # GPU + CPUæ··åˆå‡¦ç†ï¼ˆæœ€é©åŒ–ã®ãŸã‚ï¼‰
                        if np.any(pos_mask):
                            pos_indices = np.where(pos_mask)[0]
                            pos_delta_gpu = delta_gpu[pos_indices]
                            pos_amine_gpu = cp.asarray(self.amine_concentrations[n, k_arr[pos_mask], 0])
                            pos_excit_m_gpu = cp.asarray(excit_m[pos_mask])
                            pos_excit_k_gpu = cp.asarray(excit_k[pos_mask])
                            
                            weight_update_pos_gpu = (pos_delta_gpu * pos_amine_gpu * 
                                                   pos_excit_m_gpu * pos_excit_k_gpu)
                            weight_update[pos_mask] = cp.asnumpy(weight_update_pos_gpu)
                        
                        neg_mask = ~pos_mask
                        if np.any(neg_mask):
                            neg_indices = np.where(neg_mask)[0]
                            neg_delta_gpu = delta_gpu[neg_indices]
                            neg_amine_gpu = cp.asarray(self.amine_concentrations[n, k_arr[neg_mask], 1])
                            neg_excit_m_gpu = cp.asarray(excit_m[neg_mask])
                            neg_excit_k_gpu = cp.asarray(excit_k[neg_mask])
                            
                            weight_update_neg_gpu = (neg_delta_gpu * neg_amine_gpu * 
                                                   neg_excit_m_gpu * neg_excit_k_gpu)
                            weight_update[neg_mask] = cp.asnumpy(weight_update_neg_gpu)
                        
                        weight_update_gpu = cp.asarray(weight_update)
                    
                    # GPUçµæœã‚’CPUã«è»¢é€ã—ã¦é©ç”¨
                    weight_update_final = cp.asnumpy(weight_update_gpu)
                    self.output_weights[n, k_arr, m_arr] += weight_update_final
                    
                except Exception as gpu_error:
                    # GPUå¤±æ•—æ™‚ã¯è©²å½“ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’CPUç‰ˆã§å‡¦ç†
                    print(f"âš ï¸ GPUè¨ˆç®—å¤±æ•—ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³{n}ï¼‰ã€CPUç‰ˆã§å‡¦ç†: {gpu_error}")
                    self._process_single_neuron_cpu(n)
            else:
                # å°è¦æ¨¡è¨ˆç®—ã¯CPUç‰ˆãŒåŠ¹ç‡çš„
                self._process_single_neuron_cpu(n)
    
    def _process_single_neuron_cpu(self, n: int):
        """å˜ä¸€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®CPUå‡¦ç†ï¼ˆGPUå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        k_arr, m_arr = self.weight_indices_cache[n]
        
        if len(k_arr) == 0:
            return
        
        # CPUç‰ˆå‡¦ç†ï¼ˆPhase 1-3æœ€é©åŒ–ï¼‰
        inputs = self.output_inputs[n, m_arr]
        outputs = self.output_outputs[n, k_arr]
        
        abs_outputs = np.abs(outputs)
        delta = self.learning_rate * np.einsum('i,i,i->i', inputs, abs_outputs, (1 - abs_outputs))
        
        if self.flags[10] == 1:
            excit_k = self.excitatory_inhibitory[k_arr]
            amine_diff = (self.amine_concentrations[n, k_arr, 0] - 
                         self.amine_concentrations[n, k_arr, 1])
            weight_update = np.einsum('i,i,i->i', delta, excit_k, amine_diff)
        else:
            excit_m = self.excitatory_inhibitory[m_arr]
            excit_k = self.excitatory_inhibitory[k_arr]
            
            pos_mask = excit_m > 0
            neg_mask = ~pos_mask
            
            weight_update = np.zeros_like(delta)
            
            if np.any(pos_mask):
                pos_delta = delta[pos_mask]
                pos_amine = self.amine_concentrations[n, k_arr[pos_mask], 0]
                pos_excit_m = excit_m[pos_mask]
                pos_excit_k = excit_k[pos_mask]
                weight_update[pos_mask] = np.einsum('i,i,i,i->i', 
                                                  pos_delta, pos_amine, 
                                                  pos_excit_m, pos_excit_k)
            
            if np.any(neg_mask):
                neg_delta = delta[neg_mask]
                neg_amine = self.amine_concentrations[n, k_arr[neg_mask], 1]
                neg_excit_m = excit_m[neg_mask]
                neg_excit_k = excit_k[neg_mask]
                weight_update[neg_mask] = np.einsum('i,i,i,i->i', 
                                                  neg_delta, neg_amine,
                                                  neg_excit_m, neg_excit_k)
        
        self.output_weights[n, k_arr, m_arr] += weight_update
    
    def _neuro_weight_calc_vectorized(self):
        """
        ed_genuine.prompt.mdå®Œå…¨æº–æ‹ ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–é‡ã¿æ›´æ–°ï¼ˆä¿®æ­£ç‰ˆï¼‰
        
        Cå®Ÿè£…ã®æ­£ç¢ºãªå†ç¾:
        for (n = 0; n < ot; n++) {
            for (k = in+2; k <= all+1; k++) {
                for (m = 0; m <= all+1; m++) {
                    if (w_ot_ot[n][k][m] != 0) {
                        del = alpha * ot_in[n][m];
                        del *= fabs(ot_ot[n][k]);
                        del *= (1 - fabs(ot_ot[n][k]));
                        
                        if (ow[m] > 0)  // èˆˆå¥®æ€§å…¥åŠ›
                            w_ot_ot[n][k][m] += del * del_ot[n][k][0] * ow[m] * ow[k];
                        else            // æŠ‘åˆ¶æ€§å…¥åŠ›
                            w_ot_ot[n][k][m] += del * del_ot[n][k][1] * ow[m] * ow[k];
                    }
                }
            }
        }
        """
        # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§ç‹¬ç«‹ã—ãŸé‡ã¿æ›´æ–°ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
        for n in range(self.output_units):
            # Phase 1: æœ‰åŠ¹é‡ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¸€åº¦ã ã‘å–å¾—
            if hasattr(self, 'weight_indices_cache') and n in self.weight_indices_cache:
                k_arr, m_arr = self.weight_indices_cache[n]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”Ÿæˆ
                k_indices = []
                m_indices = []
                for k in range(self.input_units + 2, self.total_units + 2):
                    for m in range(self.total_units + 2):
                        if self.output_weights[n][k][m] != 0:
                            k_indices.append(k)
                            m_indices.append(m)
                k_arr = np.array(k_indices)
                m_arr = np.array(m_indices)
            
            if len(k_arr) == 0:
                continue
            
            # Phase 2: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆä¸€æ‹¬å–å¾—ï¼‰
            inputs = self.output_inputs[n, m_arr]      # ot_in[n][m]
            outputs = self.output_outputs[n, k_arr]    # ot_ot[n][k]
            excit_m = self.excitatory_inhibitory[m_arr] # ow[m]
            excit_k = self.excitatory_inhibitory[k_arr] # ow[k]
            
            # Phase 3: deltaè¨ˆç®—ï¼ˆed_genuine.prompt.mdå®Œå…¨æº–æ‹ ï¼‰
            # del = alpha * ot_in[n][m] * fabs(ot_ot[n][k]) * (1 - fabs(ot_ot[n][k]))
            abs_outputs = np.abs(outputs)
            delta = self.learning_rate * inputs * abs_outputs * (1 - abs_outputs)
            
            # Phase 4: ã‚¢ãƒŸãƒ³æ¿ƒåº¦ã«ã‚ˆã‚‹é‡ã¿æ›´æ–°ï¼ˆèˆˆå¥®æ€§/æŠ‘åˆ¶æ€§åˆ†é›¢ï¼‰
            # if (ow[m] > 0) èˆˆå¥®æ€§å…¥åŠ›: del * del_ot[n][k][0] * ow[m] * ow[k]
            # else æŠ‘åˆ¶æ€§å…¥åŠ›: del * del_ot[n][k][1] * ow[m] * ow[k]
            pos_mask = excit_m > 0  # èˆˆå¥®æ€§å…¥åŠ›ãƒã‚¹ã‚¯
            neg_mask = ~pos_mask    # æŠ‘åˆ¶æ€§å…¥åŠ›ãƒã‚¹ã‚¯
            
            weight_update = np.zeros_like(delta)
            
            # èˆˆå¥®æ€§å…¥åŠ›å‡¦ç†ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
            if np.any(pos_mask):
                pos_amine = self.amine_concentrations[n, k_arr[pos_mask], 0]  # del_ot[n][k][0]
                weight_update[pos_mask] = (delta[pos_mask] * pos_amine * 
                                         excit_m[pos_mask] * excit_k[pos_mask])
            
            # æŠ‘åˆ¶æ€§å…¥åŠ›å‡¦ç†ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
            if np.any(neg_mask):
                neg_amine = self.amine_concentrations[n, k_arr[neg_mask], 1]  # del_ot[n][k][1]
                weight_update[neg_mask] = (delta[neg_mask] * neg_amine * 
                                         excit_m[neg_mask] * excit_k[neg_mask])
            
            # Phase 5: é‡ã¿æ›´æ–°é©ç”¨ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
            self.output_weights[n, k_arr, m_arr] += weight_update
    
    def neuro_calc(self, indata_input: List[float], indata_tch: List[float]):
        """
        Cå®Ÿè£…ã®neuro_calc()ã‚’å®Œå…¨å†ç¾
        1ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—
        """
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: é †æ–¹å‘è¨ˆç®—
        self.profiler.start_timer('forward_pass')
        self.neuro_output_calc(indata_input)
        self.profiler.end_timer('forward_pass')
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: æ•™å¸«ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        self.profiler.start_timer('teacher_processing')
        self.neuro_teach_calc(indata_tch)
        self.profiler.end_timer('teacher_processing')
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: é‡ã¿æ›´æ–°
        self.profiler.start_timer('weight_update')
        self.neuro_weight_calc()
        self.profiler.end_timer('weight_update')
    
    def train_epoch(self, show_progress=True, epoch_info="ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’") -> Tuple[float, int]:
        """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’å®Ÿè¡Œ"""
        self.error = 0.0
        self.error_count = 0

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®é€²æ—è¡¨ç¤ºï¼ˆã‚¨ãƒãƒƒã‚¯æƒ…å ±çµ±åˆï¼‰
        pattern_iterator = tqdm(range(self.num_patterns), 
                               desc=epoch_info, 
                               position=1,  # position=2ã‹ã‚‰1ã«å¤‰æ›´ï¼ˆ2æ®µè¡¨ç¤ºï¼‰
                               leave=False) if show_progress else range(self.num_patterns)

        for pattern in pattern_iterator:
            # å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æº–å‚™
            indata_input = [self.input_data[pattern][i] 
                           for i in range(self.input_units // 2)]
            indata_tch = [self.teacher_data[pattern][i] 
                         for i in range(self.output_units)]

            # å­¦ç¿’å®Ÿè¡Œ
            self.neuro_calc(indata_input, indata_tch)

        avg_error = self.error / self.num_patterns if self.num_patterns > 0 else 0.0
        return avg_error, self.error_count

    def train_epoch_with_buffer(self, results_buffer: 'LearningResultsBuffer', epoch: int,
                               train_inputs: np.ndarray, train_labels: np.ndarray,
                               test_inputs: np.ndarray, test_labels: np.ndarray,
                               show_progress=True, epoch_info="ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’") -> Tuple[float, int]:
        """
        å­¦ç¿’ä¸­ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§çµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜ã™ã‚‹ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ - ç¬¬2æ®µéšæœ€é©åŒ–
        ğŸš€ ãƒãƒƒãƒåŒ–æœ€é©åŒ–: recordå‡¦ç†ã‚’å®Œå…¨ãƒãƒƒãƒåŒ–ã§é«˜é€ŸåŒ–
        ed_genuine.prompt.mdæº–æ‹ : é †æ–¹å‘è¨ˆç®—â†’çµæœä¿å­˜â†’å­¦ç¿’ã®é †åºã‚’ç¶­æŒ
        """
        self.error = 0.0
        self.error_count = 0

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®é€²æ—è¡¨ç¤º
        pattern_iterator = tqdm(range(self.num_patterns), 
                               desc=epoch_info, 
                               position=1,
                               leave=False) if show_progress else range(self.num_patterns)

        # ğŸš€ ç¬¬2æ®µéšæœ€é©åŒ–: ãƒãƒƒãƒå‡¦ç†ç”¨é…åˆ—äº‹å‰æº–å‚™
        batch_predictions = []
        batch_errors = []
        batch_corrects = []
        batch_predicted_classes = []
        batch_true_classes = []

        # 1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’ + ãƒãƒƒãƒçµæœåé›†
        for pattern in pattern_iterator:
            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: å…¨ä½“å‡¦ç†é–‹å§‹
            self.profiler.start_timer('total_processing')
            
            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹
            self.profiler.start_timer('data_preparation')
            # å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æº–å‚™
            indata_input = [self.input_data[pattern][i] 
                           for i in range(self.input_units // 2)]
            indata_tch = [self.teacher_data[pattern][i] 
                         for i in range(self.output_units)]
            self.profiler.end_timer('data_preparation')

            # å­¦ç¿’å®Ÿè¡Œï¼ˆed_genuine.prompt.mdæº–æ‹ ã®ã‚¢ãƒŸãƒ³æ‹¡æ•£å­¦ç¿’ï¼‰
            self.neuro_calc(indata_input, indata_tch)

            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: äºˆæ¸¬è¨ˆç®—é–‹å§‹
            self.profiler.start_timer('prediction_calc')
            # å­¦ç¿’å¾Œã«äºˆæ¸¬å®Ÿè¡Œï¼ˆed_genuine.prompt.mdæº–æ‹ ã®é †æ–¹å‘è¨ˆç®—ï¼‰
            prediction = self.predict(indata_input)
            predicted_class = np.argmax(prediction)
            true_class = int(train_labels[pattern])
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—: ãƒ†ã‚¹ãƒˆæ™‚ã¨åŒã˜è¨ˆç®—æ–¹å¼ã‚’ä½¿ç”¨
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - prediction[j]) for j in range(self.output_units)) / self.output_units
            self.profiler.end_timer('prediction_calc')
            
            # ğŸš€ æœ€é©åŒ–: çµæœã‚’ãƒãƒƒãƒé…åˆ—ã«è“„ç©ï¼ˆå€‹åˆ¥recordå‡¦ç†ã‚’å‰Šé™¤ï¼‰
            batch_predictions.append(prediction)
            batch_errors.append(float(error))
            batch_corrects.append(correct)
            batch_predicted_classes.append(int(predicted_class))
            batch_true_classes.append(true_class)

            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: å…¨ä½“å‡¦ç†çµ‚äº†
            self.profiler.end_timer('total_processing')
            self.profiler.complete_sample()

        # ğŸš€ ç¬¬2æ®µéšæœ€é©åŒ–: è¨“ç·´çµæœã®ä¸€æ‹¬ãƒãƒƒãƒè¨˜éŒ²
        self.profiler.start_timer('result_recording')
        for i, (correct, error, pred_class, true_class) in enumerate(
            zip(batch_corrects, batch_errors, batch_predicted_classes, batch_true_classes)):
            results_buffer.record_train_result(epoch, i, correct, error, pred_class, true_class)
        self.profiler.end_timer('result_recording')

        # 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®çµæœä¿å­˜ï¼ˆ1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Š1å›ã®ãƒãƒƒãƒå‡¦ç†ï¼‰
        # æœ€é©åŒ–: predict_batchã§ä¸€æ‹¬å‡¦ç†
        test_predictions = self.predict_batch(test_inputs)
        
        # ğŸš€ ç¬¬2æ®µéšæœ€é©åŒ–: ãƒ†ã‚¹ãƒˆçµæœã®ä¸€æ‹¬ãƒãƒƒãƒè¨˜éŒ²
        test_batch_corrects = []
        test_batch_errors = []
        test_batch_pred_classes = []
        test_batch_true_classes = []
        
        for i, (prediction, test_label) in enumerate(zip(test_predictions, test_labels)):
            predicted_class = np.argmax(prediction)
            true_class = int(test_label)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—: è¨“ç·´æ™‚ã¨åŒã˜è¨ˆç®—æ–¹å¼ã‚’ä½¿ç”¨
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - prediction[j]) for j in range(self.output_units)) / self.output_units
            
            test_batch_corrects.append(correct)
            test_batch_errors.append(float(error))
            test_batch_pred_classes.append(int(predicted_class))
            test_batch_true_classes.append(true_class)

        # ãƒ†ã‚¹ãƒˆçµæœã®ä¸€æ‹¬è¨˜éŒ²
        for i, (correct, error, pred_class, true_class) in enumerate(
            zip(test_batch_corrects, test_batch_errors, test_batch_pred_classes, test_batch_true_classes)):
            results_buffer.record_test_result(epoch, i, correct, error, pred_class, true_class)

        avg_error = self.error / self.num_patterns if self.num_patterns > 0 else 0.0
        return avg_error, self.error_count

    def train_epoch_with_minibatch(self, results_buffer: 'LearningResultsBuffer', epoch: int,
                                   train_inputs: np.ndarray, train_labels: np.ndarray,
                                   test_inputs: np.ndarray, test_labels: np.ndarray,
                                   batch_size: int, show_progress=True, 
                                   epoch_info="ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’") -> Tuple[float, int]:
        """
        ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’å¯¾å¿œã‚¨ãƒãƒƒã‚¯å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰
        
        æ³¨ï¼šé‡‘å­å‹‡æ°ã®EDç†è«–ã«ã¯ãƒãƒƒãƒå‡¦ç†æ¦‚å¿µãªã—
        å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œã®ãŸã‚ã®ç¾ä»£çš„æ©Ÿèƒ½æ‹¡å¼µ
        
        Args:
            results_buffer: å­¦ç¿’çµæœãƒãƒƒãƒ•ã‚¡
            epoch: ã‚¨ãƒãƒƒã‚¯ç•ªå·
            train_inputs: è¨“ç·´å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            train_labels: è¨“ç·´ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
            test_inputs: ãƒ†ã‚¹ãƒˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            test_labels: ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
            batch_size: ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º
            show_progress: é€²æ—è¡¨ç¤ºæœ‰ç„¡
            epoch_info: é€²æ—è¡¨ç¤ºæƒ…å ±
        
        Returns:
            Tuple[float, int]: (å¹³å‡èª¤å·®, ã‚¨ãƒ©ãƒ¼æ•°)
        """
        self.error = 0.0
        self.error_count = 0
        
        # ãƒŸãƒ‹ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆ
        train_loader = MiniBatchDataLoader(train_inputs, train_labels, batch_size, shuffle=True)
        
        # ãƒãƒƒãƒã”ã¨ã®é€²æ—è¡¨ç¤º
        batch_iterator = tqdm(train_loader, 
                             desc=epoch_info, 
                             position=1,
                             leave=False) if show_progress else train_loader
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ç•ªå·ã®è¿½è·¡ç”¨
        pattern_counter = 0
        
        # 1. ãƒŸãƒ‹ãƒãƒƒãƒå˜ä½ã§ã®å­¦ç¿’
        for batch_inputs, batch_labels in batch_iterator:
            batch_errors = []
            
            # ãƒãƒƒãƒå†…å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å­¦ç¿’å®Ÿè¡Œ
            for i, (input_data, label) in enumerate(zip(batch_inputs, batch_labels)):
                # å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æº–å‚™
                indata_input = [input_data[j] for j in range(self.input_units // 2)]
                indata_tch = [0.0] * self.output_units
                indata_tch[int(label)] = 1.0  # One-hot encoding
                
                # å­¦ç¿’å®Ÿè¡Œï¼ˆed_genuine.prompt.mdæº–æ‹ ã®ã‚¢ãƒŸãƒ³æ‹¡æ•£å­¦ç¿’ï¼‰
                self.neuro_calc(indata_input, indata_tch)
                
                # å­¦ç¿’å¾Œã«äºˆæ¸¬å®Ÿè¡Œï¼ˆed_genuine.prompt.mdæº–æ‹ ã®é †æ–¹å‘è¨ˆç®—ï¼‰
                prediction = self.predict(indata_input)
                predicted_class = np.argmax(prediction)
                true_class = int(label)
                correct = (predicted_class == true_class)
                # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—: indata_tchã¯ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆå½¢å¼
                error = sum(abs(indata_tch[j] - prediction[j]) for j in range(self.output_units)) / self.output_units
                
                # çµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«å³åº§ã«ä¿å­˜ï¼ˆæ··åŒè¡Œåˆ—ç”¨ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰
                results_buffer.record_train_result(epoch, pattern_counter, correct, float(error), 
                                                 int(predicted_class), true_class)
                
                batch_errors.append(error)
                pattern_counter += 1
            
            # ğŸ¯ ã€v0.1.6æ–°æ©Ÿèƒ½ã€‘3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„èª¤å·®è¨˜éŒ²
            # ed_genuine.prompt.mdæº–æ‹ ï¼šãƒãƒƒãƒå˜ä½ã§ã®é«˜é€Ÿèª¤å·®é›†è¨ˆ
            results_buffer.record_train_batch_error_efficient(
                epoch, 
                np.array(batch_errors), 
                len(batch_inputs)
            )
            
            # ãƒãƒƒãƒãƒ¬ãƒ™ãƒ«ã®èª¤å·®é›†è¨ˆï¼ˆå¾“æ¥æ–¹å¼ï¼šä¸‹ä½äº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰
            batch_avg_error = np.mean(batch_errors)
            self.error += batch_avg_error * len(batch_inputs)
        
        # 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®çµæœä¿å­˜ï¼ˆä¸€æ‹¬å‡¦ç† + åŠ¹ç‡çš„èª¤å·®è¨˜éŒ²ï¼‰
        test_predictions = self.predict_batch(test_inputs)
        test_errors = []
        
        for i, (prediction, test_label) in enumerate(zip(test_predictions, test_labels)):
            predicted_class = np.argmax(prediction)
            true_class = int(test_label)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—: çµ±ä¸€ã•ã‚ŒãŸè¨ˆç®—æ–¹å¼ã‚’ä½¿ç”¨
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - prediction[j]) for j in range(self.output_units)) / self.output_units
            
            # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜ï¼ˆæ··åŒè¡Œåˆ—ç”¨ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰
            results_buffer.record_test_result(epoch, i, correct, float(error), 
                                            int(predicted_class), true_class)
            test_errors.append(error)
        
        # ğŸ¯ ã€v0.1.6æ–°æ©Ÿèƒ½ã€‘ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„èª¤å·®è¨˜éŒ²
        # ed_genuine.prompt.mdæº–æ‹ ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬é«˜é€Ÿèª¤å·®é›†è¨ˆ
        results_buffer.record_test_batch_error_efficient(
            epoch,
            np.array(test_errors),
            len(test_inputs)
        )
        
        avg_error = float(self.error / pattern_counter) if pattern_counter > 0 else 0.0
        return avg_error, self.error_count

    def record_epoch_results(self, results_buffer: 'LearningResultsBuffer', epoch: int, 
                           train_inputs: np.ndarray, train_labels: np.ndarray,
                           test_inputs: np.ndarray, test_labels: np.ndarray):
        """
        ã‚¨ãƒãƒƒã‚¯çµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«é«˜é€Ÿè¨˜éŒ²ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
        predict_batchã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡çš„ã«çµæœã‚’è¨˜éŒ²
        """
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿çµæœè¨˜éŒ²
        train_predictions = self.predict_batch(train_inputs)
        for i, (pred, label) in enumerate(zip(train_predictions, train_labels)):
            predicted_class = np.argmax(pred)
            true_class = int(label)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - pred[j]) for j in range(self.output_units)) / self.output_units
            results_buffer.record_train_result(epoch, i, correct, float(error), int(predicted_class), true_class)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿çµæœè¨˜éŒ²
        test_predictions = self.predict_batch(test_inputs) 
        for i, (pred, label) in enumerate(zip(test_predictions, test_labels)):
            predicted_class = np.argmax(pred)
            true_class = int(label)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - pred[j]) for j in range(self.output_units)) / self.output_units
            results_buffer.record_test_result(epoch, i, correct, float(error), int(predicted_class), true_class)
    
    def _onehot_encode(self, label: int, num_classes: int) -> np.ndarray:
        """ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è£œåŠ©é–¢æ•°"""
        onehot = np.zeros(num_classes)
        onehot[label] = 1.0
        return onehot

    def predict(self, input_pattern: List[float]) -> List[float]:
        """äºˆæ¸¬å®Ÿè¡Œ"""
        self.neuro_output_calc(input_pattern)
        
        results = []
        for n in range(self.output_units):
            results.append(self.output_outputs[n][self.input_units + 2])
        
        return results
    
    def predict_batch(self, input_patterns: np.ndarray) -> np.ndarray:
        """
        ãƒãƒƒãƒäºˆæ¸¬å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ–ç”¨ï¼‰
        ed_genuine.prompt.mdæº–æ‹ : é †æ–¹å‘è¨ˆç®—ã®æœ¬è³ªã¯ç¶­æŒã—ã¤ã¤æœ€é©åŒ–
        """
        predictions = []
        for pattern in input_patterns:
            prediction = self.predict(pattern.tolist())
            predictions.append(prediction)
        return np.array(predictions)
    
    def calculate_accuracy_and_error(self, inputs: np.ndarray, labels: np.ndarray) -> tuple:
        """
        ç²¾åº¦ã¨èª¤å·®ã®åŠ¹ç‡çš„è¨ˆç®—
        ed_genuine.prompt.mdæº–æ‹ : è¨ˆç®—æ–¹æ³•ã¯ç¶­æŒã€å®Ÿè£…æœ€é©åŒ–
        """
        predictions = self.predict_batch(inputs)
        
        # ç²¾åº¦è¨ˆç®—
        predicted_classes = np.argmax(predictions, axis=1)
        correct = np.sum(predicted_classes == labels)
        accuracy = correct / len(labels)
        
        # èª¤å·®è¨ˆç®—ï¼ˆEDæ³•æº–æ‹ ã®çµ¶å¯¾å€¤èª¤å·®ï¼‰
        true_outputs = np.zeros((len(labels), 10))
        true_outputs[np.arange(len(labels)), labels] = 1.0
        error = np.mean(np.sum(np.abs(predictions - true_outputs), axis=1)) / self.output_units
        
        return accuracy, error
    
    def _precompute_nonzero_weights(self):
        """
        éã‚¼ãƒ­é‡ã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’äº‹å‰è¨ˆç®—ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
        ed_genuine.prompt.mdæº–æ‹ : w_ot_ot[n][k][m] != 0 ã®å ´åˆã®ã¿å‡¦ç†
        """
        if self.hyperparams.verbose:
            print("ğŸ“Š éã‚¼ãƒ­é‡ã¿è§£æä¸­...")
        self.nonzero_weight_indices = []
        
        total_weights = 0
        nonzero_weights = 0
        
        for n in range(self.output_units):
            n_indices = []
            for k in range(self.input_units + 2, self.total_units + 2):
                k_indices = []
                for m in range(self.total_units + 2):
                    total_weights += 1
                    if self.output_weights[n][k][m] != 0:
                        k_indices.append(m)
                        nonzero_weights += 1
                n_indices.append(k_indices)
            self.nonzero_weight_indices.append(n_indices)
        
        sparsity = (total_weights - nonzero_weights) / total_weights * 100
        if self.hyperparams.verbose:
            print(f"âœ… é‡ã¿ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§è§£æå®Œäº†:")
            print(f"   ç·é‡ã¿: {total_weights:,}")  
            print(f"   éã‚¼ãƒ­: {nonzero_weights:,}")
            print(f"   ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§: {sparsity:.1f}%")
            print(f"   äºˆæƒ³é«˜é€ŸåŒ–: {total_weights/max(1, nonzero_weights):.1f}å€")
    
    def get_network_status(self) -> dict:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ…‹ã®å–å¾—"""
        # é‡ã¿çµ±è¨ˆ
        total_weights = 0
        active_weights = 0
        weight_sum = 0.0
        min_weight = float('inf')
        max_weight = float('-inf')
        weight_values = []
        
        for n in range(self.output_units):
            for k in range(self.input_units + 2, self.total_units + 2):
                for m in range(self.total_units + 2):
                    total_weights += 1
                    weight_val = self.output_weights[n][k][m]
                    if weight_val != 0:
                        active_weights += 1
                        weight_sum += abs(weight_val)
                        weight_values.append(weight_val)
                        min_weight = min(min_weight, weight_val)
                        max_weight = max(max_weight, weight_val)
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é‡ã¿ãŒãªã„å ´åˆã®å‡¦ç†
        if active_weights == 0:
            min_weight = 0.0
            max_weight = 0.0
        
        return {
            'network_config': {
                'input_units': self.input_units,
                'hidden_units': self.hidden_units,
                'output_units': self.output_units,
                'total_units': self.total_units + 2
            },
            'weight_statistics': {
                'total_weights': total_weights,
                'active_weights': active_weights,
                'avg_weight': weight_sum / active_weights if active_weights > 0 else 0.0,
                'min_weight': min_weight,
                'max_weight': max_weight,
                'mean_weight': sum(weight_values) / len(weight_values) if weight_values else 0.0
            },
            'parameters': {
                'alpha': self.learning_rate,
                'beta': self.initial_amine,
            }
        }
