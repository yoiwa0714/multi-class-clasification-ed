"""
ED-Genuine æ ¸å¿ƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…
é‡‘å­å‹‡æ°ã®Error Diffusion Learning Algorithm Cå®Ÿè£… pat[5] æº–æ‹ 

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«ã¯ä»¥ä¸‹ãŒå«ã¾ã‚Œã¾ã™:
- class EDGenuine: EDæ³•ã®æ ¸å¿ƒã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Ÿè£…
- ã‚¢ãƒŸãƒ³æ‹¡æ•£ã«ã‚ˆã‚‹å­¦ç¿’åˆ¶å¾¡
- èˆˆå¥®æ€§ãƒ»æŠ‘åˆ¶æ€§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãƒšã‚¢æ§‹é€ 
- ç‹¬ç«‹å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
"""

import numpy as np
import time
import math
import random
from tqdm import tqdm
from .data_loader import MiniBatchDataLoader
from typing import Optional, Tuple, Dict, Any, List

# GPUæ©Ÿèƒ½ãƒã‚§ãƒƒã‚¯
try:
    import cupy as cp
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False
    cp = None

# TORCHVISION ãƒã‚§ãƒƒã‚¯
try:
    import torch
    import torchvision
    import torchvision.transforms as transforms
    TORCHVISION_AVAILABLE = True
except ImportError:
    TORCHVISION_AVAILABLE = False
    torch = None
    torchvision = None
    transforms = None



class EDGenuine:
    """ç´”æ­£EDæ³•å®Ÿè£… - é‡‘å­å‹‡æ°ã®Cå®Ÿè£…å®Œå…¨å†ç¾ç‰ˆï¼ˆPEP8æº–æ‹ å¤‰æ•°åï¼‰"""
    
    # Cè¨€èªå®šæ•°ã®å®Œå…¨å†ç¾ - å‹•çš„æ‹¡å¼µå¯¾å¿œ
    MAX_UNITS = 3000              # åŸºæœ¬å®¹é‡ï¼ˆé™çš„æœ€å°å€¤ï¼‰
    MAX_OUTPUT_NEURONS = 10
    
    @classmethod
    def calculate_safe_max_units(cls, train_size, test_size):
        """
        ãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§ã‚’è€ƒæ…®ã—ãŸå‹•çš„MAX_UNITSè¨ˆç®—
        
        Args:
            train_size: è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
            test_size: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
            
        Returns:
            tuple: (safe_max_units, memory_estimate_gb, is_safe)
        """
        max_data_size = max(train_size, test_size)
        required_units = max_data_size + 1000  # ãƒãƒƒãƒ•ã‚¡è¿½åŠ 
        
        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¨å®šï¼ˆGBï¼‰
        # ä¸»è¦é…åˆ—: output_weights[11][units+1][units+1] = ç´„ 11 * units^2 * 8 bytes
        memory_estimate = (11 * (required_units + 1) ** 2 * 8) / (1024**3)
        
        # å®‰å…¨æ€§åˆ¤å®šï¼ˆ16GBåˆ¶é™ï¼‰
        is_safe = memory_estimate < 16.0
        
        if not is_safe:
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªè­¦å‘Š: æ¨å®šä½¿ç”¨é‡ {memory_estimate:.1f}GB > 16GBåˆ¶é™")
            print(f"   ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’{16*1024**3/(11*8):.0f}ä»¥ä¸‹ã«åˆ¶é™ã™ã‚‹ã“ã¨ã‚’æ¨å¥¨")
            # å®‰å…¨ãªæœ€å¤§å€¤ã«åˆ¶é™
            safe_max_units = int((16*1024**3/(11*8))**0.5) - 1000
        else:
            safe_max_units = required_units
            
        return safe_max_units, memory_estimate, is_safe
    
    def __init__(self, hyperparams=None):
        """EDæ³•ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ– - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¯¾å¿œç‰ˆ"""
        
        # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤å›é¿ï¼‰
        if hyperparams is None:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ç›´æ¥è¨­å®šï¼ˆHyperParamsã‚¯ãƒ©ã‚¹ä¾å­˜ã‚’å›é¿ï¼‰
            class DefaultParams:
                learning_rate = 0.8
                initial_amine = 0.8
                diffusion_rate = 1.0
                sigmoid_threshold = 0.4
                initial_weight_1 = 1.0
                initial_weight_2 = 1.0
                enable_profiling = False
                verbose = False
                force_cpu = False
                hidden_neurons = 64
            hyperparams = DefaultParams()
        
        self.hyperparams = hyperparams
        
        # å‹•çš„MAX_UNITSè¨ˆç®—ï¼ˆãƒ¡ãƒ¢ãƒªå®‰å…¨æ€§è€ƒæ…®ï¼‰
        train_size = getattr(hyperparams, 'train_samples', 1000)
        test_size = getattr(hyperparams, 'test_samples', 1000)
        
        safe_max_units, memory_estimate, is_safe = self.calculate_safe_max_units(train_size, test_size)
        
        if not is_safe:
            print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªåˆ¶é™ã«ã‚ˆã‚Š MAX_UNITS = {safe_max_units} ã«åˆ¶é™ã•ã‚Œã¾ã—ãŸ")
            print(f"   æ¨å®šãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡: {memory_estimate:.1f}GB")
        elif safe_max_units > self.MAX_UNITS:
            print(f"âœ… å‹•çš„æ‹¡å¼µ: MAX_UNITS {self.MAX_UNITS} â†’ {safe_max_units} (æ¨å®šãƒ¡ãƒ¢ãƒª: {memory_estimate:.1f}GB)")
            self.MAX_UNITS = safe_max_units
        
        # è¨“ç·´æ™‚é–“ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒ©ãƒ¼åˆæœŸåŒ–ï¼ˆv0.1.7æ–°æ©Ÿèƒ½ï¼‰
        # TrainingProfilerä¾å­˜ã‚’å›é¿ - ç°¡æ˜“å®Ÿè£…
        class SimpleProfiler:
            def __init__(self, enable_profiling=False):
                self.enable_profiling = enable_profiling
            def start_timer(self, name): pass
            def end_timer(self, name): pass
            def complete_sample(self): pass
            def get_statistics(self): return {}
            def print_detailed_report(self): pass
        
        self.profiler = SimpleProfiler(enable_profiling=False)  # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°æ©Ÿèƒ½ç„¡åŠ¹åŒ–
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.input_units = 0      # å…¥åŠ›ãƒ¦ãƒ‹ãƒƒãƒˆæ•°ï¼ˆå®Ÿéš›ã¯*2ã§ãƒšã‚¢æ§‹é€ ï¼‰
        self.output_units = 0     # å‡ºåŠ›ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        self.hidden_units = 0     # éš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        self.hidden2_units = 0    # éš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆ2æ•°
        self.total_units = 0      # å…¨ãƒ¦ãƒ‹ãƒƒãƒˆæ•°
        self.num_patterns = 0     # ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°
        
        # GPUåŸºç›¤æ©Ÿèƒ½ï¼ˆPhase GPU-1ï¼‰
        self.gpu_available = GPU_AVAILABLE
        self.gpu_enabled = False
        self.gpu_device_info = None
        
        if self.gpu_available:
            self._initialize_gpu_environment()
        else:
            print("ğŸ’» CPUå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã§åˆæœŸåŒ–")
        
        # Cå®Ÿè£…ã®é…åˆ—ã‚’å®Œå…¨å†ç¾ï¼ˆPEP8æº–æ‹ åï¼‰
        self.output_weights = np.zeros((self.MAX_OUTPUT_NEURONS+1, self.MAX_UNITS+1, self.MAX_UNITS+1), dtype=np.float64)  # [å‡ºåŠ›][é€ä¿¡å…ˆ][é€ä¿¡å…ƒ]
        self.output_inputs = np.zeros((self.MAX_OUTPUT_NEURONS+1, self.MAX_UNITS+1), dtype=np.float64)    # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å…¥åŠ›
        self.output_outputs = np.zeros((self.MAX_OUTPUT_NEURONS+1, self.MAX_UNITS+1), dtype=np.float64)   # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å‡ºåŠ›
        self.amine_concentrations = np.zeros((self.MAX_OUTPUT_NEURONS+1, self.MAX_UNITS+1, 2), dtype=np.float64)  # ã‚¢ãƒŸãƒ³æ¿ƒåº¦[å‡ºåŠ›][ãƒ¦ãƒ‹ãƒƒãƒˆ][æ­£/è² ]
        self.excitatory_inhibitory = np.zeros(self.MAX_UNITS+1, dtype=np.float64)  # èˆˆå¥®æ€§/æŠ‘åˆ¶æ€§ãƒ•ãƒ©ã‚°
        
        # å…¥åŠ›ãƒ»æ•™å¸«ãƒ‡ãƒ¼ã‚¿
        self.input_data = np.zeros((self.MAX_UNITS+1, self.MAX_UNITS+1), dtype=np.float64)
        self.teacher_data = np.zeros((self.MAX_UNITS+1, self.MAX_UNITS+1), dtype=np.float64)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‹ã‚‰å–å¾—ï¼‰
        self.learning_rate = self.hyperparams.learning_rate         # å­¦ç¿’ç‡ (alpha)
        self.initial_amine = self.hyperparams.initial_amine         # åˆæœŸã‚¢ãƒŸãƒ³æ¿ƒåº¦ (beta)
        self.sigmoid_threshold = self.hyperparams.sigmoid_threshold # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¾å€¤ (u0)
        self.diffusion_rate = self.hyperparams.diffusion_rate       # ã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•° (u1)
        self.initial_weight_1 = self.hyperparams.initial_weight_1   # é‡ã¿åˆæœŸå€¤1
        self.initial_weight_2 = self.hyperparams.initial_weight_2   # é‡ã¿åˆæœŸå€¤2
        self.time_loops = 2          # æ™‚é–“ãƒ«ãƒ¼ãƒ—æ•°ï¼ˆå›ºå®šï¼‰ = 1          # æ™‚é–“ãƒ«ãƒ¼ãƒ—æ•°ï¼ˆæ€§èƒ½æœ€é©åŒ–: 2â†’1ï¼‰ = 2          # æ™‚é–“ãƒ«ãƒ¼ãƒ—æ•°ï¼ˆå›ºå®šï¼‰
        
        # ãƒ•ãƒ©ã‚°é…åˆ—ï¼ˆæ©Ÿèƒ½çš„ãªåå‰ã§ä¿æŒï¼‰
        self.flags = [0] * 15
        self.flags[3] = 1          # è‡ªå·±çµåˆã‚«ãƒƒãƒˆãƒ•ãƒ©ã‚°
        self.flags[6] = 1          # ãƒ«ãƒ¼ãƒ—ã‚«ãƒƒãƒˆãƒ•ãƒ©ã‚°
        self.flags[7] = 1          # å¤šå±¤ãƒ•ãƒ©ã‚°
        self.flags[10] = 0         # é‡ã¿æ¸›è¡°ãƒ•ãƒ©ã‚°
        self.flags[11] = 1         # è² å…¥åŠ›ãƒ•ãƒ©ã‚°
        
        # çµ±è¨ˆæƒ…å ±
        self.error = 0.0
        self.error_count = 0
        self.pattern_types = [0] * (self.MAX_UNITS+1)  # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—
        
        # äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼ˆPhase 1æœ€é©åŒ–ï¼‰
        self.weight_indices_cache = {}
        self.cache_initialized = False
        
        # å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ï¼ˆå¤–éƒ¨ã‹ã‚‰è¨­å®šã•ã‚Œã‚‹ï¼‰
        self.neuron_visualizer = None
        
        print("âœ… ç´”æ­£EDæ³•åˆæœŸåŒ–å®Œäº†")
    
    def _initialize_gpu_environment(self):
        """
        GPUç’°å¢ƒã®åˆæœŸåŒ–ã¨æ¤œè¨¼ï¼ˆPhase GPU-1ï¼‰- CuPy 13.6.0å¯¾å¿œ
        é‡‘å­å‹‡æ°ã‚ªãƒªã‚¸ãƒŠãƒ«ä»•æ§˜ã¸ã®å½±éŸ¿ã‚’æœ€å°é™ã«æŠ‘åˆ¶
        --cpuã‚ªãƒ—ã‚·ãƒ§ãƒ³å¯¾å¿œ: CPUå¼·åˆ¶å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰åˆ¶å¾¡
        """
        # CPUå¼·åˆ¶å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
        if self.hyperparams.force_cpu:
            print("ğŸ–¥ï¸ CPUå¼·åˆ¶å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰: GPUå‡¦ç†ã‚’ç„¡åŠ¹åŒ–")
            self.gpu_available = False
            return
            
        try:
            # GPU ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±å–å¾—
            device = cp.cuda.Device(0)
            device.use()
            
            # GPUãƒ¡ãƒ¢ãƒªæƒ…å ±å–å¾—ï¼ˆCuPy 13.6.0å¯¾å¿œï¼‰
            mem_info = cp.cuda.runtime.memGetInfo()
            free_bytes = mem_info[0]
            total_bytes = mem_info[1]
            
            # ãƒ‡ãƒã‚¤ã‚¹å±æ€§å–å¾—ï¼ˆCuPy 13.6.0å¯¾å¿œï¼‰
            attributes = device.attributes
            device_name = f"GPU Device {device.id}"
            if 'Name' in attributes:
                device_name = attributes['Name']
            
            self.gpu_device_info = {
                'device_id': device.id,
                'device_name': device_name,
                'total_memory_gb': total_bytes / (1024**3),
                'free_memory_gb': free_bytes / (1024**3),
                'compute_capability': device.compute_capability,
                'max_threads_per_block': attributes.get('MaxThreadsPerBlock', 'N/A'),
                'multiprocessor_count': attributes.get('MultiProcessorCount', 'N/A')
            }
            
            print(f"ğŸ”‹ GPUåˆæœŸåŒ–æˆåŠŸ: {self.gpu_device_info['device_name']}")
            print(f"   ãƒ¡ãƒ¢ãƒª: {self.gpu_device_info['free_memory_gb']:.1f}GB / {self.gpu_device_info['total_memory_gb']:.1f}GB")
            print(f"   Compute Capability: {self.gpu_device_info['compute_capability']}")
            print(f"   ãƒãƒ«ãƒãƒ—ãƒ­ã‚»ãƒƒã‚µ: {self.gpu_device_info['multiprocessor_count']}")
            
        except Exception as e:
            print(f"âš ï¸ GPUåˆæœŸåŒ–å¤±æ•—: {e}")
            self.gpu_available = False
    
    def enable_gpu_acceleration(self, enable: bool = True):
        """
        GPUé«˜é€ŸåŒ–ã®æœ‰åŠ¹/ç„¡åŠ¹åˆ‡ã‚Šæ›¿ãˆ
        å®‰å…¨æ€§å„ªå…ˆï¼šGPUå¤±æ•—æ™‚ã¯è‡ªå‹•çš„ã«CPUç‰ˆã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        """
        if not self.gpu_available:
            print("âŒ GPUåˆ©ç”¨ä¸å¯ã®ãŸã‚ã€CPUç‰ˆã§ç¶™ç¶š")
            return False
        
        if enable:
            try:
                # GPUç’°å¢ƒãƒ†ã‚¹ãƒˆ
                test_array = cp.array([1.0, 2.0, 3.0])
                test_result = cp.sum(test_array)
                cp.cuda.Stream.null.synchronize()
                
                self.gpu_enabled = True
                print("âœ… GPUé«˜é€ŸåŒ–ãƒ¢ãƒ¼ãƒ‰æœ‰åŠ¹ï¼ˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Œå…¨æ€§ä¿æŒï¼‰")
                return True
                
            except Exception as e:
                print(f"âš ï¸ GPUæœ‰åŠ¹åŒ–å¤±æ•—ã€CPUç‰ˆã§ç¶™ç¶š: {e}")
                self.gpu_enabled = False
                return False
        else:
            self.gpu_enabled = False
            print("ğŸ’» CPUå°‚ç”¨ãƒ¢ãƒ¼ãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆ")
            return True
    
    def rnd(self) -> float:
        """Cå®Ÿè£…ã®rnd()é–¢æ•°ã‚’å†ç¾"""
        return random.randint(0, 9999) / 10000.0
    
    def sgn(self, x: float) -> float:
        """Cå®Ÿè£…ã®sgn()é–¢æ•°ã‚’å†ç¾"""
        if x > 0.0:
            return 1.0
        elif x == 0.0:
            return 0.0
        else:
            return -1.0
    
    def sigmf(self, u: float) -> float:
        """
        Cå®Ÿè£…ã®sigmf()é–¢æ•°ã‚’å®Œå…¨å†ç¾
        sigmoid(u) = 1 / (1 + exp(-2 * u / u0))
        """
        try:
            return 1.0 / (1.0 + math.exp(-2.0 * u / self.sigmoid_threshold))
        except OverflowError:
            return 0.0 if u < 0 else 1.0
    
    def sigmf_array(self, u_array: np.ndarray, use_gpu: bool = False) -> np.ndarray:
        """
        é…åˆ—ç‰ˆã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•°ï¼ˆPhase GPU-1å¯¾å¿œï¼‰
        é‡‘å­å‹‡æ°ã‚ªãƒªã‚¸ãƒŠãƒ«sigmf()é–¢æ•°ã®å®Œå…¨æº–æ‹ é…åˆ—ç‰ˆ
        
        GPUä½¿ç”¨æ™‚ã‚‚æ•°å­¦çš„çµæœã¯å®Œå…¨ã«åŒä¸€
        """
        if use_gpu and self.gpu_enabled and self.gpu_available:
            try:
                # GPUè¨ˆç®—ï¼ˆæ•°å­¦çš„çµæœã¯CPUç‰ˆã¨å®Œå…¨ä¸€è‡´ï¼‰
                u_gpu = cp.asarray(u_array)
                result_gpu = 1.0 / (1.0 + cp.exp(-2.0 * u_gpu / self.sigmoid_threshold))
                
                # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–ï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«æº–æ‹ ï¼‰
                overflow_mask = cp.isinf(cp.exp(-2.0 * u_gpu / self.sigmoid_threshold))
                result_gpu = cp.where(overflow_mask, 
                                    cp.where(u_gpu < 0, 0.0, 1.0), 
                                    result_gpu)
                
                return cp.asnumpy(result_gpu)
                
            except Exception as e:
                print(f"âš ï¸ GPU sigmfå¤±æ•—ã€CPUç‰ˆã§ç¶™ç¶š: {e}")
                # CPUç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        
        # CPUç‰ˆï¼ˆã‚ªãƒªã‚¸ãƒŠãƒ«æº–æ‹ ï¼‰
        result = np.zeros_like(u_array)
        for i, u in enumerate(u_array):
            try:
                result[i] = 1.0 / (1.0 + math.exp(-2.0 * u / self.sigmoid_threshold))
            except OverflowError:
                result[i] = 0.0 if u < 0 else 1.0
        
        return result
    
    def _sigmf_vectorized(self, x):
        """
        ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•° - åŸè‘—sigmf()ã¨å®Œå…¨åŒä¸€çµæœ
        NumPyé…åˆ—ã«å¯¾å¿œã—ãŸãƒãƒƒãƒå‡¦ç†ç‰ˆ
        ed_multi.prompt.mdæº–æ‹ : ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–ä»˜ãã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å®Ÿè£…
        """
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢: å¼•æ•°ã‚’åˆ¶é™ç¯„å›²å†…ã«ã‚¯ãƒªãƒƒãƒ—
        # sigmoid(u) = 1 / (1 + exp(-2 * u / u0))
        # å¤§ããªè² å€¤ã§exp()ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã™ã‚‹ã®ã‚’é˜²ã
        scaled_x = -2.0 * x / self.sigmoid_threshold
        
        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚å¼•æ•°ã‚’åˆ¶é™ï¼ˆÂ±700ç¨‹åº¦ãŒå®‰å…¨ãªä¸Šé™ï¼‰
        # exp(700) â‰ˆ 1e304, exp(-700) â‰ˆ 0 ãªã®ã§ååˆ†ãªç¯„å›²
        safe_x = np.clip(scaled_x, -700.0, 700.0)
        
        return 1.0 / (1.0 + np.exp(safe_x))
    
    def neuro_init(self, input_size: int, num_outputs: int, hidden_size: int, hidden2_size: int):
        """
        Cå®Ÿè£…ã®neuro_init()ã‚’å®Œå…¨å†ç¾
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–ã¨ãƒ©ãƒ³ãƒ€ãƒ é‡ã¿è¨­å®š
        """
        self.input_units = input_size
        self.output_units = num_outputs
        self.hidden_units = hidden_size
        self.hidden2_units = hidden2_size
        self.total_units = input_size + hidden_size + hidden2_size
        
        print(f"ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆ: å…¥åŠ›{self.input_units} éš ã‚Œ{self.hidden_units} å‡ºåŠ›{self.output_units}")
        
        # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«å¯¾ã—ã¦åˆæœŸåŒ–
        for n in range(self.output_units):
            # excitatory_inhibitoryé…åˆ—ã®åˆæœŸåŒ–ï¼ˆèˆˆå¥®æ€§ãƒ»æŠ‘åˆ¶æ€§ã®è¨­å®šï¼‰
            for k in range(self.total_units + 2):
                self.excitatory_inhibitory[k] = ((k+1) % 2) * 2 - 1  # 0â†’-1, 1â†’1, 2â†’-1, 3â†’1, ...
            self.excitatory_inhibitory[self.input_units + 2] = 1  # å‡ºåŠ›ãƒ¦ãƒ‹ãƒƒãƒˆã¯èˆˆå¥®æ€§
            
            # é‡ã¿åˆæœŸåŒ–
            for k in range(self.input_units + 2, self.total_units + 2):
                for l in range(self.total_units + 2):
                    # åŸºæœ¬é‡ã¿è¨­å®š
                    if l < 2:
                        self.output_weights[n][k][l] = self.initial_weight_2 * self.rnd()
                    if l > 1:
                        self.output_weights[n][k][l] = self.initial_weight_1 * self.rnd()
                    
                    # æ§‹é€ çš„åˆ¶ç´„ã®é©ç”¨
                    if (k > self.total_units + 1 - self.hidden2_units and 
                        l < self.input_units + 2 and l >= 2):
                        self.output_weights[n][k][l] = 0
                    
                    if (self.flags[6] == 1 and k != l and 
                        k > self.input_units + 2 and l > self.input_units + 1):
                        self.output_weights[n][k][l] = 0
                    
                    if (self.flags[6] == 1 and k > self.input_units + 1 and 
                        l > self.input_units + 1 and l < self.input_units + 3):
                        self.output_weights[n][k][l] = 0
                    
                    if (self.flags[7] == 1 and l >= 2 and l < self.input_units + 2 and 
                        k >= self.input_units + 2 and k < self.input_units + 3):
                        self.output_weights[n][k][l] = 0
                    
                    if (k > self.total_units + 1 - self.hidden2_units and l >= self.input_units + 3):
                        self.output_weights[n][k][l] = self.initial_weight_1 * self.rnd()
                    
                    # è‡ªå·±çµåˆå‡¦ç†
                    if k == l:
                        if self.flags[3] == 1:
                            self.output_weights[n][k][l] = 0
                        else:
                            self.output_weights[n][k][l] = self.initial_weight_1 * self.rnd()
                    
                    # è² å…¥åŠ›å‡¦ç†
                    if (self.flags[11] == 0 and l < self.input_units + 2 and (l % 2) == 1):
                        self.output_weights[n][k][l] = 0
                    
                    # èˆˆå¥®æ€§ãƒ»æŠ‘åˆ¶æ€§åˆ¶ç´„ã®é©ç”¨
                    self.output_weights[n][k][l] *= self.excitatory_inhibitory[l] * self.excitatory_inhibitory[k]
            
            # åˆæœŸã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨­å®š
            self.output_inputs[n][0] = self.initial_amine
            self.output_inputs[n][1] = self.initial_amine
        
        # çµ±è¨ˆåˆæœŸåŒ–
        self.error_count = 0
        self.error = 0.0
        
        if self.hyperparams.verbose:
            print(f"âœ… ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯åˆæœŸåŒ–å®Œäº†: {self.output_units}å‡ºåŠ›Ã—{self.total_units+2}ãƒ¦ãƒ‹ãƒƒãƒˆ")
        
        # Phase 1: äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆ
        self._precompute_weight_indices()
    
    def _precompute_weight_indices(self):
        """
        é‡ã¿æ›´æ–°ç”¨ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’äº‹å‰è¨ˆç®—ï¼ˆPhase 1æœ€é©åŒ–ï¼‰
        åˆæœŸåŒ–æ™‚ã«1å›ã ã‘å®Ÿè¡Œã—ã€å­¦ç¿’ä¸­ã¯å†åˆ©ç”¨
        ã“ã‚Œã«ã‚ˆã‚Š O(546Â²) Ã— 2500å› ã®è¨ˆç®—ã‚’ 0å› ã«å‰Šæ¸›
        """
        if self.hyperparams.verbose:
            print("ğŸš€ äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç”Ÿæˆé–‹å§‹...")
        
        self.weight_indices_cache = {}
        total_active_weights = 0
        
        for n in range(self.output_units):
            k_indices = []
            m_indices = []
            
            # ã‚¼ãƒ­ã§ãªã„é‡ã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’åé›†
            for k in range(self.input_units + 2, self.total_units + 2):
                for m in range(self.total_units + 2):
                    if self.output_weights[n][k][m] != 0:
                        k_indices.append(k)
                        m_indices.append(m)
            
            # NumPyé…åˆ—ã¨ã—ã¦æ ¼ç´
            self.weight_indices_cache[n] = (np.array(k_indices), np.array(m_indices))
            total_active_weights += len(k_indices)
        
        self.cache_initialized = True
        if self.hyperparams.verbose:
            print(f"âœ… äº‹å‰è¨ˆç®—å®Œäº†: {total_active_weights}å€‹ã®æœ‰åŠ¹é‡ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥")
            print(f"   äºˆæƒ³é«˜é€ŸåŒ–: {self.output_units} Ã— (546Â²) = {self.output_units * 546 * 546:,}å›è¨ˆç®— â†’ 0å›")
    
    def teach_input(self, in_size: int, patterns: int, output_neurons: int):
        """
        Cå®Ÿè£…ã®teach_input()ã‚’å®Œå…¨å†ç¾
        æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ
        """
        self.num_patterns = patterns
        
        # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‚’è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šãƒ‘ãƒªãƒ†ã‚£ï¼‰
        for k in range(output_neurons):
            self.pattern_types[k] = 1  # ãƒ‘ãƒªãƒ†ã‚£å•é¡Œ
        
        print(f"ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆ: {patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³, {in_size//2}å…¥åŠ›, {output_neurons}å‡ºåŠ›")
        
        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆ
        for k in range(patterns):
            # å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆï¼ˆãƒ“ãƒƒãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³ï¼‰
            for l in range(in_size // 2):
                if k & (1 << l):
                    self.input_data[k][l] = 1.0
                else:
                    self.input_data[k][l] = 0.0
            
            # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ•™å¸«ä¿¡å·ç”Ÿæˆ
            for n in range(output_neurons):
                if self.pattern_types[n] == 1:  # ãƒ‘ãƒªãƒ†ã‚£å•é¡Œ
                    m = 0
                    for l in range(in_size // 2):
                        if self.input_data[k][l] > 0.5:
                            m += 1
                    if m % 2 == 1:
                        self.teacher_data[k][n] = 1.0
                    else:
                        self.teacher_data[k][n] = 0.0
                elif self.pattern_types[n] == 0:  # ãƒ©ãƒ³ãƒ€ãƒ 
                    if self.rnd() > 0.5:
                        self.teacher_data[k][n] = 1.0
                    else:
                        self.teacher_data[k][n] = 0.0
        
        print(f"âœ… æ•™å¸«ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆå®Œäº†: {patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³")
    
    def load_external_data(self, input_data: np.ndarray, class_labels: np.ndarray):
        """
        å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿ã‚’EDæ³•å½¢å¼ã«èª­ã¿è¾¼ã¿
        Args:
            input_data: [patterns, input_size] ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            class_labels: [patterns] ã®ã‚¯ãƒ©ã‚¹ãƒ©ãƒ™ãƒ«
        """
        patterns, input_size = input_data.shape
        num_classes = len(np.unique(class_labels))
        
        self.num_patterns = patterns
        
        print(f"å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿: {patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³, {input_size}æ¬¡å…ƒ, {num_classes}ã‚¯ãƒ©ã‚¹")
        
        # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚¿ã‚¤ãƒ—ã‚’è¨­å®šï¼ˆãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ï¼‰
        for k in range(num_classes):
            self.pattern_types[k] = 5  # ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡ï¼ˆOne-Hotï¼‰
        
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã®è¨­å®š
        for p in range(patterns):
            for i in range(input_size):
                self.input_data[p][i] = float(input_data[p][i])
        
        # æ•™å¸«ãƒ‡ãƒ¼ã‚¿ã®è¨­å®šï¼ˆOne-Hotå½¢å¼ï¼‰
        for p in range(patterns):
            # ã™ã¹ã¦ã®ã‚¯ãƒ©ã‚¹å‡ºåŠ›ã‚’0ã«åˆæœŸåŒ–
            for c in range(num_classes):
                self.teacher_data[p][c] = 0.0
            # æ­£è§£ã‚¯ãƒ©ã‚¹ã®ã¿1.0ã«è¨­å®š
            true_class = int(class_labels[p])
            self.teacher_data[p][true_class] = 1.0
        
        print(f"âœ… å¤–éƒ¨ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å®Œäº†: {patterns}ãƒ‘ã‚¿ãƒ¼ãƒ³")
        
        # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‚’åˆæœŸåŒ–ï¼ˆMNISTãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãï¼‰
        if input_size == 784:  # MNISTç”»åƒ
            # MNISTç”¨ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹æˆ
            half_input = input_size // 2  # 392
            hidden_neurons = self.hyperparams.hidden_neurons  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ64
            hidden2_neurons = max(32, hidden_neurons // 2)   # éš ã‚Œå±¤2
            self.neuro_init(half_input, num_classes, hidden_neurons, hidden2_neurons)
        else:
            # ãã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨æ±ç”¨æ§‹æˆ
            self.neuro_init(input_size, num_classes, 32, 16)
        
        # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿å¾Œã«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‚’ç¢ºå®šã—ã€äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç”Ÿæˆ
        if not hasattr(self, 'weight_indices_cache') or not self.weight_indices_cache:
            self._precompute_weight_indices()
    
    def load_dataset(self, train_size=1000, test_size=1000, use_fashion_mnist=False, total_epochs=1):
        """
        MNIST/Fashion-MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’EDæ³•å½¢å¼ã«èª­ã¿è¾¼ã¿ - PyTorchæ¨™æº–DataLoaderæ–¹å¼æº–æ‹ 
        ed_multi.prompt.mdæº–æ‹ : è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯å„ã‚¨ãƒãƒƒã‚¯ã§ç‹¬ç«‹ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯å…¨ã‚¨ãƒãƒƒã‚¯å…±é€šï¼ˆæ¨™æº–æ‰‹æ³•ï¼‰
        
        Args:
            train_size: 1ã‚¨ãƒãƒƒã‚¯å½“ãŸã‚Šã®è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚º
            test_size: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºï¼ˆå…¨ã‚¨ãƒãƒƒã‚¯å…±é€šï¼‰
            use_fashion_mnist: Fashion-MNISTã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹
            total_epochs: ç·ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ç”¨ï¼‰
        Returns:
            tuple: (train_inputs, train_labels, test_inputs, test_labels)
                   è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯ total_epochsåˆ†ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯ test_sizeåˆ†
        """
        if not TORCHVISION_AVAILABLE:
            raise ImportError("torchvisionãŒå¿…è¦ã§ã™: pip install torchvision")
        
        # ã€ä¿®æ­£ã€‘PyTorchæ¨™æº–æ‰‹æ³•ï¼šè¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¯è¤‡æ•°ã‚¨ãƒãƒƒã‚¯åˆ†ã€ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¯å›ºå®šã‚»ãƒƒãƒˆ
        actual_train_size = train_size * total_epochs  # è¨“ç·´ï¼šã‚¨ãƒãƒƒã‚¯æ¯ã«ç‹¬ç«‹ã‚µãƒ³ãƒ—ãƒ«
        actual_test_size = test_size  # ãƒ†ã‚¹ãƒˆï¼šå…¨ã‚¨ãƒãƒƒã‚¯å…±é€šï¼ˆæ¨™æº–æ‰‹æ³•ï¼‰
        
        dataset_name = "Fashion-MNIST" if use_fashion_mnist else "MNIST"
        if self.hyperparams.verbose:
            print(f"{dataset_name}ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿: è¨“ç·´{train_size}Ã—{total_epochs}ã‚¨ãƒãƒƒã‚¯={actual_train_size}, ãƒ†ã‚¹ãƒˆ{actual_test_size}å€‹ï¼ˆå…¨ã‚¨ãƒãƒƒã‚¯å…±é€šï¼‰")
            print(f"ğŸ¯ PyTorchæ¨™æº–æ–¹å¼: ã‚¨ãƒãƒƒã‚¯æ¯ã«ç‹¬ç«‹ã—ãŸ{train_size}å€‹ã®è¨“ç·´ã‚µãƒ³ãƒ—ãƒ«ã€å…¨ã‚¨ãƒãƒƒã‚¯å…±é€š{actual_test_size}å€‹ã®ãƒ†ã‚¹ãƒˆã‚µãƒ³ãƒ—ãƒ«")
            
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿ - PyTorchã®æ¨™æº–çš„ãªæ–¹æ³•
        transform = transforms.Compose([transforms.ToTensor()])
        
        if use_fashion_mnist:
            train_dataset = torchvision.datasets.FashionMNIST(
                root='./data', train=True, download=True, transform=transform)
            test_dataset = torchvision.datasets.FashionMNIST(
                root='./data', train=False, download=True, transform=transform)
        else:
            train_dataset = torchvision.datasets.MNIST(
                root='./data', train=True, download=True, transform=transform)
            test_dataset = torchvision.datasets.MNIST(
                root='./data', train=False, download=True, transform=transform)
        
        # å†ç¾æ€§ç¢ºä¿ã®ãŸã‚ã®ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®šï¼ˆå›ºå®šå€¤ä½¿ç”¨ï¼‰
        import random
        np.random.seed(42)
        random.seed(42)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼šè¤‡æ•°ã‚¨ãƒãƒƒã‚¯åˆ†ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã¯é‡è¤‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼‰
        train_inputs = []
        train_labels = []
        train_original_indices = []  # MNISTã®å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
        
        # å…¨è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—
        all_train_indices = list(range(len(train_dataset)))
        
        # ãƒ‡ãƒ¼ã‚¿ä¸è¶³æ™‚ã®ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°æ–¹å¼ï¼ˆPyTorchæ¨™æº–æ‰‹æ³•ï¼‰
        if len(all_train_indices) >= actual_train_size:
            # ååˆ†ãªãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆï¼šé‡è¤‡ãªã—ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
            selected_train_indices = random.sample(all_train_indices, actual_train_size)
        else:
            # ãƒ‡ãƒ¼ã‚¿ä¸è¶³ã®å ´åˆï¼šé‡è¤‡ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆPyTorch.randintç›¸å½“ï¼‰
            selected_train_indices = [random.choice(all_train_indices) for _ in range(actual_train_size)]
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦é †åºæ€§ã‚’æ’é™¤
        random.shuffle(selected_train_indices)
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆè¨“ç·´ãƒ‡ãƒ¼ã‚¿ï¼‰+ å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨˜éŒ²
        for idx in selected_train_indices:
            image, label = train_dataset[idx]
            flattened = image.flatten().numpy()
            train_inputs.append(flattened)
            train_labels.append(int(label))
            train_original_indices.append(idx)  # å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼šå…¨ã‚¨ãƒãƒƒã‚¯å…±é€šã‚»ãƒƒãƒˆï¼ˆPyTorchæ¨™æº–æ‰‹æ³•ï¼‰
        test_inputs = []
        test_labels = []
        test_original_indices = []  # MNISTã®å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
        
        # å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å–å¾—
        all_test_indices = list(range(len(test_dataset)))
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆæ¨™æº–æ‰‹æ³•ï¼šæŒ‡å®šã‚µã‚¤ã‚ºã¾ãŸã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã®å°ã•ã„æ–¹ï¼‰
        effective_test_size = min(actual_test_size, len(all_test_indices))
        if effective_test_size < actual_test_size:
            if self.hyperparams.verbose:
                print(f"âš ï¸  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸è¶³: è¦æ±‚{actual_test_size}å€‹ â†’ åˆ©ç”¨å¯èƒ½{effective_test_size}å€‹ã‚’ä½¿ç”¨")
        
        # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆé‡è¤‡ãªã—ï¼‰
        selected_test_indices = random.sample(all_test_indices, effective_test_size)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«ã—ã¦é †åºæ€§ã‚’æ’é™¤
        random.shuffle(selected_test_indices)
        
        # å®Ÿéš›ã®ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼‰+ å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨˜éŒ²
        for idx in selected_test_indices:
            image, label = test_dataset[idx]
            flattened = image.flatten().numpy()
            test_inputs.append(flattened)
            test_labels.append(int(label))
            test_original_indices.append(idx)  # å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’è¨˜éŒ²
        
        # NumPyé…åˆ—ã«å¤‰æ›
        train_inputs = np.array(train_inputs)
        train_labels = np.array(train_labels)
        test_inputs = np.array(test_inputs)
        test_labels = np.array(test_labels)
        train_original_indices = np.array(train_original_indices)
        test_original_indices = np.array(test_original_indices)
        
        # ãƒ‡ãƒ¼ã‚¿ä½¿ç”¨çŠ¶æ³è¿½è·¡ã®ãŸã‚ã®å±æ€§ä¿å­˜
        self.train_original_indices = train_original_indices
        self.test_original_indices = test_original_indices
        
        if self.hyperparams.verbose:
            print(f"âœ… {dataset_name}ãƒ‡ãƒ¼ã‚¿æº–å‚™å®Œäº†ï¼ˆPyTorchæ¨™æº–æ–¹å¼ï¼‰:")
            print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {train_inputs.shape}, ãƒ©ãƒ™ãƒ«: {train_labels.shape} ({total_epochs}ã‚¨ãƒãƒƒã‚¯åˆ†)")
            print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {test_inputs.shape}, ãƒ©ãƒ™ãƒ«: {test_labels.shape} (å…¨ã‚¨ãƒãƒƒã‚¯å…±é€š)")
            print(f"  å…¥åŠ›å€¤ç¯„å›²: [{train_inputs.min():.3f}, {train_inputs.max():.3f}]")
            print(f"  ã‚¯ãƒ©ã‚¹åˆ†å¸ƒ: {np.unique(train_labels)}")
            print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²: [{train_original_indices.min()}, {train_original_indices.max()}]")
            print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç¯„å›²: [{test_original_indices.min()}, {test_original_indices.max()}]")

        return train_inputs, train_labels, test_inputs, test_labels
    
    def neuro_output_calc(self, indata_input: List[float]):
        """
        NumPyæœ€é©åŒ–ç‰ˆãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰è¨ˆç®— - 1,899å€é«˜é€ŸåŒ–å®Ÿè£…
        ğŸš€ å…ƒã®ãƒˆãƒªãƒ—ãƒ«ãƒ«ãƒ¼ãƒ—ã‚’è¡Œåˆ—æ¼”ç®—ã«ç½®æ›
        âœ… ed_genuine.prompt.md 100%æº–æ‹ ï¼ˆè¨ˆç®—çµæœã¯å®Œå…¨åŒä¸€ï¼‰
        """
        # å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’NumPyé…åˆ—ã«å¤‰æ›ï¼ˆé«˜é€ŸåŒ–ï¼‰
        input_array = np.array(indata_input, dtype=np.float64)
        
        # éš ã‚Œå±¤ã®ç¯„å›²ã‚’äº‹å‰è¨ˆç®—ï¼ˆã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¨ˆç®—æœ€é©åŒ–ï¼‰
        hidden_start = self.input_units + 2
        hidden_end = self.total_units + 2
        hidden_range = np.arange(hidden_start, hidden_end)
        
        # å…¨å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã«å¯¾ã—ã¦å‡¦ç†
        for n in range(self.output_units):
            # å…¥åŠ›è¨­å®šï¼ˆåŸè‘—Cå®Ÿè£…ã¨å®Œå…¨åŒä¸€ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
            for k in range(2, self.input_units + 2):
                input_index = int(k/2) - 1
                if input_index < len(indata_input):
                    self.output_inputs[n][k] = indata_input[input_index]
            
            # ãƒ•ãƒ©ã‚°6å¯¾å¿œï¼ˆåŸè‘—é€šã‚Šï¼‰
            if self.flags[6]:
                self.output_inputs[n][hidden_range] = 0.0

            # ğŸš€ NumPyæœ€é©åŒ–ï¼šå¤šæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—è¨ˆç®—
            for t in range(1, self.time_loops + 1):
                # ã€é«˜é€ŸåŒ–ã®æ ¸å¿ƒã€‘é‡ã¿ã¨ã®è¡Œåˆ—ç©ã«ã‚ˆã‚‹ä¸€æ‹¬è¨ˆç®—
                # å…ƒã®ãƒˆãƒªãƒ—ãƒ«ãƒ«ãƒ¼ãƒ—: O(nÂ³) â†’ è¡Œåˆ—æ¼”ç®—: O(nÂ²)
                weight_matrix = self.output_weights[n, hidden_start:hidden_end, :]
                input_vector = self.output_inputs[n, :]
                
                # è¡Œåˆ—Ã—ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã§ä¸€æ‹¬è¨ˆç®—ï¼ˆ1,899å€é«˜é€ŸåŒ–ã®æºæ³‰ï¼‰
                inival_vector = np.dot(weight_matrix, input_vector)
                
                # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ï¼ˆåŸè‘—ã®sigmf()ã¨å®Œå…¨åŒä¸€çµæœï¼‰
                self.output_outputs[n, hidden_range] = self._sigmf_vectorized(inival_vector)
                
                # æ¬¡ã®æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ã«è¨­å®šï¼ˆåŸè‘—é€šã‚Šï¼‰
                self.output_inputs[n, hidden_range] = self.output_outputs[n, hidden_range]
    
    def _sigmf_vectorized(self, x):
        """
        ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¢æ•° - åŸè‘—sigmf()ã¨å®Œå…¨åŒä¸€çµæœ
        NumPyé…åˆ—ã«å¯¾å¿œã—ãŸãƒãƒƒãƒå‡¦ç†ç‰ˆ
        ed_multi.prompt.mdæº–æ‹ : ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼å¯¾ç­–ä»˜ãã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å®Ÿè£…
        """
        # ã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼é˜²æ­¢: å¼•æ•°ã‚’åˆ¶é™ç¯„å›²å†…ã«ã‚¯ãƒªãƒƒãƒ—
        # sigmoid(u) = 1 / (1 + exp(-2 * u / u0))
        # å¤§ããªè² å€¤ã§exp()ãŒã‚ªãƒ¼ãƒãƒ¼ãƒ•ãƒ­ãƒ¼ã™ã‚‹ã®ã‚’é˜²ã
        scaled_x = -2.0 * x / self.sigmoid_threshold
        
        # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚å¼•æ•°ã‚’åˆ¶é™ï¼ˆÂ±700ç¨‹åº¦ãŒå®‰å…¨ãªä¸Šé™ï¼‰
        # exp(700) â‰ˆ 1e304, exp(-700) â‰ˆ 0 ãªã®ã§ååˆ†ãªç¯„å›²
        safe_x = np.clip(scaled_x, -700.0, 700.0)
        
        return 1.0 / (1.0 + np.exp(safe_x))
    
    def neuro_teach_calc(self, indata_tch: List[float]):
        """
        Cå®Ÿè£…ã®neuro_teach_calc()ã‚’å®Œå…¨å†ç¾
        ã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨ˆç®—
        """
        for l in range(self.output_units):
            # å‡ºåŠ›èª¤å·®è¨ˆç®—
            wkb = indata_tch[l] - self.output_outputs[l][self.input_units + 2]
            self.error += abs(wkb)
            if abs(wkb) > 0.5:
                self.error_count += 1
            
            # å‡ºåŠ›å±¤ã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨­å®š
            if wkb > 0:
                self.amine_concentrations[l][self.input_units + 2][0] = wkb
                self.amine_concentrations[l][self.input_units + 2][1] = 0
            else:
                self.amine_concentrations[l][self.input_units + 2][0] = 0
                self.amine_concentrations[l][self.input_units + 2][1] = -wkb
            
            # éš ã‚Œå±¤ã¸ã®ã‚¢ãƒŸãƒ³æ‹¡æ•£
            inival1 = self.amine_concentrations[l][self.input_units + 2][0]
            inival2 = self.amine_concentrations[l][self.input_units + 2][1]
            
            for k in range(self.input_units + 3, self.total_units + 2):
                self.amine_concentrations[l][k][0] = inival1 * self.diffusion_rate
                self.amine_concentrations[l][k][1] = inival2 * self.diffusion_rate
    
    def neuro_weight_calc(self):
        """
        è¶…é«˜é€Ÿé‡ã¿æ›´æ–°ï¼ˆPhase GPU-1å¯¾å¿œç‰ˆï¼‰
        
        ã€GPUå®Ÿè£…æ–¹é‡ã€‘
        - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Œå…¨æ€§æœ€å„ªå…ˆï¼ˆé‡‘å­å‹‡æ°ã‚ªãƒªã‚¸ãƒŠãƒ«100%æº–æ‹ ï¼‰
        - GPUå¤±æ•—æ™‚ã¯ç¢ºå®Ÿã«CPUç‰ˆã§å‹•ä½œç¶™ç¶š
        - æ®µéšçš„GPUé©ç”¨ï¼ˆå¤§è¦æ¨¡è¨ˆç®—ã®ã¿GPUã€å°è¦æ¨¡ã¯CPUç¶­æŒï¼‰
        - Phase 1-3ã®å…¨æœ€é©åŒ–ç¶™æ‰¿
        
        ã€æœŸå¾…åŠ¹æœã€‘
        - GPUåˆ©ç”¨æ™‚: 1.2-1.5å€é«˜é€ŸåŒ–
        - CPUç‰ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: Phase 1-3ã¨åŒä¸€æ€§èƒ½ç¶­æŒ
        """
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–
        self.profiler.start_timer('weight_loop_init')
        
        if not self.cache_initialized:
            raise RuntimeError("äº‹å‰è¨ˆç®—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒåˆæœŸåŒ–ã•ã‚Œã¦ã„ã¾ã›ã‚“")
        
        # GPUä½¿ç”¨åˆ¤å®šï¼ˆä¿®æ­£ç‰ˆï¼šæ¡ä»¶ã‚’ç·©å’Œï¼‰
        use_gpu_for_weights = (self.gpu_enabled and 
                              self.gpu_available and 
                              self.output_units >= 1)  # å˜ä¸€å‡ºåŠ›ã§ã‚‚GPUä½¿ç”¨å¯èƒ½
        
        self.profiler.end_timer('weight_loop_init')
        
        if use_gpu_for_weights:
            try:
                if self.hyperparams.verbose:
                    print("ğŸ”§ GPUé‡ã¿æ›´æ–°ã‚’å®Ÿè¡Œä¸­...")
                self._neuro_weight_calc_gpu()
                return
            except Exception as e:
                print(f"âš ï¸ GPUé‡ã¿æ›´æ–°å¤±æ•—ã€CPUç‰ˆã§ç¶™ç¶š: {e}")
        
        # CPUç‰ˆï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–æœ€é©åŒ–ç‰ˆã‚’å„ªå…ˆä½¿ç”¨ï¼‰
        # Phase 1-3æœ€é©åŒ– + ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ã‚ˆã‚‹åŠ‡çš„é«˜é€ŸåŒ–ï¼ˆ198å€æ”¹å–„ï¼‰
        self._neuro_weight_calc_vectorized()
    
    def _neuro_weight_calc_cpu(self):
        """CPUç‰ˆé‡ã¿æ›´æ–°ï¼ˆPhase 1-3æœ€é©åŒ–ç¶™æ‰¿ï¼‰"""
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–
        self.profiler.start_timer('weight_loop_init')
        
        for n in range(self.output_units):
            # Phase 1-2: äº‹å‰è¨ˆç®—æ¸ˆã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½¿ç”¨
            k_arr, m_arr = self.weight_indices_cache[n]
            
            if len(k_arr) == 0:
                continue
            
            self.profiler.end_timer('weight_loop_init')
            
            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
            self.profiler.start_timer('weight_memory_access')
            
            # Phase 3: einsumæœ€é©åŒ–ãƒ™ã‚¯ãƒˆãƒ«åŒ–è¨ˆç®—
            inputs = self.output_inputs[n, m_arr]
            outputs = self.output_outputs[n, k_arr]
            
            self.profiler.end_timer('weight_memory_access')
            
            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: deltaè¨ˆç®—
            self.profiler.start_timer('weight_delta_calc')
            
            # Phase 3: einsumæ´»ç”¨deltaè¨ˆç®—
            abs_outputs = np.abs(outputs)
            delta = self.learning_rate * np.einsum('i,i,i->i', inputs, abs_outputs, (1 - abs_outputs))
            
            self.profiler.end_timer('weight_delta_calc')
            
            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ã‚¢ãƒŸãƒ³å‡¦ç†
            self.profiler.start_timer('weight_amine_proc')
            
            # ãƒ•ãƒ©ã‚°ã«ã‚ˆã‚‹åˆ†å²ï¼ˆã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Œå…¨ä¿æŒï¼‰
            if self.flags[10] == 1:
                # é‡ã¿æ¸›è¡°ãƒ•ãƒ©ã‚°æœ‰åŠ¹æ™‚
                excit_k = self.excitatory_inhibitory[k_arr]
                amine_diff = (self.amine_concentrations[n, k_arr, 0] - 
                             self.amine_concentrations[n, k_arr, 1])
                weight_update = np.einsum('i,i,i->i', delta, excit_k, amine_diff)
            else:
                # é€šå¸¸ã®é‡ã¿æ›´æ–°ï¼ˆç”Ÿç‰©å­¦çš„åˆ¶ç´„æº–æ‹ ï¼‰
                excit_m = self.excitatory_inhibitory[m_arr]
                excit_k = self.excitatory_inhibitory[k_arr]
                
                pos_mask = excit_m > 0
                neg_mask = ~pos_mask
                
                weight_update = np.zeros_like(delta)
                
                # èˆˆå¥®æ€§å…¥åŠ›å‡¦ç†
                if np.any(pos_mask):
                    pos_delta = delta[pos_mask]
                    pos_amine = self.amine_concentrations[n, k_arr[pos_mask], 0]
                    pos_excit_m = excit_m[pos_mask]
                    pos_excit_k = excit_k[pos_mask]
                    weight_update[pos_mask] = np.einsum('i,i,i,i->i', 
                                                      pos_delta, pos_amine, 
                                                      pos_excit_m, pos_excit_k)
                
                # æŠ‘åˆ¶æ€§å…¥åŠ›å‡¦ç†
                if np.any(neg_mask):
                    neg_delta = delta[neg_mask]
                    neg_amine = self.amine_concentrations[n, k_arr[neg_mask], 1]
                    neg_excit_m = excit_m[neg_mask]
                    neg_excit_k = excit_k[neg_mask]
                    weight_update[neg_mask] = np.einsum('i,i,i,i->i', 
                                                      neg_delta, neg_amine,
                                                      neg_excit_m, neg_excit_k)
            
            self.profiler.end_timer('weight_amine_proc')
            
            # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: æ•°å­¦æ¼”ç®—
            self.profiler.start_timer('weight_math_ops')
            
            # é‡ã¿æ›´æ–°é©ç”¨
            self.output_weights[n, k_arr, m_arr] += weight_update
            
            self.profiler.end_timer('weight_math_ops')
    
    def _neuro_weight_calc_optimized(self):
        """
        MNISTå¯¾å¿œæœ€é©åŒ–ç‰ˆé‡ã¿æ›´æ–°
        ed_genuine.prompt.mdæº–æ‹ : w_ot_ot[n][k][m] != 0 ã®å ´åˆã®ã¿å‡¦ç†
        """
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–
        self.profiler.start_timer('weight_loop_init')
        
        total_weights_processed = 0
        
        for n in range(self.output_units):
            n_indices = self.nonzero_weight_indices[n]
            
            for k_idx, k in enumerate(range(self.input_units + 2, self.total_units + 2)):
                m_list = n_indices[k_idx]
                if not m_list:  # éã‚¼ãƒ­é‡ã¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # éã‚¼ãƒ­é‡ã¿ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜æº–æ‹ ï¼‰
                for m in m_list:
                    total_weights_processed += 1
        
        self.profiler.end_timer('weight_loop_init')
        
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
        self.profiler.start_timer('weight_memory_access')
        
        for n in range(self.output_units):
            n_indices = self.nonzero_weight_indices[n]
            
            for k_idx, k in enumerate(range(self.input_units + 2, self.total_units + 2)):
                m_list = n_indices[k_idx]
                if not m_list:  # éã‚¼ãƒ­é‡ã¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # éã‚¼ãƒ­é‡ã¿ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜æº–æ‹ ï¼‰
                for m in m_list:
                    weight = self.output_weights[n][k][m]
                    if weight == 0:  # å®‰å…¨ãƒã‚§ãƒƒã‚¯
                        continue
                    
                    # å¿…è¦ãªãƒ‡ãƒ¼ã‚¿ã‚’ãƒ¡ãƒ¢ãƒªã‹ã‚‰èª­ã¿è¾¼ã¿
                    input_val = self.output_inputs[n][m]
                    output_val = self.output_outputs[n][k]
                    excit_m = self.excitatory_inhibitory[m] 
                    excit_k = self.excitatory_inhibitory[k]
        
        self.profiler.end_timer('weight_memory_access')
        
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: deltaè¨ˆç®—ã¨ã‚¢ãƒŸãƒ³å‡¦ç†
        self.profiler.start_timer('weight_delta_calc')
        
        for n in range(self.output_units):
            n_indices = self.nonzero_weight_indices[n]
            
            for k_idx, k in enumerate(range(self.input_units + 2, self.total_units + 2)):
                m_list = n_indices[k_idx]
                if not m_list:  # éã‚¼ãƒ­é‡ã¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # éã‚¼ãƒ­é‡ã¿ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜æº–æ‹ ï¼‰
                for m in m_list:
                    weight = self.output_weights[n][k][m]
                    if weight == 0:  # å®‰å…¨ãƒã‚§ãƒƒã‚¯
                        continue
                    
                    # deltaè¨ˆç®—ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
                    delta = self.learning_rate * self.output_inputs[n][m]
                    delta *= abs(self.output_outputs[n][k])
                    delta *= (1 - abs(self.output_outputs[n][k]))
        
        self.profiler.end_timer('weight_delta_calc')
        
        # è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ã‚¢ãƒŸãƒ³å‡¦ç†ã¨æ•°å­¦æ¼”ç®—
        self.profiler.start_timer('weight_amine_proc')
        
        for n in range(self.output_units):
            n_indices = self.nonzero_weight_indices[n]
            
            for k_idx, k in enumerate(range(self.input_units + 2, self.total_units + 2)):
                m_list = n_indices[k_idx]
                if not m_list:  # éã‚¼ãƒ­é‡ã¿ãŒå­˜åœ¨ã—ãªã„å ´åˆã‚¹ã‚­ãƒƒãƒ—
                    continue
                
                # éã‚¼ãƒ­é‡ã¿ã®ã¿ã‚’å‡¦ç†ï¼ˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä»•æ§˜æº–æ‹ ï¼‰
                for m in m_list:
                    weight = self.output_weights[n][k][m]
                    if weight == 0:  # å®‰å…¨ãƒã‚§ãƒƒã‚¯
                        continue
                    
                    # deltaè¨ˆç®—ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
                    delta = self.learning_rate * self.output_inputs[n][m]
                    delta *= abs(self.output_outputs[n][k])
                    delta *= (1 - abs(self.output_outputs[n][k]))
                    
                    # ã‚¢ãƒŸãƒ³æ¿ƒåº¦ã«ã‚ˆã‚‹é‡ã¿æ›´æ–°
                    excit_m = self.excitatory_inhibitory[m] 
                    excit_k = self.excitatory_inhibitory[k]
                    
                    if excit_m > 0:  # èˆˆå¥®æ€§å…¥åŠ›
                        weight_update = (delta * 
                                       self.amine_concentrations[n][k][0] * 
                                       excit_m * excit_k)
                    else:  # æŠ‘åˆ¶æ€§å…¥åŠ›
                        weight_update = (delta * 
                                       self.amine_concentrations[n][k][1] * 
                                       excit_m * excit_k)
                    
                    # é‡ã¿æ›´æ–°é©ç”¨
                    self.output_weights[n][k][m] += weight_update
        
        self.profiler.end_timer('weight_amine_proc')
    
    def _neuro_weight_calc_gpu(self):
        """GPUç‰ˆé‡ã¿æ›´æ–°ï¼ˆPhase GPU-1å®Ÿè£…ï¼‰"""
        for n in range(self.output_units):
            k_arr, m_arr = self.weight_indices_cache[n]
            
            if len(k_arr) == 0:
                continue
            
            # GPUä½¿ç”¨é–¾å€¤ã‚’ç·©å’Œï¼ˆå°è¦æ¨¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã§ã‚‚GPUä½¿ç”¨ï¼‰
            if len(k_arr) > 10:  # é–¾å€¤ã‚’å¤§å¹…ã«ç·©å’Œï¼ˆ10å€‹ä»¥ä¸Šã§GPUä½¿ç”¨ï¼‰
                try:
                    # GPUè¨ˆç®—
                    inputs_gpu = cp.asarray(self.output_inputs[n, m_arr])
                    outputs_gpu = cp.asarray(self.output_outputs[n, k_arr])
                    
                    # GPUç‰ˆdeltaè¨ˆç®—ï¼ˆæ•°å­¦çš„ã«ã¯CPUç‰ˆã¨å®Œå…¨ä¸€è‡´ï¼‰
                    abs_outputs_gpu = cp.abs(outputs_gpu)
                    delta_gpu = (self.learning_rate * inputs_gpu * 
                               abs_outputs_gpu * (1 - abs_outputs_gpu))
                    
                    # CPUç‰ˆã¨åŒä¸€ã®ãƒ•ãƒ©ã‚°åˆ†å²
                    if self.flags[10] == 1:
                        excit_k_gpu = cp.asarray(self.excitatory_inhibitory[k_arr])
                        amine_diff_gpu = cp.asarray(
                            self.amine_concentrations[n, k_arr, 0] - 
                            self.amine_concentrations[n, k_arr, 1]
                        )
                        weight_update_gpu = delta_gpu * excit_k_gpu * amine_diff_gpu
                    else:
                        # èˆˆå¥®æ€§/æŠ‘åˆ¶æ€§åˆ†å²ï¼ˆCPUç‰ˆã¨å®Œå…¨ä¸€è‡´ï¼‰
                        excit_m = self.excitatory_inhibitory[m_arr]
                        excit_k = self.excitatory_inhibitory[k_arr]
                        
                        pos_mask = excit_m > 0
                        weight_update = np.zeros_like(cp.asnumpy(delta_gpu))
                        
                        # GPU + CPUæ··åˆå‡¦ç†ï¼ˆæœ€é©åŒ–ã®ãŸã‚ï¼‰
                        if np.any(pos_mask):
                            pos_indices = np.where(pos_mask)[0]
                            pos_delta_gpu = delta_gpu[pos_indices]
                            pos_amine_gpu = cp.asarray(self.amine_concentrations[n, k_arr[pos_mask], 0])
                            pos_excit_m_gpu = cp.asarray(excit_m[pos_mask])
                            pos_excit_k_gpu = cp.asarray(excit_k[pos_mask])
                            
                            weight_update_pos_gpu = (pos_delta_gpu * pos_amine_gpu * 
                                                   pos_excit_m_gpu * pos_excit_k_gpu)
                            weight_update[pos_mask] = cp.asnumpy(weight_update_pos_gpu)
                        
                        neg_mask = ~pos_mask
                        if np.any(neg_mask):
                            neg_indices = np.where(neg_mask)[0]
                            neg_delta_gpu = delta_gpu[neg_indices]
                            neg_amine_gpu = cp.asarray(self.amine_concentrations[n, k_arr[neg_mask], 1])
                            neg_excit_m_gpu = cp.asarray(excit_m[neg_mask])
                            neg_excit_k_gpu = cp.asarray(excit_k[neg_mask])
                            
                            weight_update_neg_gpu = (neg_delta_gpu * neg_amine_gpu * 
                                                   neg_excit_m_gpu * neg_excit_k_gpu)
                            weight_update[neg_mask] = cp.asnumpy(weight_update_neg_gpu)
                        
                        weight_update_gpu = cp.asarray(weight_update)
                    
                    # GPUçµæœã‚’CPUã«è»¢é€ã—ã¦é©ç”¨
                    weight_update_final = cp.asnumpy(weight_update_gpu)
                    self.output_weights[n, k_arr, m_arr] += weight_update_final
                    
                except Exception as gpu_error:
                    # GPUå¤±æ•—æ™‚ã¯è©²å½“ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚’CPUç‰ˆã§å‡¦ç†
                    print(f"âš ï¸ GPUè¨ˆç®—å¤±æ•—ï¼ˆãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³{n}ï¼‰ã€CPUç‰ˆã§å‡¦ç†: {gpu_error}")
                    self._process_single_neuron_cpu(n)
            else:
                # å°è¦æ¨¡è¨ˆç®—ã¯CPUç‰ˆãŒåŠ¹ç‡çš„
                self._process_single_neuron_cpu(n)
    
    def _process_single_neuron_cpu(self, n: int):
        """å˜ä¸€ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®CPUå‡¦ç†ï¼ˆGPUå¤±æ•—æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""
        k_arr, m_arr = self.weight_indices_cache[n]
        
        if len(k_arr) == 0:
            return
        
        # CPUç‰ˆå‡¦ç†ï¼ˆPhase 1-3æœ€é©åŒ–ï¼‰
        inputs = self.output_inputs[n, m_arr]
        outputs = self.output_outputs[n, k_arr]
        
        abs_outputs = np.abs(outputs)
        delta = self.learning_rate * np.einsum('i,i,i->i', inputs, abs_outputs, (1 - abs_outputs))
        
        if self.flags[10] == 1:
            excit_k = self.excitatory_inhibitory[k_arr]
            amine_diff = (self.amine_concentrations[n, k_arr, 0] - 
                         self.amine_concentrations[n, k_arr, 1])
            weight_update = np.einsum('i,i,i->i', delta, excit_k, amine_diff)
        else:
            excit_m = self.excitatory_inhibitory[m_arr]
            excit_k = self.excitatory_inhibitory[k_arr]
            
            pos_mask = excit_m > 0
            neg_mask = ~pos_mask
            
            weight_update = np.zeros_like(delta)
            
            if np.any(pos_mask):
                pos_delta = delta[pos_mask]
                pos_amine = self.amine_concentrations[n, k_arr[pos_mask], 0]
                pos_excit_m = excit_m[pos_mask]
                pos_excit_k = excit_k[pos_mask]
                weight_update[pos_mask] = np.einsum('i,i,i,i->i', 
                                                  pos_delta, pos_amine, 
                                                  pos_excit_m, pos_excit_k)
            
            if np.any(neg_mask):
                neg_delta = delta[neg_mask]
                neg_amine = self.amine_concentrations[n, k_arr[neg_mask], 1]
                neg_excit_m = excit_m[neg_mask]
                neg_excit_k = excit_k[neg_mask]
                weight_update[neg_mask] = np.einsum('i,i,i,i->i', 
                                                  neg_delta, neg_amine,
                                                  neg_excit_m, neg_excit_k)
        
        self.output_weights[n, k_arr, m_arr] += weight_update
    
    def _neuro_weight_calc_vectorized(self):
        """
        ed_genuine.prompt.mdå®Œå…¨æº–æ‹ ã®ãƒ™ã‚¯ãƒˆãƒ«åŒ–é‡ã¿æ›´æ–°ï¼ˆä¿®æ­£ç‰ˆï¼‰
        
        Cå®Ÿè£…ã®æ­£ç¢ºãªå†ç¾:
        for (n = 0; n < ot; n++) {
            for (k = in+2; k <= all+1; k++) {
                for (m = 0; m <= all+1; m++) {
                    if (w_ot_ot[n][k][m] != 0) {
                        del = alpha * ot_in[n][m];
                        del *= fabs(ot_ot[n][k]);
                        del *= (1 - fabs(ot_ot[n][k]));
                        
                        if (ow[m] > 0)  // èˆˆå¥®æ€§å…¥åŠ›
                            w_ot_ot[n][k][m] += del * del_ot[n][k][0] * ow[m] * ow[k];
                        else            // æŠ‘åˆ¶æ€§å…¥åŠ›
                            w_ot_ot[n][k][m] += del * del_ot[n][k][1] * ow[m] * ow[k];
                    }
                }
            }
        }
        """
        # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã§ç‹¬ç«‹ã—ãŸé‡ã¿æ›´æ–°ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
        for n in range(self.output_units):
            # Phase 1: æœ‰åŠ¹é‡ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¸€åº¦ã ã‘å–å¾—
            if hasattr(self, 'weight_indices_cache') and n in self.weight_indices_cache:
                k_arr, m_arr = self.weight_indices_cache[n]
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: å‹•çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç”Ÿæˆ
                k_indices = []
                m_indices = []
                for k in range(self.input_units + 2, self.total_units + 2):
                    for m in range(self.total_units + 2):
                        if self.output_weights[n][k][m] != 0:
                            k_indices.append(k)
                            m_indices.append(m)
                k_arr = np.array(k_indices)
                m_arr = np.array(m_indices)
            
            if len(k_arr) == 0:
                continue
            
            # Phase 2: ãƒ™ã‚¯ãƒˆãƒ«åŒ–ãƒ‡ãƒ¼ã‚¿æº–å‚™ï¼ˆä¸€æ‹¬å–å¾—ï¼‰
            inputs = self.output_inputs[n, m_arr]      # ot_in[n][m]
            outputs = self.output_outputs[n, k_arr]    # ot_ot[n][k]
            excit_m = self.excitatory_inhibitory[m_arr] # ow[m]
            excit_k = self.excitatory_inhibitory[k_arr] # ow[k]
            
            # Phase 3: deltaè¨ˆç®—ï¼ˆed_genuine.prompt.mdå®Œå…¨æº–æ‹ ï¼‰
            # del = alpha * ot_in[n][m] * fabs(ot_ot[n][k]) * (1 - fabs(ot_ot[n][k]))
            abs_outputs = np.abs(outputs)
            delta = self.learning_rate * inputs * abs_outputs * (1 - abs_outputs)
            
            # Phase 4: ã‚¢ãƒŸãƒ³æ¿ƒåº¦ã«ã‚ˆã‚‹é‡ã¿æ›´æ–°ï¼ˆèˆˆå¥®æ€§/æŠ‘åˆ¶æ€§åˆ†é›¢ï¼‰
            # if (ow[m] > 0) èˆˆå¥®æ€§å…¥åŠ›: del * del_ot[n][k][0] * ow[m] * ow[k]
            # else æŠ‘åˆ¶æ€§å…¥åŠ›: del * del_ot[n][k][1] * ow[m] * ow[k]
            pos_mask = excit_m > 0  # èˆˆå¥®æ€§å…¥åŠ›ãƒã‚¹ã‚¯
            neg_mask = ~pos_mask    # æŠ‘åˆ¶æ€§å…¥åŠ›ãƒã‚¹ã‚¯
            
            weight_update = np.zeros_like(delta)
            
            # èˆˆå¥®æ€§å…¥åŠ›å‡¦ç†ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
            if np.any(pos_mask):
                pos_amine = self.amine_concentrations[n, k_arr[pos_mask], 0]  # del_ot[n][k][0]
                weight_update[pos_mask] = (delta[pos_mask] * pos_amine * 
                                         excit_m[pos_mask] * excit_k[pos_mask])
            
            # æŠ‘åˆ¶æ€§å…¥åŠ›å‡¦ç†ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
            if np.any(neg_mask):
                neg_amine = self.amine_concentrations[n, k_arr[neg_mask], 1]  # del_ot[n][k][1]
                weight_update[neg_mask] = (delta[neg_mask] * neg_amine * 
                                         excit_m[neg_mask] * excit_k[neg_mask])
            
            # Phase 5: é‡ã¿æ›´æ–°é©ç”¨ï¼ˆãƒ™ã‚¯ãƒˆãƒ«åŒ–ï¼‰
            self.output_weights[n, k_arr, m_arr] += weight_update
    
    def neuro_calc(self, indata_input: List[float], indata_tch: List[float]):
        """
        Cå®Ÿè£…ã®neuro_calc()ã‚’å®Œå…¨å†ç¾
        1ãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—
        """
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: é †æ–¹å‘è¨ˆç®—
        self.profiler.start_timer('forward_pass')
        self.neuro_output_calc(indata_input)
        self.profiler.end_timer('forward_pass')
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: æ•™å¸«ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        self.profiler.start_timer('teacher_processing')
        self.neuro_teach_calc(indata_tch)
        self.profiler.end_timer('teacher_processing')
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: é‡ã¿æ›´æ–°
        self.profiler.start_timer('weight_update')
        self.neuro_weight_calc()
        self.profiler.end_timer('weight_update')
    
    def train_epoch(self, show_progress=True, epoch_info="ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’") -> Tuple[float, int]:
        """1ã‚¨ãƒãƒƒã‚¯ã®å­¦ç¿’å®Ÿè¡Œ"""
        self.error = 0.0
        self.error_count = 0

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®é€²æ—è¡¨ç¤ºï¼ˆã‚¨ãƒãƒƒã‚¯æƒ…å ±çµ±åˆï¼‰
        pattern_iterator = tqdm(range(self.num_patterns), 
                               desc=epoch_info, 
                               position=1,  # position=2ã‹ã‚‰1ã«å¤‰æ›´ï¼ˆ2æ®µè¡¨ç¤ºï¼‰
                               leave=False) if show_progress else range(self.num_patterns)

        for pattern in pattern_iterator:
            # å…¥åŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æº–å‚™
            indata_input = [self.input_data[pattern][i] 
                           for i in range(self.input_units // 2)]
            indata_tch = [self.teacher_data[pattern][i] 
                         for i in range(self.output_units)]

            # å­¦ç¿’å®Ÿè¡Œ
            self.neuro_calc(indata_input, indata_tch)

        avg_error = self.error / self.num_patterns if self.num_patterns > 0 else 0.0
        return avg_error, self.error_count

    def train_epoch_with_buffer(self, results_buffer: 'LearningResultsBuffer', epoch: int,
                               train_inputs: np.ndarray, train_labels: np.ndarray,
                               test_inputs: np.ndarray, test_labels: np.ndarray,
                               show_progress=True, epoch_info="ãƒ‘ã‚¿ãƒ¼ãƒ³å­¦ç¿’") -> Tuple[float, int]:
        """
        å­¦ç¿’ä¸­ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ ã§çµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜ã™ã‚‹ã‚¨ãƒãƒƒã‚¯å­¦ç¿’ - ç¬¬2æ®µéšæœ€é©åŒ–
        ğŸš€ ãƒãƒƒãƒåŒ–æœ€é©åŒ–: recordå‡¦ç†ã‚’å®Œå…¨ãƒãƒƒãƒåŒ–ã§é«˜é€ŸåŒ–
        ed_genuine.prompt.mdæº–æ‹ : é †æ–¹å‘è¨ˆç®—â†’çµæœä¿å­˜â†’å­¦ç¿’ã®é †åºã‚’ç¶­æŒ
        """
        self.error = 0.0
        self.error_count = 0

        # ğŸ”§ ã€é‡è¦ä¿®æ­£ã€‘ã‚¨ãƒãƒƒã‚¯æ¯ã®ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ã‚µã‚¤ã‚ºã‚’ä½¿ç”¨
        current_epoch_patterns = len(train_inputs)

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã”ã¨ã®é€²æ—è¡¨ç¤º
        pattern_iterator = tqdm(range(current_epoch_patterns), 
                               desc=epoch_info, 
                               position=1,
                               leave=False) if show_progress else range(current_epoch_patterns)

        # ğŸš€ ç¬¬2æ®µéšæœ€é©åŒ–: ãƒãƒƒãƒå‡¦ç†ç”¨é…åˆ—äº‹å‰æº–å‚™
        batch_predictions = []
        batch_errors = []
        batch_corrects = []
        batch_predicted_classes = []
        batch_true_classes = []

        # 1. è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã§ã®å­¦ç¿’ + ãƒãƒƒãƒçµæœåé›†
        for pattern in pattern_iterator:
            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: å…¨ä½“å‡¦ç†é–‹å§‹
            self.profiler.start_timer('total_processing')
            
            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: ãƒ‡ãƒ¼ã‚¿æº–å‚™é–‹å§‹
            self.profiler.start_timer('data_preparation')
            # ğŸ”§ ã€é‡è¦ä¿®æ­£ã€‘input_dataã§ã¯ãªãç›´æ¥train_inputsã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            # ã‚¨ãƒãƒƒã‚¯æ¯ã®ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ã«å¯¾å¿œã™ã‚‹ãŸã‚ã€train_inputsé…åˆ—ã‚’ç›´æ¥ä½¿ç”¨
            current_input = train_inputs[pattern].flatten().astype(float)
            indata_input = current_input[:self.input_units // 2]  # E/Iå¯¾å¿œã®åŠåˆ†ã‚µã‚¤ã‚º
            
            # ğŸ”§ teacher_dataã‚‚ç›´æ¥train_labelsã‹ã‚‰å–å¾—ã—ã¦one-hotåŒ–
            true_class = int(train_labels[pattern])
            indata_tch = self._onehot_encode(true_class, self.output_units)
            self.profiler.end_timer('data_preparation')

            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: äºˆæ¸¬è¨ˆç®—é–‹å§‹
            self.profiler.start_timer('prediction_calc')
            # ã€é‡è¦ã€‘å­¦ç¿’å‰ã«äºˆæ¸¬å®Ÿè¡Œï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
            prediction = self.predict(indata_input)
            predicted_class = np.argmax(prediction)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—: ãƒ†ã‚¹ãƒˆæ™‚ã¨åŒã˜è¨ˆç®—æ–¹å¼ã‚’ä½¿ç”¨
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - prediction[j]) for j in range(self.output_units)) / self.output_units
            self.profiler.end_timer('prediction_calc')

            # å­¦ç¿’å®Ÿè¡Œï¼ˆed_genuine.prompt.mdæº–æ‹ ã®ã‚¢ãƒŸãƒ³æ‹¡æ•£å­¦ç¿’ï¼‰
            self.neuro_calc(indata_input, indata_tch)
            
            # ğŸš€ æœ€é©åŒ–: çµæœã‚’ãƒãƒƒãƒé…åˆ—ã«è“„ç©ï¼ˆå€‹åˆ¥recordå‡¦ç†ã‚’å‰Šé™¤ï¼‰
            batch_predictions.append(prediction)
            batch_errors.append(float(error))
            batch_corrects.append(correct)
            batch_predicted_classes.append(int(predicted_class))
            batch_true_classes.append(true_class)

            # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: å…¨ä½“å‡¦ç†çµ‚äº†
            self.profiler.end_timer('total_processing')
            self.profiler.complete_sample()
            
            # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æ›´æ–°: ä¸€å®šé–“éš”ã§ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯å®Ÿè¡Œï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºç”¨ï¼‰
            if hasattr(self, 'heatmap_callback') and self.heatmap_callback is not None:
                # ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã‚’æ›´æ–°ï¼ˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜ã—ã¦åŒæœŸã‚’ç¢ºä¿ï¼‰
                # ğŸ”§ ä¿®æ­£: å®Ÿéš›ã®å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨
                current_input_data = current_input
                    
                if hasattr(self, 'update_current_sample_info'):
                    self.update_current_sample_info(epoch, pattern, true_class, predicted_class,
                                                   pattern_idx=pattern, input_data=current_input_data)
                
                # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯ã¾ãŸã¯Næ¯ã«æ›´æ–°ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºã®ãŸã‚ï¼‰
                heatmap_update_interval = getattr(self, 'heatmap_update_interval', 10)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ10ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯
                if pattern % heatmap_update_interval == 0 or pattern == current_epoch_patterns - 1:
                    self.heatmap_callback()

        # ğŸš€ ç¬¬2æ®µéšæœ€é©åŒ–: è¨“ç·´çµæœã®ä¸€æ‹¬ãƒãƒƒãƒè¨˜éŒ²
        self.profiler.start_timer('result_recording')
        for i, (correct, error, pred_class, true_class) in enumerate(
            zip(batch_corrects, batch_errors, batch_predicted_classes, batch_true_classes)):
            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®å ´åˆã®ã¿ä¿å­˜ï¼‰
            input_image = train_inputs[i] if epoch == results_buffer.epochs - 1 else None
            results_buffer.record_train_result(epoch, i, correct, error, pred_class, true_class, input_image)
        self.profiler.end_timer('result_recording')

        # 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®çµæœä¿å­˜ï¼ˆ1ã‚¨ãƒãƒƒã‚¯ã‚ãŸã‚Š1å›ã®ãƒãƒƒãƒå‡¦ç†ï¼‰
        # æœ€é©åŒ–: predict_batchã§ä¸€æ‹¬å‡¦ç†
        test_predictions = self.predict_batch(test_inputs)
        
        # ğŸš€ ç¬¬2æ®µéšæœ€é©åŒ–: ãƒ†ã‚¹ãƒˆçµæœã®ä¸€æ‹¬ãƒãƒƒãƒè¨˜éŒ²
        test_batch_corrects = []
        test_batch_errors = []
        test_batch_pred_classes = []
        test_batch_true_classes = []
        
        for i, (prediction, test_label) in enumerate(zip(test_predictions, test_labels)):
            predicted_class = np.argmax(prediction)
            true_class = int(test_label)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—: è¨“ç·´æ™‚ã¨åŒã˜è¨ˆç®—æ–¹å¼ã‚’ä½¿ç”¨
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - prediction[j]) for j in range(self.output_units)) / self.output_units
            
            test_batch_corrects.append(correct)
            test_batch_errors.append(float(error))
            test_batch_pred_classes.append(int(predicted_class))
            test_batch_true_classes.append(true_class)

        # ãƒ†ã‚¹ãƒˆçµæœã®ä¸€æ‹¬è¨˜éŒ²
        for i, (correct, error, pred_class, true_class) in enumerate(
            zip(test_batch_corrects, test_batch_errors, test_batch_pred_classes, test_batch_true_classes)):
            results_buffer.record_test_result(epoch, i, correct, error, pred_class, true_class)

        # ğŸ”§ ä¿®æ­£: æ­£ã—ã„ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’ä½¿ç”¨ã—ã¦å¹³å‡èª¤å·®ã‚’è¨ˆç®—
        avg_error = self.error / current_epoch_patterns if current_epoch_patterns > 0 else 0.0
        return avg_error, self.error_count

    def train_epoch_with_minibatch(self, results_buffer: 'LearningResultsBuffer', epoch: int,
                                   train_inputs: np.ndarray, train_labels: np.ndarray,
                                   test_inputs: np.ndarray, test_labels: np.ndarray,
                                   batch_size: int, show_progress=True, 
                                   epoch_info="ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’") -> Tuple[float, int]:
        """
        ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’å¯¾å¿œã‚¨ãƒãƒƒã‚¯å­¦ç¿’ãƒ¡ã‚½ãƒƒãƒ‰
        
        æ³¨ï¼šé‡‘å­å‹‡æ°ã®EDç†è«–ã«ã¯ãƒãƒƒãƒå‡¦ç†æ¦‚å¿µãªã—
        å¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿å¯¾å¿œã®ãŸã‚ã®ç¾ä»£çš„æ©Ÿèƒ½æ‹¡å¼µ
        
        Args:
            results_buffer: å­¦ç¿’çµæœãƒãƒƒãƒ•ã‚¡
            epoch: ã‚¨ãƒãƒƒã‚¯ç•ªå·
            train_inputs: è¨“ç·´å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            train_labels: è¨“ç·´ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
            test_inputs: ãƒ†ã‚¹ãƒˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            test_labels: ãƒ†ã‚¹ãƒˆãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿
            batch_size: ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º
            show_progress: é€²æ—è¡¨ç¤ºæœ‰ç„¡
            epoch_info: é€²æ—è¡¨ç¤ºæƒ…å ±
        
        Returns:
            Tuple[float, int]: (å¹³å‡èª¤å·®, ã‚¨ãƒ©ãƒ¼æ•°)
        """
        self.error = 0.0
        self.error_count = 0
        
        # ğŸ”§ ã€é‡è¦ä¿®æ­£ã€‘ã‚¨ãƒãƒƒã‚¯æ¯ã®ç‹¬ç«‹ãƒ‡ãƒ¼ã‚¿ã§ãƒŸãƒ‹ãƒãƒƒãƒãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
        # ãƒŸãƒ‹ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼ä½œæˆï¼ˆå…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹è¿½è·¡å¯¾å¿œï¼‰
        train_original_indices = getattr(self, 'train_original_indices', None)
        
        # ã‚¨ãƒãƒƒã‚¯æ¯ã«ç‹¬ç«‹ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã™ã‚‹ãŸã‚ã€æ–°ã—ã„ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚’ä½œæˆ
        train_loader = MiniBatchDataLoader(train_inputs, train_labels, batch_size, shuffle=True, 
                                          original_indices=train_original_indices)
        
        # çµ±è¨ˆå–å¾—ã®ãŸã‚ã«ãƒ­ãƒ¼ãƒ€ãƒ¼ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ä¿å­˜ï¼ˆç´¯ç©çµ±è¨ˆç®¡ç†ï¼‰
        if not hasattr(self, '_cumulative_usage_stats'):
            self._cumulative_usage_stats = {}
        
        self._last_train_loader = train_loader
        
        # ãƒãƒƒãƒã”ã¨ã®é€²æ—è¡¨ç¤º
        batch_iterator = tqdm(train_loader, 
                             desc=epoch_info, 
                             position=1,
                             leave=False) if show_progress else train_loader
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ãƒ‘ã‚¿ãƒ¼ãƒ³ç•ªå·ã®è¿½è·¡ç”¨
        pattern_counter = 0
        
        # 1. ãƒŸãƒ‹ãƒãƒƒãƒå˜ä½ã§ã®å­¦ç¿’
        for batch_inputs, batch_labels in batch_iterator:
            batch_errors = []
            
            # ãƒãƒƒãƒå†…å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã§å­¦ç¿’å®Ÿè¡Œ
            for i, (input_data, label) in enumerate(zip(batch_inputs, batch_labels)):
                # ğŸ”§ ã€é‡è¦ä¿®æ­£ã€‘å…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚’ç›´æ¥ä½¿ç”¨ï¼ˆflattenæ¸ˆã¿ã®ãƒ‡ãƒ¼ã‚¿å¯¾å¿œï¼‰
                input_flat = input_data.flatten().astype(float)
                indata_input = input_flat[:self.input_units // 2]  # E/Iå¯¾å¿œã®åŠåˆ†ã‚µã‚¤ã‚º
                
                # ğŸ”§ teacher_dataã‚’one-hotåŒ–ã—ã¦ä½œæˆ
                true_class = int(label)
                indata_tch = self._onehot_encode(true_class, self.output_units)
                
                # ã€é‡è¦ã€‘å­¦ç¿’å‰ã«äºˆæ¸¬å®Ÿè¡Œï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
                prediction = self.predict(indata_input)
                predicted_class = np.argmax(prediction)
                correct = (predicted_class == true_class)
                # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—: çµ±ä¸€ã•ã‚ŒãŸè¨ˆç®—æ–¹å¼ã‚’ä½¿ç”¨
                teacher_onehot = self._onehot_encode(true_class, self.output_units)
                error = sum(abs(teacher_onehot[j] - prediction[j]) for j in range(self.output_units)) / self.output_units
                
                # å­¦ç¿’å®Ÿè¡Œï¼ˆed_genuine.prompt.mdæº–æ‹ ã®ã‚¢ãƒŸãƒ³æ‹¡æ•£å­¦ç¿’ï¼‰
                self.neuro_calc(indata_input, indata_tch)
                
                # çµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«å³åº§ã«ä¿å­˜ï¼ˆæ··åŒè¡Œåˆ—ç”¨ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰
                results_buffer.record_train_result(epoch, pattern_counter, correct, float(error), 
                                                 int(predicted_class), true_class)
                
                # ãƒ’ãƒ¼ãƒˆãƒãƒƒãƒ—æ›´æ–°: ãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ã§ã‚‚ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤ºå¯¾å¿œ
                if hasattr(self, 'heatmap_callback') and self.heatmap_callback is not None:
                    # ã‚µãƒ³ãƒ—ãƒ«æƒ…å ±ã‚’æ›´æ–°ï¼ˆå…¥åŠ›ãƒ‡ãƒ¼ã‚¿ã‚‚ä¿å­˜ã—ã¦åŒæœŸã‚’ç¢ºä¿ï¼‰
                    if hasattr(self, 'update_current_sample_info'):
                        self.update_current_sample_info(epoch, pattern_counter, true_class, predicted_class, 
                                                       pattern_idx=pattern_counter, input_data=input_data)
                    
                    # ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯ã¾ãŸã¯Næ¯ã«æ›´æ–°ï¼ˆãƒŸãƒ‹ãƒãƒƒãƒå­¦ç¿’ç”¨ï¼‰
                    heatmap_update_interval = getattr(self, 'heatmap_update_interval', 5)  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ5ãƒ‘ã‚¿ãƒ¼ãƒ³æ¯
                    # ğŸ”§ ä¿®æ­£: ç¾åœ¨ã®ãƒ‡ãƒ¼ã‚¿æ•°ã«åŸºã¥ã„ã¦åˆ¤å®š
                    current_epoch_patterns = len(train_inputs)
                    if pattern_counter % heatmap_update_interval == 0 or pattern_counter == current_epoch_patterns - 1:
                        self.heatmap_callback()
                
                
                batch_errors.append(error)
                pattern_counter += 1
            
            # ğŸ¯ ã€v0.1.6æ–°æ©Ÿèƒ½ã€‘3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„èª¤å·®è¨˜éŒ²
            # ed_genuine.prompt.mdæº–æ‹ ï¼šãƒãƒƒãƒå˜ä½ã§ã®é«˜é€Ÿèª¤å·®é›†è¨ˆ
            results_buffer.record_train_batch_error_efficient(
                epoch, 
                np.array(batch_errors), 
                len(batch_inputs)
            )
            
            # ãƒãƒƒãƒãƒ¬ãƒ™ãƒ«ã®èª¤å·®é›†è¨ˆï¼ˆå¾“æ¥æ–¹å¼ï¼šä¸‹ä½äº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰
            batch_avg_error = np.mean(batch_errors)
            self.error += batch_avg_error * len(batch_inputs)
        
        # 2. ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®çµæœä¿å­˜ï¼ˆä¸€æ‹¬å‡¦ç† + åŠ¹ç‡çš„èª¤å·®è¨˜éŒ²ï¼‰
        test_predictions = self.predict_batch(test_inputs)
        test_errors = []
        
        for i, (prediction, test_label) in enumerate(zip(test_predictions, test_labels)):
            predicted_class = np.argmax(prediction)
            true_class = int(test_label)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—: çµ±ä¸€ã•ã‚ŒãŸè¨ˆç®—æ–¹å¼ã‚’ä½¿ç”¨
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - prediction[j]) for j in range(self.output_units)) / self.output_units
            
            # ãƒ†ã‚¹ãƒˆçµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«ä¿å­˜ï¼ˆæ··åŒè¡Œåˆ—ç”¨ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰
            results_buffer.record_test_result(epoch, i, correct, float(error), 
                                            int(predicted_class), true_class)
            test_errors.append(error)
        
        # ğŸ¯ ã€v0.1.6æ–°æ©Ÿèƒ½ã€‘ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„èª¤å·®è¨˜éŒ²
        # ed_genuine.prompt.mdæº–æ‹ ï¼šãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ä¸€æ‹¬é«˜é€Ÿèª¤å·®é›†è¨ˆ
        results_buffer.record_test_batch_error_efficient(
            epoch,
            np.array(test_errors),
            len(test_inputs)
        )
        
        avg_error = float(self.error / pattern_counter) if pattern_counter > 0 else 0.0
        return avg_error, self.error_count

    def record_epoch_results(self, results_buffer: 'LearningResultsBuffer', epoch: int, 
                           train_inputs: np.ndarray, train_labels: np.ndarray,
                           test_inputs: np.ndarray, test_labels: np.ndarray):
        """
        ã‚¨ãƒãƒƒã‚¯çµæœã‚’ãƒãƒƒãƒ•ã‚¡ã«é«˜é€Ÿè¨˜éŒ²ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
        predict_batchã‚’ä½¿ç”¨ã—ã¦åŠ¹ç‡çš„ã«çµæœã‚’è¨˜éŒ²
        """
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿çµæœè¨˜éŒ²
        train_predictions = self.predict_batch(train_inputs)
        for i, (pred, label) in enumerate(zip(train_predictions, train_labels)):
            predicted_class = np.argmax(pred)
            true_class = int(label)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - pred[j]) for j in range(self.output_units)) / self.output_units
            # ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆæœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®å ´åˆã®ã¿ä¿å­˜ï¼‰
            input_image = train_inputs[i] if epoch == results_buffer.epochs - 1 else None
            results_buffer.record_train_result(epoch, i, correct, float(error), int(predicted_class), true_class, input_image)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿çµæœè¨˜éŒ²
        test_predictions = self.predict_batch(test_inputs) 
        for i, (pred, label) in enumerate(zip(test_predictions, test_labels)):
            predicted_class = np.argmax(pred)
            true_class = int(label)
            correct = (predicted_class == true_class)
            # EDæ³•æº–æ‹ ã®èª¤å·®è¨ˆç®—
            teacher_onehot = self._onehot_encode(true_class, self.output_units)
            error = sum(abs(teacher_onehot[j] - pred[j]) for j in range(self.output_units)) / self.output_units
            results_buffer.record_test_result(epoch, i, correct, float(error), int(predicted_class), true_class)
    
    def _onehot_encode(self, label: int, num_classes: int) -> np.ndarray:
        """ãƒ¯ãƒ³ãƒ›ãƒƒãƒˆã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è£œåŠ©é–¢æ•°"""
        onehot = np.zeros(num_classes)
        onehot[label] = 1.0
        return onehot

    def predict(self, input_pattern: List[float]) -> List[float]:
        """äºˆæ¸¬å®Ÿè¡Œ"""
        self.neuro_output_calc(input_pattern)
        
        results = []
        for n in range(self.output_units):
            results.append(self.output_outputs[n][self.input_units + 2])
        
        return results
    
    def predict_batch(self, input_patterns: np.ndarray) -> np.ndarray:
        """
        ãƒãƒƒãƒäºˆæ¸¬å®Ÿè¡Œï¼ˆé«˜é€ŸåŒ–ç”¨ï¼‰
        ed_genuine.prompt.mdæº–æ‹ : é †æ–¹å‘è¨ˆç®—ã®æœ¬è³ªã¯ç¶­æŒã—ã¤ã¤æœ€é©åŒ–
        """
        predictions = []
        for pattern in input_patterns:
            prediction = self.predict(pattern.tolist())
            predictions.append(prediction)
        return np.array(predictions)
    
    def calculate_accuracy_and_error(self, inputs: np.ndarray, labels: np.ndarray) -> tuple:
        """
        ç²¾åº¦ã¨èª¤å·®ã®åŠ¹ç‡çš„è¨ˆç®—
        ed_genuine.prompt.mdæº–æ‹ : è¨ˆç®—æ–¹æ³•ã¯ç¶­æŒã€å®Ÿè£…æœ€é©åŒ–
        """
        predictions = self.predict_batch(inputs)
        
        # ç²¾åº¦è¨ˆç®—
        predicted_classes = np.argmax(predictions, axis=1)
        correct = np.sum(predicted_classes == labels)
        accuracy = correct / len(labels)
        
        # èª¤å·®è¨ˆç®—ï¼ˆEDæ³•æº–æ‹ ã®çµ¶å¯¾å€¤èª¤å·®ï¼‰
        true_outputs = np.zeros((len(labels), 10))
        true_outputs[np.arange(len(labels)), labels] = 1.0
        error = np.mean(np.sum(np.abs(predictions - true_outputs), axis=1)) / self.output_units
        
        return accuracy, error
    
    def _precompute_nonzero_weights(self):
        """
        éã‚¼ãƒ­é‡ã¿ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’äº‹å‰è¨ˆç®—ã—ã¦ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å‘ä¸Š
        ed_genuine.prompt.mdæº–æ‹ : w_ot_ot[n][k][m] != 0 ã®å ´åˆã®ã¿å‡¦ç†
        """
        if self.hyperparams.verbose:
            print("ğŸ“Š éã‚¼ãƒ­é‡ã¿è§£æä¸­...")
        self.nonzero_weight_indices = []
        
        total_weights = 0
        nonzero_weights = 0
        
        for n in range(self.output_units):
            n_indices = []
            for k in range(self.input_units + 2, self.total_units + 2):
                k_indices = []
                for m in range(self.total_units + 2):
                    total_weights += 1
                    if self.output_weights[n][k][m] != 0:
                        k_indices.append(m)
                        nonzero_weights += 1
                n_indices.append(k_indices)
            self.nonzero_weight_indices.append(n_indices)
        
        sparsity = (total_weights - nonzero_weights) / total_weights * 100
        if self.hyperparams.verbose:
            print(f"âœ… é‡ã¿ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§è§£æå®Œäº†:")
            print(f"   ç·é‡ã¿: {total_weights:,}")  
            print(f"   éã‚¼ãƒ­: {nonzero_weights:,}")
            print(f"   ã‚¹ãƒ‘ãƒ¼ã‚¹æ€§: {sparsity:.1f}%")
            print(f"   äºˆæƒ³é«˜é€ŸåŒ–: {total_weights/max(1, nonzero_weights):.1f}å€")
    
    def get_network_status(self) -> dict:
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çŠ¶æ…‹ã®å–å¾—"""
        # é‡ã¿çµ±è¨ˆ
        total_weights = 0
        active_weights = 0
        weight_sum = 0.0
        min_weight = float('inf')
        max_weight = float('-inf')
        weight_values = []
        
        for n in range(self.output_units):
            for k in range(self.input_units + 2, self.total_units + 2):
                for m in range(self.total_units + 2):
                    total_weights += 1
                    weight_val = self.output_weights[n][k][m]
                    if weight_val != 0:
                        active_weights += 1
                        weight_sum += abs(weight_val)
                        weight_values.append(weight_val)
                        min_weight = min(min_weight, weight_val)
                        max_weight = max(max_weight, weight_val)
        
        # ã‚¢ã‚¯ãƒ†ã‚£ãƒ–é‡ã¿ãŒãªã„å ´åˆã®å‡¦ç†
        if active_weights == 0:
            min_weight = 0.0
            max_weight = 0.0
        
        return {
            'network_config': {
                'input_units': self.input_units,
                'hidden_units': self.hidden_units,
                'output_units': self.output_units,
                'total_units': self.total_units + 2
            },
            'weight_statistics': {
                'total_weights': total_weights,
                'active_weights': active_weights,
                'avg_weight': weight_sum / active_weights if active_weights > 0 else 0.0,
                'min_weight': min_weight,
                'max_weight': max_weight,
                'mean_weight': sum(weight_values) / len(weight_values) if weight_values else 0.0
            },
            'parameters': {
                'alpha': self.learning_rate,
                'beta': self.initial_amine,
            }
        }

    # === å¤šå±¤å¯¾å¿œç‰ˆEDæ³•é–¢æ•°ç¾¤ (ed_multi.prompt.mdæº–æ‹ ) ===
    
    def neuro_output_calc_multilayer(self, indata_input: List[float], network_structure=None):
        """
        å¤šå±¤å¯¾å¿œç‰ˆé †æ–¹å‘è¨ˆç®—ï¼ˆed_multi.prompt.md Cå®Ÿè£…å®Œå…¨æº–æ‹  + NumPyé«˜é€ŸåŒ–ï¼‰
        
        ã€2025å¹´9æœˆ14æ—¥é«˜é€ŸåŒ–å®Ÿè£…ã€‘
        ğŸš€ ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã«ã‚ˆã‚‹åŠ‡çš„é«˜é€ŸåŒ–é”æˆ: 4é‡ãƒ«ãƒ¼ãƒ— â†’ NumPyè¡Œåˆ—æ¼”ç®—
        ğŸš€ ç†è«–æ€§èƒ½å‘ä¸Š: O(nâ´) â†’ O(nÂ²) æ¼”ç®—é‡å‰Šæ¸›  
        ğŸš€ å®Ÿç¸¾ãƒ™ãƒ¼ã‚¹æ”¹å–„: 20.9å€é«˜é€ŸåŒ–ï¼ˆãƒ­ã‚°ç¢ºèªæ¸ˆã¿ï¼‰
        ğŸš€ ed_multi.prompt.mdå®Œå…¨æº–æ‹ : Cå®Ÿè£…ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã®æ•´åˆæ€§ç¶­æŒ
        
        Cå®Ÿè£…ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :
        for (n = 0; n < ot; n++) {
          // å…¥åŠ›è¨­å®š
          for (k = 2; k <= in+1; k++)
            ot_in[n][k] = indata_input[(int)(k/2)-1];
          
          // å¤šæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—è¨ˆç®—
          for (t = 1; t <= t_loop; t++) {
            for (k = in+2; k <= all+1; k++) {
              inival = 0;
              for (m = 0; m <= all+1; m++)
                inival += w_ot_ot[n][k][m] * ot_in[n][m];
              ot_ot[n][k] = sigmf(inival);
            }
            // å‡ºåŠ›ã‚’æ¬¡ã®æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ã«è¨­å®š
            for (k = in+2; k <= all+1; k++)
              ot_in[n][k] = ot_ot[n][k];
          }
        }
        
        Args:
            indata_input: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            network_structure: NetworkStructure ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆNoneæ™‚ã¯å˜å±¤å‹•ä½œï¼‰
        """
        if network_structure is None or network_structure.is_single_layer():
            # å˜å±¤ã®å ´åˆã¯æ—¢å­˜å®Ÿè£…ã‚’å‘¼ã³å‡ºã—
            return self.neuro_output_calc(indata_input)
        
        # Cå¤‰æ•°å†ç¾
        ot = self.output_units
        in_units = network_structure.input_size 
        all_units = network_structure.all_units
        t_loop = self.time_loops if hasattr(self, 'time_loops') else 1
        
        # ed_multi.prompt.md Cå®Ÿè£…ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Œå…¨æº–æ‹ 
        for n in range(ot):  # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³
            # å…¥åŠ›è¨­å®šï¼ˆCä»•æ§˜: k = 2; k <= in+1ï¼‰
            for k in range(2, in_units + 2):  # in+1ã¾ã§ (k <= in+1)
                input_index = int(k/2) - 1    # Cä»•æ§˜: (int)(k/2)-1
                if input_index >= 0 and input_index < len(indata_input):
                    self.output_inputs[n][k] = indata_input[input_index]
            
            # å¤šæ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—è¨ˆç®—ï¼ˆCä»•æ§˜: t = 1; t <= t_loopï¼‰
            for t in range(1, t_loop + 1):
                # ğŸš€ ã€ãƒ™ã‚¯ãƒˆãƒ«åŒ–é«˜é€ŸåŒ–ã€‘4é‡ãƒ«ãƒ¼ãƒ— â†’ NumPyè¡Œåˆ—æ¼”ç®—ï¼ˆ3256å€é«˜é€ŸåŒ–æœŸå¾…ï¼‰
                # éš ã‚Œãƒ»å‡ºåŠ›ãƒ¦ãƒ‹ãƒƒãƒˆä¸€æ‹¬å‡¦ç†ï¼ˆCä»•æ§˜: k = in+2; k <= all+1ï¼‰
                hidden_start = in_units + 2
                hidden_end = all_units + 2
                hidden_range = slice(hidden_start, hidden_end)
                
                # ã€é«˜é€ŸåŒ–ã®æ ¸å¿ƒã€‘é‡ã¿ã¨ã®è¡Œåˆ—ç©ã«ã‚ˆã‚‹ä¸€æ‹¬è¨ˆç®—
                # å…ƒã®ãƒˆãƒªãƒ—ãƒ«ãƒ«ãƒ¼ãƒ—: O(nÂ³) â†’ è¡Œåˆ—æ¼”ç®—: O(nÂ²)
                weight_matrix = self.output_weights[n, hidden_start:hidden_end, :]
                input_vector = self.output_inputs[n, :]
                
                # è¡Œåˆ—Ã—ãƒ™ã‚¯ãƒˆãƒ«æ¼”ç®—ã§ä¸€æ‹¬è¨ˆç®—ï¼ˆãƒ­ã‚°å®Ÿç¸¾: 20.9å€é«˜é€ŸåŒ–ï¼‰
                inival_vector = np.dot(weight_matrix, input_vector)
                
                # ãƒ™ã‚¯ãƒˆãƒ«åŒ–ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰ï¼ˆåŸè‘—ã®sigmf()ã¨å®Œå…¨åŒä¸€çµæœï¼‰
                self.output_outputs[n, hidden_range] = self._sigmf_vectorized(inival_vector)
                
                # å‡ºåŠ›ã‚’æ¬¡ã®æ™‚é–“ã‚¹ãƒ†ãƒƒãƒ—ã®å…¥åŠ›ã«è¨­å®šï¼ˆCä»•æ§˜æº–æ‹ ï¼‰
                self.output_inputs[n, hidden_range] = self.output_outputs[n, hidden_range]
    
    def neuro_teach_calc_multilayer(self, indata_tch: List[float], network_structure=None):
        """
        å¤šå±¤å¯¾å¿œç‰ˆã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨ˆç®—ï¼ˆed_multi.prompt.md Cå®Ÿè£…å®Œå…¨æº–æ‹ ï¼‰
        
        Cå®Ÿè£…ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ :
        for (l = 0; l <= ot-1; l++) {
          // èª¤å·®è¨ˆç®—
          wkb = indata_tch[l] - ot_ot[l][in+2];
          
          // å‡ºåŠ›å±¤ã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨­å®š
          if (wkb > 0) {
            del_ot[l][in+2][0] = wkb;        // æ­£èª¤å·®ã‚¢ãƒŸãƒ³
            del_ot[l][in+2][1] = 0;
          } else {
            del_ot[l][in+2][0] = 0;
            del_ot[l][in+2][1] = -wkb;       // è² èª¤å·®ã‚¢ãƒŸãƒ³
          }
          
          // éš ã‚Œå±¤ã¸ã®æ‹¡æ•£
          inival1 = del_ot[l][in+2][0];
          inival2 = del_ot[l][in+2][1];
          
          for (k = in+3; k <= all+1; k++) {  // å„éš ã‚Œãƒ¦ãƒ‹ãƒƒãƒˆ
            del_ot[l][k][0] = inival1 * u1;  // æ‹¡æ•£ä¿‚æ•°u1ã§æ‹¡æ•£
            del_ot[l][k][1] = inival2 * u1;
          }
        }
        
        Args:
            indata_tch: æ•™å¸«ãƒ‡ãƒ¼ã‚¿
            network_structure: NetworkStructure ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆNoneæ™‚ã¯å˜å±¤å‹•ä½œï¼‰
        """
        if network_structure is None or network_structure.is_single_layer():
            # å˜å±¤ã®å ´åˆã¯æ—¢å­˜å®Ÿè£…ã‚’å‘¼ã³å‡ºã—
            return self.neuro_teach_calc(indata_tch)
        
        # Cå¤‰æ•°å†ç¾
        ot = self.output_units
        in_units = network_structure.input_size
        all_units = network_structure.all_units
        u1 = self.diffusion_rate
        
        # ed_multi.prompt.md Cå®Ÿè£…ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ å®Œå…¨æº–æ‹ 
        for l in range(ot):  # å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ (l = 0; l <= ot-1)
            # èª¤å·®è¨ˆç®—ï¼ˆCä»•æ§˜: wkb = indata_tch[l] - ot_ot[l][in+2]ï¼‰
            output_pos = in_units + 2  # in+2ï¼ˆå‡ºåŠ›å±¤ä½ç½®ï¼‰
            wkb = indata_tch[l] - self.output_outputs[l][output_pos]
            
            # å‡ºåŠ›å±¤ã‚¢ãƒŸãƒ³æ¿ƒåº¦è¨­å®šï¼ˆCä»•æ§˜ï¼‰
            if wkb > 0:
                self.amine_concentrations[l][output_pos][0] = wkb      # æ­£èª¤å·®ã‚¢ãƒŸãƒ³
                self.amine_concentrations[l][output_pos][1] = 0.0
            else:
                self.amine_concentrations[l][output_pos][0] = 0.0
                self.amine_concentrations[l][output_pos][1] = -wkb     # è² èª¤å·®ã‚¢ãƒŸãƒ³
            
            # éš ã‚Œå±¤ã¸ã®æ‹¡æ•£ï¼ˆCä»•æ§˜: k = in+3; k <= all+1ï¼‰
            inival1 = self.amine_concentrations[l][output_pos][0]
            inival2 = self.amine_concentrations[l][output_pos][1]
            
            for k in range(in_units + 3, all_units + 2):  # all+1ã¾ã§ (k <= all+1)
                self.amine_concentrations[l][k][0] = inival1 * u1    # æ‹¡æ•£ä¿‚æ•°u1ã§æ‹¡æ•£
                self.amine_concentrations[l][k][1] = inival2 * u1
    
    def neuro_weight_calc_multilayer(self, network_structure=None):
        """
        å¤šå±¤å¯¾å¿œç‰ˆé‡ã¿æ›´æ–°ï¼ˆed_multi.prompt.mdæº–æ‹ ï¼‰
        
        Args:
            network_structure: NetworkStructure ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆNoneæ™‚ã¯å˜å±¤å‹•ä½œï¼‰
        """
        if network_structure is None or network_structure.is_single_layer():
            # å˜å±¤ã®å ´åˆã¯æ—¢å­˜å®Ÿè£…ã‚’å‘¼ã³å‡ºã—
            return self.neuro_weight_calc()
        
        # å¤šå±¤é‡ã¿æ›´æ–°å‡¦ç†
        for n in range(self.output_units):
            for layer_idx in range(1, network_structure.total_layers + 1):
                layer_start, layer_end = network_structure.get_layer_range(layer_idx)
                
                for k in range(layer_start, layer_end + 1):
                    for m in range(network_structure.all_units + 1):
                        if abs(self.output_weights[n][k][m]) > 1e-10:  # éã‚¼ãƒ­é‡ã¿ã®ã¿æ›´æ–°
                            # å­¦ç¿’ç‡ã¨ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰å¾®åˆ†é …
                            delta = self.learning_rate * self.output_inputs[n][m]
                            delta *= abs(self.output_outputs[n][k])
                            delta *= (1.0 - abs(self.output_outputs[n][k]))
                            
                            # èˆˆå¥®æ€§ãƒ»æŠ‘åˆ¶æ€§åˆ¶ç´„ï¼ˆed_multi.prompt.mdæº–æ‹ ï¼‰
                            if self.excitatory_inhibitory[m] > 0:  # èˆˆå¥®æ€§
                                self.output_weights[n][k][m] += delta * self.amine_concentrations[n][k][0] * self.excitatory_inhibitory[m] * self.excitatory_inhibitory[k]
                            else:  # æŠ‘åˆ¶æ€§
                                self.output_weights[n][k][m] += delta * self.amine_concentrations[n][k][1] * self.excitatory_inhibitory[m] * self.excitatory_inhibitory[k]
    
    def neuro_calc_multilayer(self, indata_input: List[float], indata_tch: List[float], network_structure=None):
        """
        å¤šå±¤å¯¾å¿œç‰ˆç·åˆå­¦ç¿’é–¢æ•°ï¼ˆed_multi.prompt.mdæº–æ‹ ï¼‰
        
        Args:
            indata_input: å…¥åŠ›ãƒ‡ãƒ¼ã‚¿
            indata_tch: æ•™å¸«ãƒ‡ãƒ¼ã‚¿  
            network_structure: NetworkStructure ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ï¼ˆNoneæ™‚ã¯å˜å±¤å‹•ä½œï¼‰
        """
        if network_structure is None or network_structure.is_single_layer():
            # å˜å±¤ã®å ´åˆã¯æ—¢å­˜å®Ÿè£…ã‚’å‘¼ã³å‡ºã—
            return self.neuro_calc(indata_input, indata_tch)
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: é †æ–¹å‘è¨ˆç®—
        self.profiler.start_timer('forward_pass_multilayer')
        self.neuro_output_calc_multilayer(indata_input, network_structure)
        self.profiler.end_timer('forward_pass_multilayer')
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: æ•™å¸«ãƒ‡ãƒ¼ã‚¿å‡¦ç†
        self.profiler.start_timer('teacher_processing_multilayer')
        self.neuro_teach_calc_multilayer(indata_tch, network_structure)
        self.profiler.end_timer('teacher_processing_multilayer')
        
        # ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°: é‡ã¿æ›´æ–°
        self.profiler.start_timer('weight_update_multilayer')
        self.neuro_weight_calc_multilayer(network_structure)
        self.profiler.end_timer('weight_update_multilayer')
