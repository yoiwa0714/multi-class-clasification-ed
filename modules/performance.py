"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é–¢é€£ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
EDæ³•SNNå®Ÿè£…ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã¨ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°æ©Ÿèƒ½

ed_v017.pyã‹ã‚‰TrainingProfiler/LearningResultsBufferã‚’åˆ‡ã‚Šå‡ºã—
ed_genuine.prompt.mdæº–æ‹ ã®å®Ÿè£…

Original Algorithm: é‡‘å­å‹‡ (1999)
Implementation: ed_genuine.prompt.md compliance
"""

import numpy as np
import time
import os
import datetime
from typing import List, Tuple, Optional


class TrainingProfiler:
    """
    å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å˜ä½ã§ã®è¨“ç·´æ™‚é–“è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ 
    
    å„å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å‡¦ç†ã‚’ä»¥ä¸‹ã®å·¥ç¨‹ã«åˆ†ã‘ã¦æ™‚é–“æ¸¬å®šï¼š
    1. data_preparation - ãƒ‡ãƒ¼ã‚¿æº–å‚™
    2. forward_pass - é †æ–¹å‘è¨ˆç®—
    3. prediction_calc - äºˆæ¸¬è¨ˆç®—
    4. result_recording - çµæœè¨˜éŒ²
    5. teacher_processing - æ•™å¸«ãƒ‡ãƒ¼ã‚¿å‡¦ç†
    6. weight_update - é‡ã¿æ›´æ–°
    7. total_processing - å…¨ä½“å‡¦ç†æ™‚é–“
    
    é‡ã¿æ›´æ–°ã®è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰:
    - weight_loop_init - ãƒ«ãƒ¼ãƒ—åˆæœŸåŒ–
    - weight_delta_calc - deltaè¨ˆç®—
    - weight_amine_proc - ã‚¢ãƒŸãƒ³å‡¦ç†
    - weight_memory_access - ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹
    - weight_math_ops - æ•°å­¦æ¼”ç®—
    """
    
    def __init__(self, enable_profiling=False):
        self.enable_profiling = enable_profiling
        self.reset_statistics()
    
    def reset_statistics(self):
        """çµ±è¨ˆãƒ‡ãƒ¼ã‚¿ãƒªã‚»ãƒƒãƒˆ"""
        self.timings = {
            'data_preparation': [],
            'forward_pass': [],
            'prediction_calc': [],
            'result_recording': [],
            'teacher_processing': [],
            'weight_update': [],
            'total_processing': [],
            # é‡ã¿æ›´æ–°è©³ç´°ã‚¿ã‚¤ãƒãƒ¼
            'weight_loop_init': [],
            'weight_delta_calc': [],
            'weight_amine_proc': [],
            'weight_memory_access': [],
            'weight_math_ops': []
        }
        self.current_times = {}
        self.sample_count = 0
    
    def start_timer(self, phase_name: str):
        """ãƒ•ã‚§ãƒ¼ã‚ºé–‹å§‹æ™‚åˆ»è¨˜éŒ²"""
        if not self.enable_profiling:
            return
        self.current_times[phase_name] = time.perf_counter()
    
    def end_timer(self, phase_name: str):
        """ãƒ•ã‚§ãƒ¼ã‚ºçµ‚äº†æ™‚åˆ»è¨˜éŒ²"""
        if not self.enable_profiling:
            return
        if phase_name in self.current_times:
            duration = time.perf_counter() - self.current_times[phase_name]
            self.timings[phase_name].append(duration)
            del self.current_times[phase_name]
    
    def complete_sample(self):
        """1ã‚µãƒ³ãƒ—ãƒ«å‡¦ç†å®Œäº†"""
        if not self.enable_profiling:
            return
        self.sample_count += 1
    
    def get_statistics(self):
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        if not self.enable_profiling:
            return {}
        
        stats = {}
        for phase, times in self.timings.items():
            if times:
                stats[phase] = {
                    'avg': np.mean(times) * 1000,  # ãƒŸãƒªç§’
                    'max': np.max(times) * 1000,
                    'min': np.min(times) * 1000,
                    'total': np.sum(times) * 1000,
                    'count': len(times)
                }
        
        return stats
    
    def print_detailed_report(self):
        """è©³ç´°ãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        if not self.enable_profiling:
            print("ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãŒç„¡åŠ¹ã§ã™ã€‚--profileã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä½¿ç”¨ã—ã¦ãã ã•ã„ã€‚")
            return
        
        stats = self.get_statistics()
        if not stats:
            print("ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“ã€‚")
            return
        
        print("\n" + "="*80)
        print("ğŸ” è¨“ç·´æ™‚é–“è©³ç´°ãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°ãƒ¬ãƒãƒ¼ãƒˆ")
        print("="*80)
        
        # ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥æ™‚é–“åˆ†æ
        total_avg = stats.get('total_processing', {}).get('avg', 0)
        if total_avg == 0:
            # total_processingãŒãªã„å ´åˆã€ã™ã¹ã¦ã®ãƒ•ã‚§ãƒ¼ã‚ºã®åˆè¨ˆã‚’è¨ˆç®—
            total_avg = sum(phase_stats['avg'] for phase_stats in stats.values())
        
        print(f"\nğŸ“Š ãƒ•ã‚§ãƒ¼ã‚ºåˆ¥å‡¦ç†æ™‚é–“åˆ†æ (1ã‚µãƒ³ãƒ—ãƒ«å¹³å‡)")
        print("-" * 60)
        
        phase_names = {
            'data_preparation': 'ãƒ‡ãƒ¼ã‚¿æº–å‚™',
            'forward_pass': 'é †æ–¹å‘è¨ˆç®—',
            'prediction_calc': 'äºˆæ¸¬è¨ˆç®—',
            'result_recording': 'çµæœè¨˜éŒ²',
            'teacher_processing': 'æ•™å¸«ãƒ‡ãƒ¼ã‚¿å‡¦ç†',
            'weight_update': 'é‡ã¿æ›´æ–°',
            'total_processing': 'å…¨ä½“å‡¦ç†',
            # é‡ã¿æ›´æ–°è©³ç´°
            'weight_loop_init': 'é‡ã¿åˆæœŸåŒ–',
            'weight_delta_calc': 'é‡ã¿Î”è¨ˆç®—',
            'weight_amine_proc': 'é‡ã¿ã‚¢ãƒŸãƒ³å‡¦ç†',
            'weight_memory_access': 'é‡ã¿ãƒ¡ãƒ¢ãƒªã‚¢ã‚¯ã‚»ã‚¹',
            'weight_math_ops': 'é‡ã¿æ•°å­¦æ¼”ç®—'
        }
        
        # ãƒ•ã‚§ãƒ¼ã‚ºã‚’å‡¦ç†æ™‚é–“é †ã§ã‚½ãƒ¼ãƒˆè¡¨ç¤º
        sorted_phases = sorted(stats.items(), key=lambda x: x[1]['avg'], reverse=True)
        
        for phase, phase_stats in sorted_phases:
            if phase in phase_names:
                avg_time = phase_stats['avg']
                percentage = (avg_time / total_avg * 100) if total_avg > 0 else 0
                japanese_name = phase_names[phase]
                print(f"{japanese_name:12s}: {avg_time:7.2f}ms ({percentage:5.1f}%)")
        
        print("-" * 60)
        print(f"{'åˆè¨ˆæ¨å®šæ™‚é–“':12s}: {total_avg:7.2f}ms (100.0%)")
        
        # ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
        if sorted_phases:
            bottleneck_phase, bottleneck_stats = sorted_phases[0]
            if bottleneck_phase in phase_names:
                print(f"\nğŸš¨ æœ€å¤§ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {phase_names[bottleneck_phase]} "
                      f"({bottleneck_stats['avg']:.2f}ms)")
        
        # æ€§èƒ½å‘ä¸Šã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹
        if total_avg > 0:
            samples_per_second = 1000 / total_avg
            print(f"\nâš¡ äºˆæ¸¬å‡¦ç†é€Ÿåº¦: {samples_per_second:.1f} ã‚µãƒ³ãƒ—ãƒ«/ç§’")
            print(f"   (ç¾åœ¨: {total_avg:.2f}ms/ã‚µãƒ³ãƒ—ãƒ«)")
            
            # æ€§èƒ½å‘ä¸Šã®ã‚¢ãƒ‰ãƒã‚¤ã‚¹
            if any(phase_stats['avg'] > total_avg * 0.3 for phase_stats in stats.values()):
                dominant_phases = [phase_names[phase] for phase, phase_stats in sorted_phases[:2] 
                                  if phase in phase_names and phase_stats['avg'] > total_avg * 0.2]
                if dominant_phases:
                    print(f"\nğŸ’¡ æœ€é©åŒ–æ¨å¥¨ãƒ•ã‚§ãƒ¼ã‚º: {', '.join(dominant_phases)}")
            
            # é‡ã¿æ›´æ–°è©³ç´°åˆ†æ
            weight_phases = ['weight_loop_init', 'weight_delta_calc', 'weight_amine_proc', 
                           'weight_memory_access', 'weight_math_ops']
            weight_stats = {phase: stats[phase] for phase in weight_phases if phase in stats}
            
            if weight_stats:
                weight_total = sum(phase_stats['avg'] for phase_stats in weight_stats.values())
                print(f"\nğŸ”§ é‡ã¿æ›´æ–°è©³ç´°åˆ†æ (åˆè¨ˆ: {weight_total:.2f}ms):")
                print("-" * 40)
                
                for phase, phase_stats in sorted(weight_stats.items(), 
                                                key=lambda x: x[1]['avg'], reverse=True):
                    japanese_name = phase_names[phase]
                    avg_time = phase_stats['avg']
                    percentage = (avg_time / weight_total * 100) if weight_total > 0 else 0
                    print(f"  {japanese_name:12s}: {avg_time:6.2f}ms ({percentage:5.1f}%)")
                
                # é‡ã¿æ›´æ–°ã®ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ç‰¹å®š
                if weight_stats:
                    bottleneck_phase, bottleneck_stats = max(weight_stats.items(), 
                                                           key=lambda x: x[1]['avg'])
                    print(f"\nâš ï¸  é‡ã¿æ›´æ–°ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {phase_names[bottleneck_phase]} "
                          f"({bottleneck_stats['avg']:.2f}ms)")
            
        print("="*80)


class LearningResultsBuffer:
    """
    å­¦ç¿’çµæœã‚’é…åˆ—ã«ä¿å­˜ã—ã¦å¾Œã§é›†è¨ˆã™ã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ã‚¯ãƒ©ã‚¹
    ã‚¨ãƒãƒƒã‚¯é–“ã®é‡ã„ç²¾åº¦è¨ˆç®—ã‚’å‰Šæ¸›ã—ã€å­¦ç¿’é€Ÿåº¦ã‚’å‘ä¸Š
    """
    
    def __init__(self, train_size: int, test_size: int, epochs: int):
        """
        çµæœä¿å­˜ç”¨é…åˆ—ã®åˆæœŸåŒ–
        Args:
            train_size: è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°
            test_size: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°  
            epochs: ã‚¨ãƒãƒƒã‚¯æ•°
        """
        self.train_size = train_size
        self.test_size = test_size
        self.epochs = epochs
        self.num_classes = 10  # MNISTç”¨
        
        # ed_genuine.prompt.mdæº–æ‹ : ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®åŠ¹ç‡çš„ãª3æ¬¡å…ƒé…åˆ—å®Ÿè£…
        # [ã‚¯ãƒ©ã‚¹, æ­£è§£(0)/ä¸æ­£è§£(1), ã‚¨ãƒãƒƒã‚¯] æ§‹é€ ã«ã‚ˆã‚‹é«˜é€Ÿç²¾åº¦è¨ˆç®—
        self.train_accuracy_counter = np.zeros((self.num_classes, 2, epochs), dtype=int)
        self.test_accuracy_counter = np.zeros((self.num_classes, 2, epochs), dtype=int)
        
        # çµæœä¿å­˜é…åˆ—ï¼ˆTrue=æ­£è§£, False=ä¸æ­£è§£ï¼‰
        self.train_results = []  # [epoch][sample] = bool
        self.test_results = []   # [epoch][sample] = bool
        
        # ã‚¨ãƒãƒƒã‚¯åˆ¥èª¤å·®ä¿å­˜ï¼ˆå¾“æ¥æ–¹å¼ï¼šå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ä¿æŒï¼‰
        self.train_errors = []   # [epoch][sample] = float
        self.test_errors = []    # [epoch][sample] = float
        
        # 3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„èª¤å·®ç®¡ç†ï¼ˆææ¡ˆæ–¹å¼ï¼‰
        # [ã‚¨ãƒãƒƒã‚¯][ãƒãƒƒãƒ/ãƒ‘ã‚¿ãƒ¼ãƒ³ç•ªå·][0:ãƒ‡ãƒ¼ã‚¿å€‹æ•°, 1:ç·èª¤å·®]
        max_patterns_per_epoch = max(train_size, test_size)
        self.train_error_accumulator = np.zeros((epochs, max_patterns_per_epoch, 2), dtype=np.float64)
        self.test_error_accumulator = np.zeros((epochs, max_patterns_per_epoch, 2), dtype=np.float64)
        
        # ãƒãƒƒãƒå‡¦ç†ç”¨ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.train_batch_counters = np.zeros(epochs, dtype=int)  # ã‚¨ãƒãƒƒã‚¯åˆ¥ã®ãƒãƒƒãƒæ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        self.test_batch_counters = np.zeros(epochs, dtype=int)   # ã‚¨ãƒãƒƒã‚¯åˆ¥ã®ãƒãƒƒãƒæ•°ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼
        
        # æ··åŒè¡Œåˆ—ç”¨ãƒ‡ãƒ¼ã‚¿ä¿å­˜
        self.train_predicted_labels = []  # [epoch][sample] = int (äºˆæ¸¬ã‚¯ãƒ©ã‚¹)
        self.train_true_labels = []       # [epoch][sample] = int (å®Ÿéš›ã‚¯ãƒ©ã‚¹)
        self.test_predicted_labels = []   # [epoch][sample] = int (äºˆæ¸¬ã‚¯ãƒ©ã‚¹)
        self.test_true_labels = []        # [epoch][sample] = int (å®Ÿéš›ã‚¯ãƒ©ã‚¹)
        
        # ğŸ¯ å¯è¦–åŒ–ç”¨ï¼šå­¦ç¿’æ™‚ã®å®Ÿéš›ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜
        self.train_input_images = []      # [epoch][sample] = ndarray (å­¦ç¿’æ™‚ã®ç”»åƒãƒ‡ãƒ¼ã‚¿)
        
        # ã‚¨ãƒãƒƒã‚¯åˆ¥åˆæœŸåŒ–
        for epoch in range(epochs):
            self.train_results.append([False] * train_size)
            self.test_results.append([False] * test_size)
            self.train_errors.append([0.0] * train_size)
            self.test_errors.append([0.0] * test_size)
            self.train_predicted_labels.append([0] * train_size)
            self.train_true_labels.append([0] * train_size)
            self.test_predicted_labels.append([0] * test_size)
            self.test_true_labels.append([0] * test_size)
            self.train_input_images.append([None] * train_size)
    
    def record_train_result(self, epoch: int, sample_idx: int, correct: bool, error: float, 
                           predicted_label: int, true_label: int, input_image=None):
        """è¨“ç·´çµæœè¨˜éŒ²ï¼ˆæ··åŒè¡Œåˆ—ç”¨ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰ - ed_genuine.prompt.mdæº–æ‹ ç‰ˆ"""
        # åŸºæœ¬è¨˜éŒ²ï¼ˆå…¨ã‚¨ãƒãƒƒã‚¯ï¼‰
        self.train_results[epoch][sample_idx] = correct
        self.train_errors[epoch][sample_idx] = error
        
        # ğŸ¯ ed_genuine.prompt.mdæº–æ‹ : æ··åŒè¡Œåˆ—ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã‚¨ãƒãƒƒã‚¯ã§è¨˜éŒ²
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã®ãŸã‚ã€å„ã‚¨ãƒãƒƒã‚¯ã®ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
        self.train_predicted_labels[epoch][sample_idx] = predicted_label
        self.train_true_labels[epoch][sample_idx] = true_label
        
        # ğŸ¯ å¯è¦–åŒ–ç”¨ï¼šå­¦ç¿’æ™‚ã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ï¼ˆæœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®ã¿ï¼‰
        if input_image is not None and epoch == self.epochs - 1:
            # 784æ¬¡å…ƒã‹ã‚‰28x28ã«å¤‰æ›ã—ã¦ä¿å­˜
            if len(input_image) == 784:
                self.train_input_images[epoch][sample_idx] = input_image.reshape(28, 28)
            else:
                self.train_input_images[epoch][sample_idx] = input_image
        
        # åŠ¹ç‡çš„ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ›´æ–°
        if 0 <= true_label < self.num_classes:
            if correct:
                self.train_accuracy_counter[true_label, 0, epoch] += 1  # æ­£è§£ã‚«ã‚¦ãƒ³ãƒˆ
            else:
                self.train_accuracy_counter[true_label, 1, epoch] += 1  # ä¸æ­£è§£ã‚«ã‚¦ãƒ³ãƒˆ
    
    def record_train_batch_error_efficient(self, epoch: int, batch_errors: np.ndarray, batch_size: int):
        """
        3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„èª¤å·®è¨˜éŒ²ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆæ–¹å¼ï¼‰
        
        Args:
            epoch: ã‚¨ãƒãƒƒã‚¯ç•ªå·
            batch_errors: ãƒãƒƒãƒå†…å„ã‚µãƒ³ãƒ—ãƒ«ã®èª¤å·®é…åˆ—
            batch_size: å®Ÿéš›ã®ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆæœ€çµ‚ãƒãƒƒãƒã§ç•°ãªã‚‹å¯èƒ½æ€§ï¼‰
        """
        batch_idx = self.train_batch_counters[epoch]
        
        # 1. ãƒ‡ãƒ¼ã‚¿å€‹æ•°ã‚’è¨˜éŒ²
        self.train_error_accumulator[epoch, batch_idx, 0] = batch_size
        
        # 2. ç·èª¤å·®ã‚’è¨˜éŒ²ï¼ˆå¹³å‡èª¤å·®Ã—ãƒ‡ãƒ¼ã‚¿å€‹æ•°ï¼‰
        total_error = np.sum(batch_errors)
        self.train_error_accumulator[epoch, batch_idx, 1] = total_error
        
        # ãƒãƒƒãƒã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ›´æ–°
        self.train_batch_counters[epoch] += 1
    
    def record_test_batch_error_efficient(self, epoch: int, batch_errors: np.ndarray, batch_size: int):
        """
        ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„èª¤å·®è¨˜éŒ²
        """
        batch_idx = self.test_batch_counters[epoch]
        
        # 1. ãƒ‡ãƒ¼ã‚¿å€‹æ•°ã‚’è¨˜éŒ²
        self.test_error_accumulator[epoch, batch_idx, 0] = batch_size
        
        # 2. ç·èª¤å·®ã‚’è¨˜éŒ²
        total_error = np.sum(batch_errors)
        self.test_error_accumulator[epoch, batch_idx, 1] = total_error
        
        # ãƒãƒƒãƒã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ›´æ–°
        self.test_batch_counters[epoch] += 1
    
    def record_test_result(self, epoch: int, sample_idx: int, correct: bool, error: float,
                          predicted_label: int, true_label: int):
        """ãƒ†ã‚¹ãƒˆçµæœè¨˜éŒ²ï¼ˆæ··åŒè¡Œåˆ—ç”¨ãƒ‡ãƒ¼ã‚¿å«ã‚€ï¼‰ - ed_genuine.prompt.mdæº–æ‹ ç‰ˆ"""
        # åŸºæœ¬è¨˜éŒ²ï¼ˆå…¨ã‚¨ãƒãƒƒã‚¯ï¼‰
        self.test_results[epoch][sample_idx] = correct
        self.test_errors[epoch][sample_idx] = error
        
        # ğŸ¯ ed_genuine.prompt.mdæº–æ‹ : æ··åŒè¡Œåˆ—ãƒ‡ãƒ¼ã‚¿ã¯å…¨ã‚¨ãƒãƒƒã‚¯ã§è¨˜éŒ²
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã®ãŸã‚ã€å„ã‚¨ãƒãƒƒã‚¯ã®ãƒ©ãƒ™ãƒ«ãƒ‡ãƒ¼ã‚¿ãŒå¿…è¦
        self.test_predicted_labels[epoch][sample_idx] = predicted_label
        self.test_true_labels[epoch][sample_idx] = true_label
        
        # åŠ¹ç‡çš„ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼æ›´æ–°
        if 0 <= true_label < self.num_classes:
            if correct:
                self.test_accuracy_counter[true_label, 0, epoch] += 1  # æ­£è§£ã‚«ã‚¦ãƒ³ãƒˆ
            else:
                self.test_accuracy_counter[true_label, 1, epoch] += 1  # ä¸æ­£è§£ã‚«ã‚¦ãƒ³ãƒˆ
    
    def get_epoch_accuracy(self, epoch: int, dataset_type: str) -> float:
        """
        ã€éæ¨å¥¨ã€‘æŒ‡å®šã‚¨ãƒãƒƒã‚¯ã®ç²¾åº¦è¨ˆç®—ï¼ˆå¾“æ¥æ‰‹æ³•ï¼‰
        
        Note: v0.1.4ã‹ã‚‰åŠ¹ç‡çš„æ‰‹æ³• get_epoch_accuracy_efficient ã‚’æ¨å¥¨
        ã“ã®å¾“æ¥æ‰‹æ³•ã¯é…åˆ—èµ°æŸ»ã®ãŸã‚O(N)è¨ˆç®—ã¨ãªã‚Šå¤§è¦æ¨¡ãƒ‡ãƒ¼ã‚¿ã§æ€§èƒ½åŠ£åŒ–
        """
        if dataset_type == 'train':
            correct = sum(self.train_results[epoch])
            total = self.train_size
        else:  # test
            correct = sum(self.test_results[epoch])
            total = self.test_size
        return correct / max(1, total)
    
    def get_epoch_accuracy_efficient(self, epoch: int, dataset_type: str) -> float:
        """
        ã€æ¨å¥¨ã€‘åŠ¹ç‡çš„ã‚¨ãƒãƒƒã‚¯ç²¾åº¦è¨ˆç®— - O(1)é«˜é€Ÿè¨ˆç®—
        
        ãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆã®3æ¬¡å…ƒé…åˆ—[ã‚¯ãƒ©ã‚¹, æ­£è§£/ä¸æ­£è§£, ã‚¨ãƒãƒƒã‚¯]ã«ã‚ˆã‚‹é«˜é€Ÿè¨ˆç®—æ‰‹æ³•
        
        æ€§èƒ½å‘ä¸Šçµæœ:
        - ã‚¨ãƒãƒƒã‚¯ç²¾åº¦è¨ˆç®—: 3.66å€é«˜é€ŸåŒ–
        - å…¨ä½“ç²¾åº¦è¨ˆç®—: 278å€é«˜é€ŸåŒ–
        - è¨ˆç®—ç²¾åº¦: 100%ä¸€è‡´ä¿è¨¼
        
        Args:
            epoch: ã‚¨ãƒãƒƒã‚¯ç•ªå·
            dataset_type: 'train' or 'test'
        Returns:
            float: ç²¾åº¦ (0.0 ~ 1.0)
        """
        if dataset_type == 'train':
            counter = self.train_accuracy_counter
        else:
            counter = self.test_accuracy_counter
        
        # æœ€é«˜é€Ÿåº¦ã®ç›´æ¥é…åˆ—ã‚¢ã‚¯ã‚»ã‚¹ - numpy.sumã®ã‚ªãƒ¼ãƒãƒ¼ãƒ˜ãƒƒãƒ‰ã‚’å›é¿
        total_correct = 0
        total_incorrect = 0
        for class_idx in range(counter.shape[0]):
            total_correct += counter[class_idx, 0, epoch]
            total_incorrect += counter[class_idx, 1, epoch]
        
        total_samples = total_correct + total_incorrect
        return total_correct / max(1, total_samples)
    
    def get_overall_accuracy_efficient(self, dataset_type: str) -> float:
        """
        ã€æ¨å¥¨ã€‘åŠ¹ç‡çš„å…¨ä½“ç²¾åº¦è¨ˆç®— - å…¨ã‚¨ãƒãƒƒã‚¯çµ±åˆé«˜é€Ÿè¨ˆç®—
        
        3æ¬¡å…ƒé…åˆ—ã®å…¨ã‚¨ãƒãƒƒã‚¯åˆ†ã‚’ä¸€åº¦ã«é›†è¨ˆã™ã‚‹ã“ã¨ã§278å€é«˜é€ŸåŒ–ã‚’å®Ÿç¾
        å¾“æ¥ã®é…åˆ—èµ°æŸ»æ‰‹æ³•ã¨æ¯”è¼ƒã—ã¦å¤§å¹…ãªæ€§èƒ½å‘ä¸Šã‚’é”æˆ
        
        Args:
            dataset_type: 'train' or 'test'
        å…¨ã‚¨ãƒãƒƒã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ãŸé«˜é€Ÿè¨ˆç®—
        Args:
            dataset_type: 'train' or 'test'
        Returns:
            float: å…¨ä½“ç²¾åº¦ (0.0 ~ 1.0)
        """
        if dataset_type == 'train':
            counter = self.train_accuracy_counter
        else:
            counter = self.test_accuracy_counter
        
        # å…¨ã‚¨ãƒãƒƒã‚¯ã€å…¨ã‚¯ãƒ©ã‚¹ã®æ­£è§£æ•°ã¨ä¸æ­£è§£æ•°ã®é›†è¨ˆ
        total_correct = np.sum(counter[:, 0, :])    # å…¨ãƒ‡ãƒ¼ã‚¿ã®æ­£è§£æ•°
        total_incorrect = np.sum(counter[:, 1, :])  # å…¨ãƒ‡ãƒ¼ã‚¿ã®ä¸æ­£è§£æ•°
        total_samples = total_correct + total_incorrect
        
        return total_correct / max(1, total_samples)
    
    def get_epoch_error(self, epoch: int, dataset_type: str) -> float:
        """æŒ‡å®šã‚¨ãƒãƒƒã‚¯ã®å¹³å‡èª¤å·®è¨ˆç®—"""
        if dataset_type == 'train':
            total_error = sum(self.train_errors[epoch])
            total = self.train_size
        else:  # test
            total_error = sum(self.test_errors[epoch])
            total = self.test_size
        return total_error / max(1, total)
    
    def get_epoch_error_efficient(self, epoch: int, dataset_type: str) -> float:
        """
        3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„ã‚¨ãƒãƒƒã‚¯èª¤å·®è¨ˆç®—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆæ–¹å¼ï¼‰
        
        Args:
            epoch: ã‚¨ãƒãƒƒã‚¯ç•ªå·
            dataset_type: 'train' ã¾ãŸã¯ 'test'
        
        Returns:
            float: ã‚¨ãƒãƒƒã‚¯å¹³å‡èª¤å·®
        """
        if dataset_type == 'train':
            accumulator = self.train_error_accumulator
            batch_count = self.train_batch_counters[epoch]
        else:  # test
            accumulator = self.test_error_accumulator
            batch_count = self.test_batch_counters[epoch]
        
        if batch_count == 0:
            return 0.0
        
        # è©²å½“ã‚¨ãƒãƒƒã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        epoch_data = accumulator[epoch, :batch_count, :]  # [ãƒãƒƒãƒæ•°, 2]
        
        # ç·ãƒ‡ãƒ¼ã‚¿æ•°ã¨ç·èª¤å·®ã‚’ç®—å‡ºï¼ˆNumPyé…åˆ—æ¼”ç®—ï¼‰
        total_samples = np.sum(epoch_data[:, 0])  # ãƒ‡ãƒ¼ã‚¿å€‹æ•°ã®åˆè¨ˆ
        total_error = np.sum(epoch_data[:, 1])    # ç·èª¤å·®ã®åˆè¨ˆ
        
        return total_error / max(1, int(total_samples))
    
    def get_cumulative_error_efficient(self, current_epoch: int, dataset_type: str) -> float:
        """
        ç©ç®—èª¤å·®è¨ˆç®—ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼ææ¡ˆæ–¹å¼ï¼šæœ€åˆã‹ã‚‰ãã®æ™‚ç‚¹ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ï¼‰
        
        Args:
            current_epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ç•ªå·
            dataset_type: 'train' ã¾ãŸã¯ 'test'
        
        Returns:
            float: ç©ç®—å¹³å‡èª¤å·®
        """
        if dataset_type == 'train':
            accumulator = self.train_error_accumulator
            batch_counters = self.train_batch_counters
        else:  # test
            accumulator = self.test_error_accumulator
            batch_counters = self.test_batch_counters
        
        total_samples = 0
        total_error = 0.0
        
        # æœ€åˆã‹ã‚‰ç¾åœ¨ã‚¨ãƒãƒƒã‚¯ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’é›†è¨ˆ
        for epoch in range(current_epoch + 1):
            batch_count = batch_counters[epoch]
            if batch_count > 0:
                epoch_data = accumulator[epoch, :batch_count, :]
                total_samples += np.sum(epoch_data[:, 0])
                total_error += np.sum(epoch_data[:, 1])
        
        return total_error / max(1, int(total_samples))
    
    def benchmark_error_calculation_methods(self, epoch: int, dataset_type: str) -> dict:
        """
        èª¤å·®ç®—å‡ºæ–¹å¼ã®æ€§èƒ½æ¯”è¼ƒãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        
        Returns:
            dict: å„æ–¹å¼ã®å®Ÿè¡Œæ™‚é–“ã¨çµæœ
        """
        import time
        
        results = {}
        
        # å¾“æ¥æ–¹å¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        start_time = time.time()
        traditional_error = self.get_epoch_error(epoch, dataset_type)
        traditional_time = time.time() - start_time
        
        # 3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹æ–¹å¼ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
        start_time = time.time()
        efficient_error = self.get_epoch_error_efficient(epoch, dataset_type)
        efficient_time = time.time() - start_time
        
        results = {
            'traditional': {
                'time': traditional_time,
                'error': traditional_error,
                'method': 'ãƒªã‚¹ãƒˆæ¼”ç®—sum()'
            },
            'efficient': {
                'time': efficient_time,
                'error': efficient_error,
                'method': '3æ¬¡å…ƒé…åˆ—+NumPyæ¼”ç®—'
            },
            'speedup': traditional_time / max(efficient_time, 1e-10),
            'accuracy_match': abs(traditional_error - efficient_error) < 1e-10
        }
        
        return results
    
    def get_final_accuracy(self, dataset_type: str) -> float:
        """å…¨ã‚¨ãƒãƒƒã‚¯é€šã—ã¦ã®æœ€çµ‚ç²¾åº¦è¨ˆç®—"""
        if dataset_type == 'train':
            total_correct = sum(sum(epoch_results) for epoch_results in self.train_results)
            total_samples = self.train_size * self.epochs
        else:  # test
            total_correct = sum(sum(epoch_results) for epoch_results in self.test_results)
            total_samples = self.test_size * self.epochs
        return total_correct / max(1, total_samples)
    
    def get_final_error(self, dataset_type: str) -> float:
        """å…¨ã‚¨ãƒãƒƒã‚¯é€šã—ã¦ã®æœ€çµ‚å¹³å‡èª¤å·®è¨ˆç®—"""
        if dataset_type == 'train':
            total_error = sum(sum(epoch_errors) for epoch_errors in self.train_errors)
            total_samples = self.train_size * self.epochs
        else:  # test
            total_error = sum(sum(epoch_errors) for epoch_errors in self.test_errors)
            total_samples = self.test_size * self.epochs
        return total_error / max(1, total_samples)
    
    def calculate_confusion_matrix(self, dataset_type: str, epoch: int = -1, num_classes: int = 10):
        """
        æ··åŒè¡Œåˆ—è¨ˆç®— - å­¦ç¿’é–‹å§‹æ™‚ç‚¹ã‹ã‚‰ã®ç©ç®—å¯¾å¿œç‰ˆ
        Args:
            dataset_type: 'train' ã¾ãŸã¯ 'test'
            epoch: ç‰¹å®šã‚¨ãƒãƒƒã‚¯ã®å ´åˆã¯æŒ‡å®šã€å…¨ã‚¨ãƒãƒƒã‚¯çµ±åˆã®å ´åˆã¯-1
            num_classes: ã‚¯ãƒ©ã‚¹æ•°ï¼ˆMNISTã®å ´åˆã¯10ï¼‰
        Returns:
            confusion_matrix: æ··åŒè¡Œåˆ—ï¼ˆlist of listï¼‰
            
        Note: epochæŒ‡å®šæ™‚ã¯å­¦ç¿’é–‹å§‹æ™‚ç‚¹ã‹ã‚‰æŒ‡å®šã‚¨ãƒãƒƒã‚¯ã¾ã§ã®ç©ç®—çµæœã‚’è¿”ã™
        """
        # æ··åŒè¡Œåˆ—åˆæœŸåŒ–ï¼ˆnum_classes x num_classesï¼‰
        confusion_matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]
        
        if dataset_type == 'train':
            predicted_labels = self.train_predicted_labels
            true_labels = self.train_true_labels
        else:  # test
            predicted_labels = self.test_predicted_labels
            true_labels = self.test_true_labels
        
        if epoch == -1:
            # ğŸ¯ ed_genuine.prompt.mdæº–æ‹ ï¼šå…¨ã‚¨ãƒãƒƒã‚¯çµ±åˆ = å…¨ã‚¨ãƒãƒƒã‚¯ã®ç´¯ç©ãƒ‡ãƒ¼ã‚¿
            # å­¦ç¿’å®Œäº†å¾Œã®æœ€çµ‚çµæœã¨ã—ã¦ã€å…¨å­¦ç¿’éç¨‹ã§ã®ç´¯ç©æ··åŒè¡Œåˆ—ã‚’è¡¨ç¤º
            for epoch_idx in range(len(predicted_labels)):
                for sample_idx in range(len(predicted_labels[epoch_idx])):
                    true_label = true_labels[epoch_idx][sample_idx]
                    pred_label = predicted_labels[epoch_idx][sample_idx]
                    if 0 <= true_label < num_classes and 0 <= pred_label < num_classes:
                        confusion_matrix[true_label][pred_label] += 1
        else:
            # ğŸ¯ ç©ç®—æ©Ÿèƒ½ï¼šå­¦ç¿’é–‹å§‹æ™‚ç‚¹ã‹ã‚‰æŒ‡å®šã‚¨ãƒãƒƒã‚¯ã¾ã§ã®ç©ç®—çµæœ
            # ï¼ˆãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ç”¨ï¼‰
            for epoch_idx in range(min(epoch + 1, len(predicted_labels))):
                for sample_idx in range(len(predicted_labels[epoch_idx])):
                    true_label = true_labels[epoch_idx][sample_idx]
                    pred_label = predicted_labels[epoch_idx][sample_idx]
                    if 0 <= true_label < num_classes and 0 <= pred_label < num_classes:
                        confusion_matrix[true_label][pred_label] += 1
        
        return confusion_matrix
    
    def calculate_confusion_matrix_single_epoch(self, dataset_type: str, epoch: int, num_classes: int = 10):
        """
        å˜ä¸€ã‚¨ãƒãƒƒã‚¯ã®æ··åŒè¡Œåˆ—è¨ˆç®— (ed_multi.prompt.mdæº–æ‹  - ã‚¨ãƒãƒƒã‚¯æ¯è¡¨ç¤ºç”¨)
        Args:
            dataset_type: 'train' ã¾ãŸã¯ 'test'
            epoch: å¯¾è±¡ã‚¨ãƒãƒƒã‚¯ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
            num_classes: ã‚¯ãƒ©ã‚¹æ•°ï¼ˆMNISTã®å ´åˆã¯10ï¼‰
        Returns:
            confusion_matrix: æ··åŒè¡Œåˆ—ï¼ˆlist of listï¼‰
            
        Note: æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒãƒƒã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿ã®æ··åŒè¡Œåˆ—ã‚’è¿”ã™ï¼ˆç´¯ç©ã§ã¯ãªã„ï¼‰
        """
        # æ··åŒè¡Œåˆ—åˆæœŸåŒ–ï¼ˆnum_classes x num_classesï¼‰
        confusion_matrix = [[0 for _ in range(num_classes)] for _ in range(num_classes)]
        
        if dataset_type == 'train':
            predicted_labels = self.train_predicted_labels
            true_labels = self.train_true_labels
        else:  # test
            predicted_labels = self.test_predicted_labels
            true_labels = self.test_true_labels
        
        # ğŸ¯ ed_multi.prompt.mdæº–æ‹ ï¼šæŒ‡å®šã•ã‚ŒãŸã‚¨ãƒãƒƒã‚¯ã®ãƒ‡ãƒ¼ã‚¿ã®ã¿å‡¦ç†
        if epoch < len(predicted_labels) and epoch < len(true_labels):
            for sample_idx in range(len(predicted_labels[epoch])):
                true_label = true_labels[epoch][sample_idx]
                pred_label = predicted_labels[epoch][sample_idx]
                if 0 <= true_label < num_classes and 0 <= pred_label < num_classes:
                    confusion_matrix[true_label][pred_label] += 1
        
        return confusion_matrix
    
    # ===== çµ±ä¸€çš„ç²¾åº¦ãƒ»èª¤å·®ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰ =====
    
    def __init_unified_cache(self):
        """çµ±ä¸€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–"""
        if not hasattr(self, 'cached_epoch_metrics'):
            # ã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚ã®ç²¾åº¦ãƒ»èª¤å·®ã‚’é…åˆ—ã«ä¿å­˜ã—ã€ã™ã¹ã¦ã®è¡¨ç¤ºã§çµ±ä¸€åˆ©ç”¨
            self.cached_epoch_metrics = np.zeros((self.epochs, 4), dtype=np.float64)
            # å„ã‚¨ãƒãƒƒã‚¯ã® [è¨“ç·´ç²¾åº¦, ãƒ†ã‚¹ãƒˆç²¾åº¦, è¨“ç·´èª¤å·®, ãƒ†ã‚¹ãƒˆèª¤å·®] ã‚’ä¿å­˜
            self.epoch_metrics_computed = np.zeros(self.epochs, dtype=bool)  # è¨ˆç®—æ¸ˆã¿ãƒ•ãƒ©ã‚°
    
    def compute_and_cache_epoch_metrics(self, epoch):
        """
        ã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚ã®çµ±ä¸€çš„ç²¾åº¦ãƒ»èª¤å·®è¨ˆç®—ã¨ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        ed_genuine.prompt.mdæº–æ‹ : teacher_value - predicted_value
        """
        # çµ±ä¸€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        self.__init_unified_cache()
        
        if self.epoch_metrics_computed[epoch]:
            return  # æ—¢ã«è¨ˆç®—æ¸ˆã¿
        
        # è¨“ç·´ç²¾åº¦è¨ˆç®—ï¼ˆåŠ¹ç‡çš„æ‰‹æ³•ä½¿ç”¨ï¼‰
        train_accuracy = self.get_epoch_accuracy_efficient(epoch, 'train')
        
        # ãƒ†ã‚¹ãƒˆç²¾åº¦è¨ˆç®—ï¼ˆåŠ¹ç‡çš„æ‰‹æ³•ä½¿ç”¨ï¼‰
        test_accuracy = self.get_epoch_accuracy_efficient(epoch, 'test')
        
        # è¨“ç·´èª¤å·®è¨ˆç®—ï¼ˆ3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„æ‰‹æ³•ä½¿ç”¨ï¼‰
        train_error = self.get_epoch_error_efficient(epoch, 'train')
        
        # ãƒ†ã‚¹ãƒˆèª¤å·®è¨ˆç®—ï¼ˆ3æ¬¡å…ƒé…åˆ—ãƒ™ãƒ¼ã‚¹åŠ¹ç‡çš„æ‰‹æ³•ä½¿ç”¨ï¼‰
        test_error = self.get_epoch_error_efficient(epoch, 'test')
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        self.cached_epoch_metrics[epoch] = [train_accuracy, test_accuracy, train_error, test_error]
        self.epoch_metrics_computed[epoch] = True
    
    def get_unified_epoch_metrics(self, epoch):
        """
        çµ±ä¸€çš„ã‚¨ãƒãƒƒã‚¯ç²¾åº¦ãƒ»èª¤å·®å–å¾—
        æˆ»ã‚Šå€¤: (è¨“ç·´ç²¾åº¦, ãƒ†ã‚¹ãƒˆç²¾åº¦, è¨“ç·´èª¤å·®, ãƒ†ã‚¹ãƒˆèª¤å·®)
        """
        # çµ±ä¸€ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚·ã‚¹ãƒ†ãƒ åˆæœŸåŒ–ï¼ˆå¿…è¦ã«å¿œã˜ã¦ï¼‰
        self.__init_unified_cache()
        
        if not self.epoch_metrics_computed[epoch]:
            self.compute_and_cache_epoch_metrics(epoch)
        
        return tuple(self.cached_epoch_metrics[epoch])
    
    def get_unified_progress_display_data(self, epoch):
        """
        é€²æ—ãƒãƒ¼è¡¨ç¤ºç”¨çµ±ä¸€ãƒ‡ãƒ¼ã‚¿å–å¾—
        é€²æ—ãƒãƒ¼ã®ä¸æ•´åˆã‚’æ’é™¤ã—ã€é…åˆ—ãƒ‡ãƒ¼ã‚¿ã‚’çµ±ä¸€åˆ©ç”¨
        """
        train_acc, test_acc, train_err, test_err = self.get_unified_epoch_metrics(epoch)
        
        return {
            'train_accuracy': train_acc * 100,  # ãƒ‘ãƒ¼ã‚»ãƒ³ãƒˆè¡¨ç¤º
            'test_accuracy': test_acc * 100,
            'train_error': train_err,
            'test_error': test_err,
            'è¨“ç·´ç²¾åº¦': f'{train_acc*100:.1f}%',  # æ—¥æœ¬èªè¡¨ç¤ºç”¨
            'ãƒ†ã‚¹ãƒˆç²¾åº¦': f'{test_acc*100:.1f}%',
            'è¨“ç·´èª¤å·®': f'{train_err:.4f}',
            'ãƒ†ã‚¹ãƒˆèª¤å·®': f'{test_err:.4f}'
        }
    
    def _display_confusion_matrix_console(self, dataset_type: str, epoch: int = -1):
        """
        æ··åŒè¡Œåˆ—ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º (ed_multi.prompt.mdæº–æ‹  - å‹•çš„åˆ—å¹…èª¿æ•´å¯¾å¿œ)
        
        Args:
            dataset_type: 'train' ã¾ãŸã¯ 'test'
            epoch: è¡¨ç¤ºã™ã‚‹ã‚¨ãƒãƒƒã‚¯ï¼ˆ-1ãªã‚‰å…¨ã‚¨ãƒãƒƒã‚¯çµ±åˆï¼‰
        """
        confusion_matrix = self.calculate_confusion_matrix(dataset_type, epoch)
        
        # æ··åŒè¡Œåˆ—å†…ã®æœ€å¤§å€¤ã‚’å–å¾—ã—ã¦æ¡æ•°ã‚’è¨ˆç®—
        max_value = max(max(row) for row in confusion_matrix)
        max_digits = len(str(max_value))
        
        # åˆ—å¹…æ±ºå®š: 3æ¡ä»¥ä¸‹ãªã‚‰4æ–‡å­—ã€4æ¡ä»¥ä¸Šãªã‚‰(æœ€å¤§æ¡æ•°+1)æ–‡å­—
        if max_digits <= 3:
            col_width = 4
        else:
            col_width = max_digits + 1
        
        print(f"\nğŸ“Š æ··åŒè¡Œåˆ— ({dataset_type} data, epoch={epoch if epoch != -1 else 'all'}):")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œè¡¨ç¤ºï¼ˆåˆ—ç•ªå·ï¼‰
        print(" " * (col_width - 1), end="")  # è¡Œãƒ©ãƒ™ãƒ«åˆ†ã®ã‚¹ãƒšãƒ¼ã‚¹èª¿æ•´
        for j in range(self.num_classes):
            print(f"{j:>{col_width}}", end="")
        print()
        
        # å„è¡Œã®è¡¨ç¤º
        for i in range(self.num_classes):
            print(f"{i:>{col_width-2}}: ", end="")  # è¡Œãƒ©ãƒ™ãƒ«
            for j in range(self.num_classes):
                print(f"{confusion_matrix[i][j]:>{col_width}}", end="")
            print()
    
    def display_confusion_matrix(self, dataset_type: str, epoch: int = -1, save_dir=None):
        """
        æ··åŒè¡Œåˆ—ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤ºï¼ˆnetwork_mnist.pyã¨ã®äº’æ›æ€§ï¼‰
        
        Args:
            dataset_type: 'train' ã¾ãŸã¯ 'test'
            epoch: è¡¨ç¤ºã™ã‚‹ã‚¨ãƒãƒƒã‚¯ï¼ˆ-1ãªã‚‰å…¨ã‚¨ãƒãƒƒã‚¯çµ±åˆï¼‰
            save_dir: ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆç¾åœ¨æœªä½¿ç”¨ï¼‰
        """
        self._display_confusion_matrix_console(dataset_type, epoch)
        print()
    
    def display_confusion_matrix_single_epoch(self, dataset_type: str, epoch: int):
        """
        å˜ä¸€ã‚¨ãƒãƒƒã‚¯ã®æ··åŒè¡Œåˆ—ã‚’ã‚³ãƒ³ã‚½ãƒ¼ãƒ«è¡¨ç¤º (ed_multi.prompt.mdæº–æ‹ )
        
        Args:
            dataset_type: 'train' ã¾ãŸã¯ 'test'
            epoch: è¡¨ç¤ºã™ã‚‹ã‚¨ãƒãƒƒã‚¯ï¼ˆ0ãƒ™ãƒ¼ã‚¹ï¼‰
        """
        confusion_matrix = self.calculate_confusion_matrix_single_epoch(dataset_type, epoch)
        
        # æ··åŒè¡Œåˆ—å†…ã®æœ€å¤§å€¤ã‚’å–å¾—ã—ã¦æ¡æ•°ã‚’è¨ˆç®—
        max_value = max(max(row) for row in confusion_matrix)
        max_digits = len(str(max_value))
        
        # åˆ—å¹…æ±ºå®š: 3æ¡ä»¥ä¸‹ãªã‚‰4æ–‡å­—ã€4æ¡ä»¥ä¸Šãªã‚‰(æœ€å¤§æ¡æ•°+1)æ–‡å­—
        if max_digits <= 3:
            col_width = 4
        else:
            col_width = max_digits + 1
        
        print(f"\nğŸ“Š æ··åŒè¡Œåˆ— ({dataset_type} data, epoch={epoch}):")
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œè¡¨ç¤ºï¼ˆåˆ—ç•ªå·ï¼‰
        print(" " * (col_width - 1), end="")  # è¡Œãƒ©ãƒ™ãƒ«åˆ†ã®ã‚¹ãƒšãƒ¼ã‚¹èª¿æ•´
        for j in range(self.num_classes):
            print(f"{j:>{col_width}}", end="")
        print()
        
        # å„è¡Œã®è¡¨ç¤º
        for i in range(self.num_classes):
            print(f"{i:>{col_width-2}}: ", end="")  # è¡Œãƒ©ãƒ™ãƒ«
            for j in range(self.num_classes):
                print(f"{confusion_matrix[i][j]:>{col_width}}", end="")
            print()
        print()
