#!/usr/bin/env python3
"""
ED-ANN v5.6.4 - ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±è¡¨ç¤ºç‰ˆ

ğŸ¯ ED-ANN (Error Diffusion Artificial Neural Network):
ç´”ç²‹ãªEDæ³•ã«ã‚ˆã‚‹å¤šã‚¯ãƒ©ã‚¹åˆ†é¡ã‚·ã‚¹ãƒ†ãƒ  + è©³ç´°æƒ…å ±è¡¨ç¤ºå¼·åŒ–
Pure Multi-class classification using ED method with detailed model info

âœ… ä¸»è¦æ©Ÿèƒ½:
- ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’: ãƒã‚¤ãƒŠãƒªåˆ†é¡ + ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’ (88.30%ç²¾åº¦)
- ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’: æ¨™æº–ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ + ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’ (89.40%ç²¾åº¦ã€å…¨ã‚¯ãƒ©ã‚¹æˆåŠŸ)
- PyTorchæ¨™æº–Dataset/DataLoaderå®Œå…¨çµ±åˆ

ğŸ”¬ EDæ³•ç†è«–å®Ÿè£…:
1. Error Diffusion (èª¤å·®æ‹¡æ•£): ã‚¢ãƒŸãƒ³æ¿ƒåº¦ã«ã‚ˆã‚‹å­¦ç¿’åˆ¶å¾¡
2. Multi-class ED learning: å„ã‚¯ãƒ©ã‚¹ã«å¯¾ã™ã‚‹å€‹åˆ¥èª¤å·®æ‹¡æ•£
3. Adaptive learning: ã‚¢ãƒŸãƒ³æ¿ƒåº¦ã«åŸºã¥ãé©å¿œçš„å­¦ç¿’ç‡èª¿æ•´
4. Pure ANN implementation: SNNã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆå®Œå…¨å‰Šé™¤

âœ… v5.6.4æ–°æ©Ÿèƒ½:
1. ãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º: TensorFlow model.summary()ãƒ©ã‚¤ã‚¯ãªè©³ç´°ãƒ¢ãƒ‡ãƒ«æ§‹é€ è¡¨ç¤º
2. ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¡¨ç¤º: HyperParametersã‚¯ãƒ©ã‚¹å…¨é …ç›®ã®è‡ªå‹•è¡¨ç¤º
3. å®Ÿè¡Œé–‹å§‹æ™‚è¡¨ç¤º: å­¦ç¿’å‰ã«ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ç¢ºèªå¯èƒ½
4. å…±é€šæƒ…å ±è¡¨ç¤º: ã‚¯ãƒ©ã‚¹å˜ä½ãƒ»ã‚¨ãƒãƒƒã‚¯å˜ä½ä¸¡ãƒ¢ãƒ¼ãƒ‰ã§å…±é€šè¡¨ç¤º

ğŸ“Š æ€§èƒ½å®Ÿç¸¾ï¼ˆå¤‰æ›´ãªã—ï¼‰:
ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’ (ãƒã‚¤ãƒŠãƒª+ã‚¯ãƒ©ã‚¹å˜ä½): å…¨ä½“88.30%, ç²¾åº¦ç¯„å›²28.50%, æˆåŠŸ9/10
ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’ (ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹+ã‚¨ãƒãƒƒã‚¯): å…¨ä½“89.40%, ç²¾åº¦ç¯„å›²16.56%, æˆåŠŸ10/10

ğŸš€ å®Ÿè¡Œæ–¹æ³•:
--mode epoch: ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’
--mode class: ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’  
--mode both: ä¸¡æ‰‹æ³•æ¯”è¼ƒ
--realtime: ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ï¼ˆã‚¨ãƒãƒƒã‚¯å˜ä½ãƒ»ã‚¯ãƒ©ã‚¹å˜ä½å¯¾å¿œï¼‰

ğŸŒ è‹±èªè¡¨è¨˜: "Multi-class classification using ED method"
ğŸ¯ ã‚·ã‚¹ãƒ†ãƒ : Pure ED-ANN + Detailed Model Information Display
å®Œæˆæ—¥: 2025-08-17
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from torchvision import datasets, transforms
from torch.utils.data import DataLoader, Dataset, Subset
import argparse
import json
import time
import threading
import sys
import csv
from datetime import datetime
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict
import warnings
warnings.filterwarnings('ignore')

# ã‚°ãƒ­ãƒ¼ãƒãƒ«verboseãƒ•ãƒ©ã‚°
VERBOSE_MODE = False

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆED-ANNä»•æ§˜æº–æ‹ ï¼‰
import matplotlib.pyplot as plt
import matplotlib
import platform

def setup_japanese_font():
    """æ¨™æº–ã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®š"""
    try:
        # ç¢ºå®Ÿã«å‹•ä½œã™ã‚‹æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã®ã¿ï¼ˆè­¦å‘Šãªã—ï¼‰
        japanese_fonts = [
            'Noto Sans CJK JP',     # ç¢ºèªæ¸ˆã¿å‹•ä½œOK: /usr/share/fonts/opentype/noto/
            'Yu Gothic',            # ç¢ºèªæ¸ˆã¿å‹•ä½œOK: /home/yoichi/.fonts/YuGothR.ttc
            'Noto Sans JP',         # Google Noto æ—¥æœ¬èªç‰¹åŒ–ç‰ˆ
            'sans-serif'            # ã‚·ã‚¹ãƒ†ãƒ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        ]
        
        # è­¦å‘Šãªã—ãƒ•ã‚©ãƒ³ãƒˆè¨­å®š
        plt.rcParams['font.family'] = japanese_fonts
        plt.rcParams['axes.unicode_minus'] = False  # ãƒã‚¤ãƒŠã‚¹è¨˜å·æ–‡å­—åŒ–ã‘å¯¾ç­–
        
        # è¨­å®šæˆåŠŸã®ç¢ºèª
        current_font = plt.rcParams['font.family'][0]
        # å‰Šé™¤: print(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šæˆåŠŸ: {current_font}")
        return current_font
        
    except Exception as e:
        print(f"ãƒ•ã‚©ãƒ³ãƒˆè¨­å®šã§ã‚¨ãƒ©ãƒ¼: {e}")  # loggerã®ä»£ã‚ã‚Šã«printä½¿ç”¨
        plt.rcParams['font.family'] = ['sans-serif']
        return 'default'

def setup_logging(verbose=False):
    """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
    handlers = [logging.StreamHandler()]
    
    if verbose:
        # verboseãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        log_filename = f'ed_ann_v563_restored_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'
        handlers.append(logging.FileHandler(log_filename))
        print(f"ğŸ“ è©³ç´°ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«: {log_filename}")
    
    logging.basicConfig(
        level=logging.INFO if verbose else logging.WARNING,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=handlers,
        force=True  # æ—¢å­˜ã®è¨­å®šã‚’ä¸Šæ›¸ã
    )
    return logging.getLogger(__name__)

# åˆæœŸãƒ­ã‚°è¨­å®šï¼ˆä¸€æ™‚çš„ã€mainé–¢æ•°ã§å†è¨­å®šã•ã‚Œã‚‹ï¼‰
logger = setup_logging(verbose=False)

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆåˆæœŸåŒ–ï¼ˆãƒ­ã‚°è¨­å®šå¾Œï¼‰
setup_japanese_font()

@dataclass
@dataclass
class TrainingConfig:
    """è¨“ç·´è¨­å®šï¼ˆv5.5.4æº–æ‹ ï¼‰"""
    epochs: int = 3
    learning_rate: float = 0.01
    batch_size: int = 32
    train_samples_per_class: int = 1000
    test_samples_per_class: int = 1000  # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã¨åŒæ•°ã«å¤‰æ›´
    hidden_size: int = 64
    num_classes: int = 10
    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'
    force_cpu: bool = False  # CPUå¼·åˆ¶ä½¿ç”¨ãƒ•ãƒ©ã‚°
    realtime: bool = False
    random_seed: int = 42
    verbose: bool = False
    verify: bool = False  # ç²¾åº¦æ¤œè¨¼æ©Ÿèƒ½ãƒ•ãƒ©ã‚°

@dataclass
class HyperParameters:
    """EDæ³•é–¢é€£ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿é›†ç´„ã‚¯ãƒ©ã‚¹
    
    EDæ³•ç†è«–ã«åŸºã¥ãå­¦ç¿’åˆ¶å¾¡ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä¸€å…ƒç®¡ç†
    é‡‘å­å‹‡æ°è€ƒæ¡ˆã®Error Diffusionæ³•å®Ÿè£…ç”¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    """
    
    # EDæ³•ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
    d_plus: float = 0.1      # ã‚¢ãƒŸãƒ³æ¿ƒåº¦å¢—åŠ é‡ï¼ˆæ­£ç­”æ™‚ã®é‡ã¿å¢—åŠ åˆ¶å¾¡ï¼‰
    d_minus: float = 0.05    # ã‚¢ãƒŸãƒ³æ¿ƒåº¦æ¸›å°‘é‡ï¼ˆèª¤ç­”æ™‚ã®é‡ã¿æ¸›å°‘åˆ¶å¾¡ï¼‰
    
    # å­¦ç¿’ç‡é–¢é€£
    base_learning_rate: float = 0.01    # åŸºæœ¬å­¦ç¿’ç‡ï¼ˆAdam optimizerç”¨ï¼‰
    ed_learning_rate: float = 0.001     # EDæ³•å°‚ç”¨å­¦ç¿’ç‡ï¼ˆé‡ã¿æ›´æ–°åˆ¶å¾¡ï¼‰
    
    # é‡ã¿æ›´æ–°åˆ¶å¾¡
    weight_decay: float = 1e-5          # é‡ã¿æ¸›è¡°ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰
    momentum: float = 0.9               # ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ï¼ˆå­¦ç¿’å®‰å®šæ€§å‘ä¸Šï¼‰
    
    # EDæ³•ç‰¹æœ‰åˆ¶ç´„
    preserve_sign: bool = True          # é‡ã¿ç¬¦å·ä¿æŒï¼ˆEDæ³•æ ¸å¿ƒåˆ¶ç´„ï¼‰
    abs_increase_only: bool = True      # çµ¶å¯¾å€¤å¢—åŠ ã®ã¿è¨±å¯
    
    # ã‚¢ãƒŸãƒ³æ¿ƒåº¦åˆ¶å¾¡
    amine_threshold: float = 0.5        # ã‚¢ãƒŸãƒ³æ¿ƒåº¦é–¾å€¤ï¼ˆå­¦ç¿’åˆ¤å®šåŸºæº–ï¼‰
    concentration_decay: float = 0.99   # æ¿ƒåº¦æ¸›è¡°ç‡ï¼ˆæ™‚é–“çµŒéã«ã‚ˆã‚‹æ¸›è¡°ï¼‰
    
    # ã‚¯ãƒ©ã‚¹åˆ¥å­¦ç¿’åˆ¶å¾¡
    class_learning_balance: bool = True  # ã‚¯ãƒ©ã‚¹é–“å­¦ç¿’ãƒãƒ©ãƒ³ã‚¹èª¿æ•´
    adaptive_rate: bool = False         # é©å¿œçš„å­¦ç¿’ç‡ï¼ˆå®Ÿé¨“çš„æ©Ÿèƒ½ï¼‰

class MultiClassSingleOutputEDDense(nn.Module):
    """
    v5.5.4æº–æ‹  - ã‚¯ãƒ©ã‚¹åˆ¥é‡ã¿é…åˆ—ç®¡ç†ã‚·ã‚¹ãƒ†ãƒ 
    å„å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ãŒã‚¯ãƒ©ã‚¹åˆ¥ã®é‡ã¿é…åˆ—ã‚’æŒã¤EDæ³•ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
    """
    
    def __init__(self, in_features: int, out_features: int, num_output_classes: int = 10):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.num_output_classes = num_output_classes
        
        # v5.5.4æº–æ‹ : ã‚¯ãƒ©ã‚¹åˆ¥é‡ã¿é…åˆ— [out_features, num_output_classes, in_features]
        self.weights_per_class = nn.Parameter(
            torch.randn(out_features, num_output_classes, in_features) * 0.1
        )
        self.bias = nn.Parameter(torch.zeros(out_features))
        
        print(f"MultiClassEDDenseåˆæœŸåŒ–: {in_features}â†’{out_features}, ã‚¯ãƒ©ã‚¹æ•°{num_output_classes}") if VERBOSE_MODE else None
        
    def forward(self, x: torch.Tensor, target_class: int) -> torch.Tensor:
        """æŒ‡å®šã‚¯ãƒ©ã‚¹ã®é‡ã¿ã‚’ä½¿ç”¨ã—ãŸå‰å‘ãè¨ˆç®—"""
        # target_classã®é‡ã¿ã‚’é¸æŠ
        selected_weights = self.weights_per_class[:, target_class, :]  # [out_features, in_features]
        
        # ç·šå½¢å¤‰æ›
        output = torch.matmul(x, selected_weights.t()) + self.bias
        return output
    
    def get_weight_stats(self, target_class: int) -> Dict:
        """é‡ã¿çµ±è¨ˆå–å¾—"""
        with torch.no_grad():
            weights = self.weights_per_class[:, target_class, :]
            return {
                'norm': torch.norm(weights).item(),
                'mean': torch.mean(weights).item(),
                'std': torch.std(weights).item(),
                'min': torch.min(weights).item(),
                'max': torch.max(weights).item()
            }

class SingleOutputClassifier(nn.Module):
    """
    v5.5.4æº–æ‹  - å˜ä¸€ã‚¯ãƒ©ã‚¹å°‚ç”¨åˆ†é¡å™¨
    MultiClassSingleOutputEDDenseã‚’ä½¿ç”¨ã—ãŸäºŒå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
    """
    
    def __init__(self, input_size: int = 784, hidden_size: int = 64, target_class: int = 0):
        super().__init__()
        self.target_class = target_class
        self.input_size = input_size
        self.hidden_size = hidden_size
        
        # v5.5.4æº–æ‹ : EDå¯†çµåˆå±¤ä½¿ç”¨
        self.hidden_layer = MultiClassSingleOutputEDDense(
            input_size, hidden_size, num_output_classes=1
        )
        
        self.output_layer = MultiClassSingleOutputEDDense(
            hidden_size, 1, num_output_classes=1
        )
        
        print(f"SingleOutputClassifieråˆæœŸåŒ–å®Œäº†: ã‚¯ãƒ©ã‚¹{target_class}, éš ã‚Œå±¤{hidden_size}") if VERBOSE_MODE else None
        
    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """å‰å‘ãè¨ˆç®—"""
        # ãƒãƒƒãƒæ¬¡å…ƒã‚’ç¶­æŒã—ãŸã¾ã¾å¹³å¦åŒ–
        if len(x.shape) > 2:
            x = x.view(x.size(0), -1)
            
        # éš ã‚Œå±¤ï¼ˆã‚¯ãƒ©ã‚¹0ã‚’ä½¿ç”¨ã€å®Ÿè³ªçš„ã«å¾“æ¥ã®å˜ä¸€å‡ºåŠ›ï¼‰
        hidden = self.hidden_layer(x, target_class=0)
        hidden = torch.sigmoid(hidden)
        
        # å‡ºåŠ›å±¤
        output = self.output_layer(hidden, target_class=0)
        output = torch.sigmoid(output)
        
        return output.squeeze(-1)  # [batch_size]
    
    def get_classifier_stats(self) -> Dict:
        """åˆ†é¡å™¨çµ±è¨ˆå–å¾—"""
        return {
            'hidden_layer': self.hidden_layer.get_weight_stats(0),
            'output_layer': self.output_layer.get_weight_stats(0)
        }

class StandardMNISTDataset:
    """
    Phase 1: æ¨™æº–Dataset/DataLoaderçµ±åˆ
    - PyTorchæ¨™æº–MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨
    - ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’ã‚µãƒãƒ¼ãƒˆ
    - ãƒã‚¤ãƒŠãƒªåˆ†é¡ãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ
    """
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.1307,), (0.3081,))
        ])
        
        # æ¨™æº–MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆèª­ã¿è¾¼ã¿
        self.train_dataset = datasets.MNIST(
            'data', train=True, download=True, transform=self.transform
        )
        self.test_dataset = datasets.MNIST(
            'data', train=False, transform=self.transform
        )
        
        print(f"ğŸ“Š æ¨™æº–MNISTè¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(self.train_dataset)} ã‚µãƒ³ãƒ—ãƒ«") if self.config.verbose else None
        print(f"ğŸ“Š æ¨™æº–MNISTãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(self.test_dataset)} ã‚µãƒ³ãƒ—ãƒ«") if self.config.verbose else None
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æº–å‚™ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        self.class_indices = self._prepare_class_indices()
        
    def _prepare_class_indices(self) -> Dict:
        """é«˜åŠ¹ç‡ã‚¯ãƒ©ã‚¹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æº–å‚™"""
        train_indices = defaultdict(list)
        test_indices = defaultdict(list)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ - ç›´æ¥ãƒ©ãƒ™ãƒ«å‚ç…§
        train_targets = self.train_dataset.targets.numpy()
        for i, label in enumerate(train_targets):
            train_indices[int(label)].append(i)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ - ç›´æ¥ãƒ©ãƒ™ãƒ«å‚ç…§
        test_targets = self.test_dataset.targets.numpy()
        for i, label in enumerate(test_targets):
            test_indices[int(label)].append(i)
        
        print("ğŸ” ã‚¯ãƒ©ã‚¹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æº–å‚™å®Œäº†ï¼ˆé«˜åŠ¹ç‡ç‰ˆï¼‰") if self.config.verbose else None
        if self.config.verbose:
            for class_idx in range(10):
                print(f"   ã‚¯ãƒ©ã‚¹ {class_idx}: è¨“ç·´{len(train_indices[class_idx])}, ãƒ†ã‚¹ãƒˆ{len(test_indices[class_idx])}")
        
        return {
            'train': train_indices,
            'test': test_indices
        }
    
    def get_binary_datasets(self, target_class: int) -> Tuple[DataLoader, DataLoader]:
        """
        Phase 1æ¨™æº–åŒ–: ãƒã‚¤ãƒŠãƒªåˆ†é¡DataLoaderç”Ÿæˆ
        target_class vs others ã®é«˜åŠ¹ç‡äºŒé …åˆ†é¡ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        """
        print(f"ğŸ¯ ãƒã‚¤ãƒŠãƒªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ: ã‚¯ãƒ©ã‚¹{target_class} vs others") if self.config.verbose else None
        
        # æ¨™æº–Datasetå½¢å¼ã§ã®é«˜åŠ¹ç‡ãƒ‡ãƒ¼ã‚¿ä½œæˆ
        class StandardBinaryDataset(Dataset):
            """æ¨™æº–PyTorch Datasetå®Ÿè£…"""
            def __init__(self, base_dataset, indices, labels):
                self.base_dataset = base_dataset
                self.indices = indices
                self.labels = labels
                
            def __len__(self):
                return len(self.indices)
                
            def __getitem__(self, idx):
                actual_idx = self.indices[idx]
                data, _ = self.base_dataset[actual_idx]  # å…ƒã®ãƒ©ãƒ™ãƒ«ã¯ç„¡è¦–
                binary_label = self.labels[idx]
                return data, binary_label
        
        # åŠ¹ç‡çš„ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ»ãƒ©ãƒ™ãƒ«æº–å‚™
        train_indices = []
        train_labels = []
        test_indices = []
        test_labels = []
        
        # ãƒã‚¸ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ï¼ˆtarget_class = 1.0ï¼‰
        target_train_indices = self.class_indices['train'][target_class][:self.config.train_samples_per_class]
        target_test_indices = self.class_indices['test'][target_class][:self.config.test_samples_per_class]
        
        train_indices.extend(target_train_indices)
        train_labels.extend([1.0] * len(target_train_indices))
        
        test_indices.extend(target_test_indices)
        test_labels.extend([1.0] * len(target_test_indices))
        
        # ãƒã‚¬ãƒ†ã‚£ãƒ–ã‚µãƒ³ãƒ—ãƒ«ï¼ˆothers = 0.0ï¼‰v5.5.4æº–æ‹ åˆ†æ•£
        samples_per_neg_class = self.config.train_samples_per_class // (self.config.num_classes - 1)
        test_samples_per_neg_class = self.config.test_samples_per_class // (self.config.num_classes - 1)
        
        for class_idx in range(self.config.num_classes):
            if class_idx != target_class:
                # è¨“ç·´ãƒã‚¬ãƒ†ã‚£ãƒ–
                neg_train_indices = self.class_indices['train'][class_idx][:samples_per_neg_class]
                train_indices.extend(neg_train_indices)
                train_labels.extend([0.0] * len(neg_train_indices))
                
                # ãƒ†ã‚¹ãƒˆãƒã‚¬ãƒ†ã‚£ãƒ–
                neg_test_indices = self.class_indices['test'][class_idx][:test_samples_per_neg_class]
                test_indices.extend(neg_test_indices)
                test_labels.extend([0.0] * len(neg_test_indices))
        
        # ã‚·ãƒ£ãƒƒãƒ•ãƒ«ï¼ˆå†ç¾æ€§ç¶­æŒï¼‰
        np.random.seed(self.config.random_seed + target_class)
        
        # è¨“ç·´ãƒ‡ãƒ¼ã‚¿ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        combined_train = list(zip(train_indices, train_labels))
        np.random.shuffle(combined_train)
        train_indices, train_labels = zip(*combined_train)
        
        # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        combined_test = list(zip(test_indices, test_labels))
        np.random.shuffle(combined_test)
        test_indices, test_labels = zip(*combined_test)
        
        print(f"   æ­£ä¾‹: è¨“ç·´{sum(train_labels):.0f}, ãƒ†ã‚¹ãƒˆ{sum(test_labels):.0f}") if self.config.verbose else None
        print(f"   è² ä¾‹: è¨“ç·´{len(train_labels) - sum(train_labels):.0f}, ãƒ†ã‚¹ãƒˆ{len(test_labels) - sum(test_labels):.0f}") if self.config.verbose else None
        
        # æ¨™æº–Datasetä½œæˆ
        train_dataset = StandardBinaryDataset(self.train_dataset, train_indices, train_labels)
        test_dataset = StandardBinaryDataset(self.test_dataset, test_indices, test_labels)
        
        # æ¨™æº–DataLoaderä½œæˆ
        train_loader = DataLoader(
            train_dataset, 
            batch_size=self.config.batch_size, 
            shuffle=True,
            num_workers=0,  # å®‰å®šæ€§ã®ãŸã‚
            pin_memory=False
        )
        test_loader = DataLoader(
            test_dataset, 
            batch_size=self.config.batch_size, 
            shuffle=False,
            num_workers=0,
            pin_memory=False
        )
        
        return train_loader, test_loader
    
    def create_epoch_based_datasets(self) -> Dict[str, DataLoader]:
        """
        Phase 2: ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’ç”¨ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        å…¨ã‚¯ãƒ©ã‚¹ã‚’åŒæ™‚ã«å­¦ç¿’ã™ã‚‹æ¨™æº–çš„ãªãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡æ–¹å¼
        """
        print("ğŸ”„ ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™ä¸­") if self.config.verbose else None
        
        # æ¨™æº–ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹Datasetå®Ÿè£…
        class EpochBasedDataset(Dataset):
            """ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’ç”¨æ¨™æº–Dataset"""
            def __init__(self, base_dataset, samples_per_class=1000):
                self.base_dataset = base_dataset
                self.samples_per_class = samples_per_class
                
                # ã‚¯ãƒ©ã‚¹åˆ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æº–å‚™
                self.class_indices = defaultdict(list)
                targets = base_dataset.targets.numpy()
                for i, label in enumerate(targets):
                    self.class_indices[int(label)].append(i)
                
                # ãƒãƒ©ãƒ³ã‚¹å–å¾—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
                self.selected_indices = []
                self.selected_labels = []
                
                for class_idx in range(10):
                    indices = self.class_indices[class_idx][:samples_per_class]
                    self.selected_indices.extend(indices)
                    self.selected_labels.extend([class_idx] * len(indices))
            
            def __len__(self):
                return len(self.selected_indices)
            
            def __getitem__(self, idx):
                actual_idx = self.selected_indices[idx]
                data, _ = self.base_dataset[actual_idx]
                label = self.selected_labels[idx]
                return data, label
        
        # ã‚¨ãƒãƒƒã‚¯å˜ä½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
        train_dataset = EpochBasedDataset(
            self.train_dataset, 
            self.config.train_samples_per_class
        )
        test_dataset = EpochBasedDataset(
            self.test_dataset, 
            self.config.test_samples_per_class
        )
        
        # DataLoaderä½œæˆ
        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            num_workers=0,
            pin_memory=False
        )
        
        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            num_workers=0,
            pin_memory=False
        )
        
        print(f"   ã‚¨ãƒãƒƒã‚¯å˜ä½ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆæº–å‚™å®Œäº†") if self.config.verbose else None
        print(f"   è¨“ç·´ãƒ‡ãƒ¼ã‚¿: {len(train_dataset)} ã‚µãƒ³ãƒ—ãƒ«") if self.config.verbose else None
        print(f"   ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿: {len(test_dataset)} ã‚µãƒ³ãƒ—ãƒ«") if self.config.verbose else None
        
        return {
            'train': train_loader,
            'test': test_loader
        }

class EpochBasedTrainer:
    """
    Phase 2: ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
    å…¨ã‚¯ãƒ©ã‚¹åŒæ™‚å­¦ç¿’ã«ã‚ˆã‚‹æ¨™æº–ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡å®Ÿè£…
    """
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        
        # ãƒ‡ãƒã‚¤ã‚¹æ±ºå®šï¼ˆCPUå¼·åˆ¶ã‚ªãƒ—ã‚·ãƒ§ãƒ³è€ƒæ…®ï¼‰
        if config.force_cpu:
            self.device = torch.device('cpu')
        else:
            self.device = torch.device(config.device)
        
        # æ±ºå®šè«–çš„ã‚·ãƒ¼ãƒ‰è¨­å®š
        torch.manual_seed(config.random_seed)
        np.random.seed(config.random_seed)
        if torch.cuda.is_available() and not config.force_cpu:
            torch.cuda.manual_seed(config.random_seed)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†
        self.dataset_manager = StandardMNISTDataset(config)
        
        # ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.model = self._create_multiclass_model()
        self.model.to(self.device)
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ãƒ»æå¤±é–¢æ•°
        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=config.learning_rate)
        self.criterion = nn.CrossEntropyLoss()
        
        # å­¦ç¿’çµ±è¨ˆ
        self.training_stats = {
            'train_accuracy': [],
            'test_accuracy': [],
            'train_loss': [],
            'test_loss': []
        }
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–æ©Ÿèƒ½ï¼ˆPhase 1ã¨åŒæ§˜ï¼‰
        self.visualizer = None
        self.multiclass_fig = None
        self.acc_ax = None
        self.loss_ax = None
        if config.realtime:
            self.visualizer = "multiclass"
        
        # ãƒ‡ãƒã‚¤ã‚¹æƒ…å ±è¡¨ç¤ºï¼ˆCPUå¼·åˆ¶ã‚ªãƒ—ã‚·ãƒ§ãƒ³è€ƒæ…®ï¼‰
        device_info = f"ãƒ‡ãƒã‚¤ã‚¹: {self.device}"
        if config.force_cpu:
            device_info += " (CPUå¼·åˆ¶ä½¿ç”¨)"
        elif torch.cuda.is_available():
            device_info += " (è‡ªå‹•é¸æŠ)"
        print(f"ğŸ”„ ã‚¨ãƒãƒƒã‚¯å˜ä½ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–å®Œäº†: {device_info}") if self.config.verbose else None
        
    def _create_multiclass_model(self):
        """ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹åˆ†é¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆ"""
        class MultiClassNetwork(nn.Module):
            def __init__(self, input_size=784, hidden_size=64, num_classes=10):
                super().__init__()
                self.hidden = nn.Linear(input_size, hidden_size)
                self.output = nn.Linear(hidden_size, num_classes)
                
                # v5.5.4æº–æ‹ é‡ã¿åˆæœŸåŒ–
                nn.init.normal_(self.hidden.weight, mean=0, std=0.1)
                nn.init.normal_(self.output.weight, mean=0, std=0.1)
                nn.init.zeros_(self.hidden.bias)
                nn.init.zeros_(self.output.bias)
                
            def forward(self, x):
                if len(x.shape) > 2:
                    x = x.view(x.size(0), -1)
                x = torch.sigmoid(self.hidden(x))
                x = self.output(x)
                return x
        
        return MultiClassNetwork(
            input_size=784,
            hidden_size=self.config.hidden_size,
            num_classes=self.config.num_classes
        )
    
    def train_epoch_based(self):
        """ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’å®Ÿè¡Œ"""
        print("ğŸš€ ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’é–‹å§‹") if self.config.verbose else None
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—
        if self.config.realtime and self.visualizer:
            self.setup_multiclass_visualization(self.config.epochs)
            print("=== Phase 2 ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦è¡¨ç¤ºå®Œäº† ===") if self.config.verbose else None
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼æº–å‚™
        dataloaders = self.dataset_manager.create_epoch_based_datasets()
        train_loader = dataloaders['train']
        test_loader = dataloaders['test']
        
        start_time = time.time()
        
        # ã‚¨ãƒãƒƒã‚¯ãƒ«ãƒ¼ãƒ—
        for epoch in range(self.config.epochs):
            epoch_start = time.time()
            
            # è¨“ç·´ãƒ•ã‚§ãƒ¼ã‚º
            self.model.train()
            train_correct = 0
            train_total = 0
            train_loss_sum = 0
            
            for batch_data, batch_labels in train_loader:
                batch_data = batch_data.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                self.optimizer.zero_grad()
                outputs = self.model(batch_data)
                loss = self.criterion(outputs, batch_labels)
                loss.backward()
                self.optimizer.step()
                
                # çµ±è¨ˆæ›´æ–°
                train_loss_sum += loss.item()
                _, predicted = torch.max(outputs.data, 1)
                train_total += batch_labels.size(0)
                train_correct += (predicted == batch_labels).sum().item()
            
            train_accuracy = train_correct / train_total
            train_loss = train_loss_sum / len(train_loader)
            
            # ãƒ†ã‚¹ãƒˆãƒ•ã‚§ãƒ¼ã‚º
            self.model.eval()
            test_correct = 0
            test_total = 0
            test_loss_sum = 0
            
            with torch.no_grad():
                for batch_data, batch_labels in test_loader:
                    batch_data = batch_data.to(self.device)
                    batch_labels = batch_labels.to(self.device)
                    
                    outputs = self.model(batch_data)
                    loss = self.criterion(outputs, batch_labels)
                    
                    test_loss_sum += loss.item()
                    _, predicted = torch.max(outputs.data, 1)
                    test_total += batch_labels.size(0)
                    test_correct += (predicted == batch_labels).sum().item()
            
            test_accuracy = test_correct / test_total
            test_loss = test_loss_sum / len(test_loader)
            
            # çµ±è¨ˆè¨˜éŒ²
            self.training_stats['train_accuracy'].append(train_accuracy)
            self.training_stats['test_accuracy'].append(test_accuracy)
            self.training_stats['train_loss'].append(train_loss)
            self.training_stats['test_loss'].append(test_loss)
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–æ›´æ–°
            if self.config.realtime and self.visualizer:
                self.update_visualization(epoch)
            
            epoch_time = time.time() - epoch_start
            print(f"ã‚¨ãƒãƒƒã‚¯ {epoch}: è¨“ç·´ç²¾åº¦ {train_accuracy:.4f}, ãƒ†ã‚¹ãƒˆç²¾åº¦ {test_accuracy:.4f}, æ™‚é–“ {epoch_time:.2f}ç§’") if self.config.verbose else None
        
        total_time = time.time() - start_time
        print(f"ğŸ¯ Phase 2å®Œäº†: ç·å­¦ç¿’æ™‚é–“ {total_time:.2f}ç§’") if self.config.verbose else None
        
        return self._evaluate_final_performance(test_loader, save_predictions=self.config.verify)
    
    def _evaluate_final_performance(self, test_loader, save_predictions: bool = False):
        """æœ€çµ‚æ€§èƒ½è©•ä¾¡ï¼ˆäºˆæ¸¬çµæœè¨˜éŒ²æ©Ÿèƒ½ä»˜ãï¼‰"""
        self.model.eval()
        class_correct = defaultdict(int)
        class_total = defaultdict(int)
        
        # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿åé›†
        all_true_labels = []
        all_predicted_labels = []
        all_predicted_probs = []
        
        with torch.no_grad():
            for batch_data, batch_labels in test_loader:
                batch_data = batch_data.to(self.device)
                batch_labels = batch_labels.to(self.device)
                
                outputs = self.model(batch_data)
                probabilities = F.softmax(outputs, dim=1)
                _, predicted = torch.max(outputs, 1)
                
                # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿åé›†
                if save_predictions:
                    all_true_labels.extend(batch_labels.cpu().numpy().tolist())
                    all_predicted_labels.extend(predicted.cpu().numpy().tolist())
                    all_predicted_probs.extend(probabilities.cpu().numpy().tolist())
                
                for i in range(batch_labels.size(0)):
                    label = batch_labels[i].item()
                    class_total[label] += 1
                    if predicted[i] == batch_labels[i]:
                        class_correct[label] += 1
                    label = batch_labels[i].item()
                    class_total[label] += 1
                    if predicted[i] == batch_labels[i]:
                        class_correct[label] += 1
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦è¨ˆç®—
        class_accuracies = {}
        total_correct = 0
        total_samples = 0
        
        print("=== ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’çµæœ ===")
        for class_idx in range(self.config.num_classes):
            if class_total[class_idx] > 0:
                accuracy = class_correct[class_idx] / class_total[class_idx]
                class_accuracies[class_idx] = accuracy
                total_correct += class_correct[class_idx]
                total_samples += class_total[class_idx]
                print(f"ã‚¯ãƒ©ã‚¹ {class_idx} ç²¾åº¦: {accuracy:.4f} ({class_correct[class_idx]}/{class_total[class_idx]})")
        
        overall_accuracy = total_correct / total_samples
        accuracies_list = list(class_accuracies.values())
        accuracy_range = max(accuracies_list) - min(accuracies_list) if accuracies_list else 0
        success_classes = sum(1 for acc in accuracies_list if acc >= 0.75)
        
        # æ¤œè¨¼ç”¨CSVä¿å­˜ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ä»¥ä¸Šã§æœ€çµ‚ã‚¨ãƒãƒƒã‚¯ã®å ´åˆï¼‰
        csv_file = None
        if save_predictions and self.config.epochs >= 5:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file = save_predictions_to_csv(
                all_true_labels, all_predicted_labels, all_predicted_probs,
                "epoch", self.config.epochs, timestamp
            )
            
            # æ¤œè¨¼å®Ÿè¡Œ
            verification_result = verify_accuracy_from_csv(csv_file, overall_accuracy)
            print_verification_report(verification_result)
        
        results = {
            'overall_accuracy': overall_accuracy,
            'class_accuracies': class_accuracies,
            'accuracy_range': accuracy_range,
            'success_classes': success_classes,
            'training_stats': self.training_stats,
            'csv_file': csv_file
        }
        
        print(f"=== ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’ è©•ä¾¡çµæœ ===")
        print(f"å…¨ä½“ç²¾åº¦: {overall_accuracy:.4f}")
        print(f"ç²¾åº¦ç¯„å›²: {accuracy_range:.4f}")
        print(f"æˆåŠŸã‚¯ãƒ©ã‚¹æ•°: {success_classes}/{self.config.num_classes}")
        
        return results
    
    def setup_multiclass_visualization(self, max_epochs: int = 10) -> None:
        """Phase 2: ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å¯è¦–åŒ–ã®åˆæœŸè¨­å®š"""
        if self.visualizer:
            self._setup_multiclass_visualization(max_epochs)
    
    def _setup_multiclass_visualization(self, max_epochs: int = 10) -> None:
        """Phase 2: ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å¯è¦–åŒ–ã®åˆæœŸè¨­å®š"""
        import matplotlib.pyplot as plt
        
        # æ—¢å­˜ã®figureã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if hasattr(self, 'multiclass_fig') and self.multiclass_fig is not None:
            plt.close(self.multiclass_fig)
        
        # æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°ã‚’ä¿å­˜
        self.max_epochs = max_epochs
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šç¢ºèª
        plt.rcParams['font.family'] = ['Noto Sans CJK JP', 'Yu Gothic', 'Noto Sans JP', 'sans-serif']
        
        self.multiclass_fig, (self.acc_ax, self.loss_ax) = plt.subplots(1, 2, figsize=(16, 8))
        self.multiclass_fig.suptitle('ED-ANN v5.6.4 ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’é€²æ—', fontsize=16, fontweight='bold')
        
        # ç²¾åº¦ã‚°ãƒ©ãƒ•è¨­å®šï¼ˆå·¦ï¼‰
        self.acc_ax.set_title('ç²¾åº¦ (Accuracy)', fontsize=14, fontweight='bold')
        self.acc_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
        self.acc_ax.set_ylabel('ç²¾åº¦', fontsize=12)
        self.acc_ax.grid(True, alpha=0.3)
        self.acc_ax.set_ylim(0.5, 1.0)
        
        # æå¤±ã‚°ãƒ©ãƒ•è¨­å®šï¼ˆå³ï¼‰
        self.loss_ax.set_title('æå¤± (Loss)', fontsize=14, fontweight='bold')
        self.loss_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
        self.loss_ax.set_ylabel('æå¤±', fontsize=12)
        self.loss_ax.grid(True, alpha=0.3)
        self.loss_ax.set_ylim(0.0, 0.5)  # æå¤±ç¯„å›²ã‚’0.0-0.5ã«è¨­å®š
        
        # åˆæœŸçŠ¶æ…‹è¡¨ç¤º
        self.acc_ax.text(0.5, 0.75, 'ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’\nã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚ã«ãƒ—ãƒ­ãƒƒãƒˆæ›´æ–°', 
                        ha='center', va='center', fontsize=12, 
                        transform=self.acc_ax.transAxes)
        self.loss_ax.text(0.5, 0.75, 'ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’\nã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚ã«ãƒ—ãƒ­ãƒƒãƒˆæ›´æ–°', 
                         ha='center', va='center', fontsize=12,
                         transform=self.loss_ax.transAxes)
        
        # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰è¨­å®š
        plt.ion()
        plt.show(block=False)
        
        # åˆæœŸæç”»
        self.multiclass_fig.canvas.draw()
        self.multiclass_fig.canvas.flush_events()
    
    def update_visualization(self, epoch: int) -> None:
        """Phase 2: å¯è¦–åŒ–æ›´æ–°"""
        if not self.visualizer or not hasattr(self, 'multiclass_fig') or self.multiclass_fig is None:
            return
            
        try:
            # ã‚°ãƒ©ãƒ•ã‚’ã‚¯ãƒªã‚¢
            self.acc_ax.clear()
            self.loss_ax.clear()
            
            # ã‚°ãƒ©ãƒ•è¨­å®šã‚’å†é©ç”¨
            self.acc_ax.set_title('ç²¾åº¦ (Accuracy)', fontsize=14, fontweight='bold')
            self.acc_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
            self.acc_ax.set_ylabel('ç²¾åº¦', fontsize=12)
            self.acc_ax.grid(True, alpha=0.3)
            self.acc_ax.set_ylim(0.5, 1.0)
            
            self.loss_ax.set_title('æå¤± (Loss)', fontsize=14, fontweight='bold')
            self.loss_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
            self.loss_ax.set_ylabel('æå¤±', fontsize=12)
            self.loss_ax.grid(True, alpha=0.3)
            self.loss_ax.set_ylim(0.0, 0.5)  # æå¤±ç¯„å›²ã‚’0.0-0.5ã«è¨­å®š
            
            epochs_range = list(range(1, epoch + 2))
            
            # ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
            if self.training_stats['train_accuracy']:
                self.acc_ax.plot(epochs_range, self.training_stats['train_accuracy'], 
                               color='blue', linewidth=2, linestyle='-', marker='o', 
                               label='è¨“ç·´ç²¾åº¦')
            if self.training_stats['test_accuracy']:
                self.acc_ax.plot(epochs_range, self.training_stats['test_accuracy'], 
                               color='red', linewidth=2, linestyle='--', marker='s',
                               label='ãƒ†ã‚¹ãƒˆç²¾åº¦')
            
            if self.training_stats['train_loss']:
                self.loss_ax.plot(epochs_range, self.training_stats['train_loss'], 
                                color='blue', linewidth=2, linestyle='-', marker='o',
                                label='è¨“ç·´Loss')
            if self.training_stats['test_loss']:
                self.loss_ax.plot(epochs_range, self.training_stats['test_loss'], 
                                color='red', linewidth=2, linestyle='--', marker='s',
                                label='ãƒ†ã‚¹ãƒˆLoss')
            
            # å‡¡ä¾‹è¿½åŠ ï¼ˆç²¾åº¦ã‚°ãƒ©ãƒ•ã¯å³ä¸‹ã€æå¤±ã‚°ãƒ©ãƒ•ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰
            self.acc_ax.legend(loc='lower right')
            self.loss_ax.legend()
            
            # Xè»¸ç¯„å›²è¨­å®š
            self.acc_ax.set_xlim(1, max(self.max_epochs, epoch + 1))
            self.loss_ax.set_xlim(1, max(self.max_epochs, epoch + 1))
            
            # æç”»æ›´æ–°
            self.multiclass_fig.canvas.draw()
            self.multiclass_fig.canvas.flush_events()
            
        except Exception as e:
            pass  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶™ç¶š

class RestoredTrainer:
    """
    v5.5.4æº–æ‹  - å¾©å…ƒãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
    ãƒã‚¤ãƒŠãƒªåˆ†é¡æ–¹å¼ã«ã‚ˆã‚‹å„ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨ã®ç‹¬ç«‹å­¦ç¿’
    """
    
    def __init__(self, config: TrainingConfig):
        self.config = config
        
        # ãƒ‡ãƒã‚¤ã‚¹æ±ºå®šï¼ˆCPUå¼·åˆ¶ã‚ªãƒ—ã‚·ãƒ§ãƒ³è€ƒæ…®ï¼‰
        if config.force_cpu:
            self.device = torch.device('cpu')
        else:
            self.device = torch.device(config.device)
        
        # æ±ºå®šè«–çš„ã‚·ãƒ¼ãƒ‰è¨­å®š
        torch.manual_seed(config.random_seed)
        np.random.seed(config.random_seed)
        if torch.cuda.is_available() and not config.force_cpu:
            torch.cuda.manual_seed(config.random_seed)
        
        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç®¡ç†
        # Phase 1: æ¨™æº–Datasetçµ±åˆ
        self.dataset_manager = StandardMNISTDataset(config)
        
        # å„ã‚¯ãƒ©ã‚¹å°‚ç”¨åˆ†é¡å™¨
        self.classifiers = {}
        self.optimizers = {}
        
        # æå¤±é–¢æ•°ï¼ˆv5.5.4æº–æ‹ ï¼‰
        self.criterion = nn.BCELoss()
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–æ©Ÿèƒ½ï¼ˆãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å°‚ç”¨ï¼‰
        self.visualizer = None
        if config.realtime:
            # ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å¯è¦–åŒ–ã‚·ã‚¹ãƒ†ãƒ ã‚’æœ‰åŠ¹åŒ–
            self.visualizer = "multiclass"  # ãƒ•ãƒ©ã‚°ã¨ã—ã¦ä½¿ç”¨
        
        # å­¦ç¿’çµ±è¨ˆ
        self.training_stats = defaultdict(list)
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ç”¨ã®å…¨ä½“è¨“ç·´çµæœç®¡ç†
        self.global_training_results = {}  # ã‚¯ãƒ©ã‚¹åˆ¥è¨“ç·´çµæœã‚’è“„ç©
        
        # çµ±åˆè©•ä¾¡ãƒ•ãƒ©ã‚°ï¼ˆã‚¿ã‚¤ãƒˆãƒ«ç®¡ç†ç”¨ï¼‰
        self._evaluation_started = False
        
        print(f"å¾©å…ƒãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼åˆæœŸåŒ–å®Œäº†: ãƒ‡ãƒã‚¤ã‚¹{self.device}") if self.config.verbose else None
        
    def initialize_classifiers(self):
        """å…¨ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨ã®åˆæœŸåŒ–"""
        print("ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨ã‚’åˆæœŸåŒ–ä¸­...") if self.config.verbose else None
        
        for class_idx in range(self.config.num_classes):
            # v5.5.4æº–æ‹ åˆ†é¡å™¨ä½œæˆ
            classifier = SingleOutputClassifier(
                input_size=784,
                hidden_size=self.config.hidden_size,
                target_class=class_idx
            ).to(self.device)
            
            # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶è¨­å®šï¼ˆv5.5.4æº–æ‹ ï¼‰
            optimizer = torch.optim.Adam(
                classifier.parameters(),
                lr=self.config.learning_rate
            )
            
            self.classifiers[class_idx] = classifier
            self.optimizers[class_idx] = optimizer
            
            # åˆæœŸé‡ã¿çµ±è¨ˆ
            stats = classifier.get_classifier_stats()
            print(f"ã‚¯ãƒ©ã‚¹{class_idx}åˆæœŸé‡ã¿ - éš ã‚Œå±¤ãƒãƒ«ãƒ : {stats['hidden_layer']['norm']:.4f}") if self.config.verbose else None
        
        print("å…¨ã‚¯ãƒ©ã‚¹åˆ†é¡å™¨åˆæœŸåŒ–å®Œäº†") if self.config.verbose else None
    
    def train_classifier(self, class_idx: int) -> Dict:
        """æŒ‡å®šã‚¯ãƒ©ã‚¹ã®åˆ†é¡å™¨ã‚’è¨“ç·´ï¼ˆv5.5.4æº–æ‹ ï¼‰"""
        print(f"=== ã‚¯ãƒ©ã‚¹ {class_idx} ã®è¨“ç·´é–‹å§‹ ===") if self.config.verbose else None
        
        classifier = self.classifiers[class_idx]
        optimizer = self.optimizers[class_idx]
        
        # ãƒã‚¤ãƒŠãƒªåˆ†é¡ç”¨ãƒ‡ãƒ¼ã‚¿ãƒ­ãƒ¼ãƒ€ãƒ¼å–å¾—
        train_loader, test_loader = self.dataset_manager.get_binary_datasets(class_idx)
        
        classifier.train()
        
        class_stats = {
            'epoch_losses': [],
            'epoch_train_accuracies': [],
            'epoch_test_accuracies': [],
            'training_time': 0
        }
        
        training_start = time.time()
        
        for epoch in range(self.config.epochs):
            epoch_start = time.time()
            epoch_losses = []
            epoch_accuracies = []
            
            for step, (data, labels) in enumerate(train_loader):
                data = data.to(self.device)
                labels = labels.to(self.device).float()
                
                # å‰å‘ãè¨ˆç®—
                outputs = classifier(data)
                loss = self.criterion(outputs, labels)
                
                # é€†ä¼æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                # ç²¾åº¦è¨ˆç®—
                predictions = (outputs > 0.5).float()
                accuracy = (predictions == labels).float().mean().item()
                
                epoch_losses.append(loss.item())
                epoch_accuracies.append(accuracy)
                
                # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ è¡¨ç¤º
                if self.config.realtime and step % 10 == 0 and self.config.verbose:
                    logger.info(f"  ã‚¹ãƒ†ãƒƒãƒ— {step}: æå¤± {loss.item():.4f}, ç²¾åº¦ {accuracy:.4f}")
            
            # ã‚¨ãƒãƒƒã‚¯çµ‚äº†å‡¦ç†
            epoch_time = time.time() - epoch_start
            train_accuracy = np.mean(epoch_accuracies)
            train_loss = np.mean(epoch_losses)
            
            # ãƒ†ã‚¹ãƒˆè©•ä¾¡
            test_accuracy, test_loss = self.evaluate_classifier(class_idx, test_loader)
            
            class_stats['epoch_losses'].append(train_loss)
            class_stats['epoch_train_accuracies'].append(train_accuracy)
            class_stats['epoch_test_accuracies'].append(test_accuracy)
            
            # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã®ãŸã‚ã€ã“ã®ã‚¯ãƒ©ã‚¹ã®çµæœã‚’å…¨ä½“çµæœã«æ›´æ–°
            self.global_training_results[class_idx] = {
                'epoch_losses': class_stats['epoch_losses'].copy(),
                'epoch_train_accuracies': class_stats['epoch_train_accuracies'].copy(),
                'epoch_test_accuracies': class_stats['epoch_test_accuracies'].copy()
            }
            
            # ã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚ã«ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã‚’æ›´æ–°
            if self.config.realtime and self.visualizer:
                # ã‚¯ãƒ©ã‚¹9ã®æœ€çµ‚ã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚ã¯ã€å…ˆã«ãƒ•ãƒ©ã‚°ã‚’è¨­å®š
                if class_idx == 9 and epoch == self.config.epochs - 1:
                    self._evaluation_started = True
                
                self.update_multiclass_visualization(epoch + 1, class_idx, float(train_accuracy), float(test_accuracy), float(train_loss), float(test_loss))
                print(f"=== ã‚¨ãƒãƒƒã‚¯ {epoch + 1} å®Œäº†ï¼šãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–æ›´æ–° (ã‚¯ãƒ©ã‚¹{class_idx}) ===") if self.config.verbose else None
            
            print(f"ã‚¨ãƒãƒƒã‚¯ {epoch}: è¨“ç·´ç²¾åº¦ {train_accuracy:.4f}, ãƒ†ã‚¹ãƒˆç²¾åº¦ {test_accuracy:.4f}, æ™‚é–“ {epoch_time:.2f}ç§’") if self.config.verbose else None
        
        class_stats['training_time'] = time.time() - training_start
        
        # æœ€çµ‚çµ±è¨ˆ
        final_stats = classifier.get_classifier_stats()
        print(f"ã‚¯ãƒ©ã‚¹{class_idx}æœ€çµ‚é‡ã¿ - éš ã‚Œå±¤ãƒãƒ«ãƒ : {final_stats['hidden_layer']['norm']:.4f}") if self.config.verbose else None
        
        return class_stats
    
    def evaluate_classifier(self, class_idx: int, test_loader: DataLoader) -> Tuple[float, float]:
        """æŒ‡å®šã‚¯ãƒ©ã‚¹åˆ†é¡å™¨ã®è©•ä¾¡ï¼ˆç²¾åº¦ã¨æå¤±ã‚’è¿”ã™ï¼‰"""
        classifier = self.classifiers[class_idx]
        classifier.eval()
        
        correct = 0
        total = 0
        test_losses = []
        
        with torch.no_grad():
            for data, labels in test_loader:
                data = data.to(self.device)
                labels = labels.to(self.device).float()
                
                outputs = classifier(data)
                predictions = (outputs > 0.5).float()
                
                # ãƒ†ã‚¹ãƒˆæå¤±è¨ˆç®—
                test_loss = self.criterion(outputs, labels)
                test_losses.append(test_loss.item())
                
                correct += (predictions == labels).sum().item()
                total += labels.size(0)
        
        accuracy = correct / total if total > 0 else 0.0
        avg_test_loss = float(np.mean(test_losses)) if test_losses else 0.0
        return accuracy, avg_test_loss
    
    def update_multiclass_visualization(self, epoch: int, class_idx: int,
                                       train_acc: float, test_acc: float, train_loss: float, test_loss: float) -> None:
        """
        å€‹åˆ¥ã‚¯ãƒ©ã‚¹ã®ã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚å¯è¦–åŒ–æ›´æ–°ï¼ˆED-ANNä»•æ§˜æº–æ‹ ï¼‰
        
        Args:
            epoch: ç¾åœ¨ã®ã‚¨ãƒãƒƒã‚¯ (1ã‹ã‚‰é–‹å§‹)
            class_idx: å®Œäº†ã—ãŸã‚¯ãƒ©ã‚¹ç•ªå· (0-9)
            train_acc: è©²å½“ã‚¯ãƒ©ã‚¹ã®è¨“ç·´ç²¾åº¦
            test_acc: è©²å½“ã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆç²¾åº¦  
            train_loss: è©²å½“ã‚¯ãƒ©ã‚¹ã®è¨“ç·´æå¤±
            test_loss: è©²å½“ã‚¯ãƒ©ã‚¹ã®ãƒ†ã‚¹ãƒˆæå¤±
        """
        if not hasattr(self, 'multiclass_fig'):
            # æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°ã‚’å–å¾—ï¼ˆconfigã‹ã‚‰ï¼‰
            max_epochs = getattr(self, 'config', None)
            if max_epochs and hasattr(max_epochs, 'epochs'):
                max_epochs = max_epochs.epochs
            else:
                max_epochs = 10  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            self._setup_multiclass_visualization(max_epochs)
        
        # å‹•çš„ã‚¿ã‚¤ãƒˆãƒ«æ›´æ–°ï¼ˆçµ±åˆè©•ä¾¡é–‹å§‹å¾Œã¯ã‚¹ã‚­ãƒƒãƒ—ï¼‰
        evaluation_started = getattr(self, '_evaluation_started', False)
        if not evaluation_started:
            title = f"ED-ANN v5.6.2 ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å­¦ç¿’é€²æ—ã€€ã€€ã‚¯ãƒ©ã‚¹ {class_idx} ã‚’å­¦ç¿’ä¸­"
            self.multiclass_fig.suptitle(title, fontsize=16, fontweight='bold')
        else:
            # çµ±åˆè©•ä¾¡é–‹å§‹æ¸ˆã¿ã®å ´åˆã¯ã€Œå…¨ä½“ã®ç²¾åº¦ã¨Lossã‚’è¨ˆç®—ä¸­ã€ã«æ›´æ–°
            title = "ED-ANN v5.6.2 ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å­¦ç¿’é€²æ—ã€€ã€€å…¨ä½“ã®ç²¾åº¦ã¨Lossã‚’è¨ˆç®—ä¸­"
            self.multiclass_fig.suptitle(title, fontsize=16, fontweight='bold')
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ãƒ‡ãƒ¼ã‚¿è“„ç©ç”¨ã®åˆæœŸåŒ–
        if not hasattr(self, 'mc_class_data'):
            # ã‚¯ãƒ©ã‚¹åˆ¥ã®å„ã‚¨ãƒãƒƒã‚¯ãƒ‡ãƒ¼ã‚¿ã‚’è¨˜éŒ²
            self.mc_class_data = {
                i: {
                    'train_accs': [0.0] * self.config.epochs,
                    'test_accs': [0.0] * self.config.epochs, 
                    'train_losses': [0.0] * self.config.epochs,
                    'test_losses': [0.0] * self.config.epochs
                } for i in range(10)
            }
            # å…¨ä½“çµ±åˆçµæœç”¨ï¼ˆæœ€å¾Œã«è¨ˆç®—ï¼‰
            self.mc_overall_data = {
                'train_accs': [],
                'test_accs': [],
                'train_losses': [],
                'test_losses': []
            }
        
        # è©²å½“ã‚¯ãƒ©ã‚¹ã®ã‚¨ãƒãƒƒã‚¯çµæœã‚’è¨˜éŒ²
        epoch_idx = epoch - 1  # 0ãƒ™ãƒ¼ã‚¹ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.mc_class_data[class_idx]['train_accs'][epoch_idx] = train_acc
        self.mc_class_data[class_idx]['test_accs'][epoch_idx] = test_acc
        self.mc_class_data[class_idx]['train_losses'][epoch_idx] = train_loss
        self.mc_class_data[class_idx]['test_losses'][epoch_idx] = test_loss
        
        self._update_individual_class_plots()
    
    def _update_individual_class_plots(self) -> None:
        """å€‹åˆ¥ã‚¯ãƒ©ã‚¹å®Œäº†æ™‚ã®ãƒ—ãƒ­ãƒƒãƒˆæ›´æ–°"""
        # ã‚°ãƒ©ãƒ•ã‚’ã‚¯ãƒªã‚¢
        self.acc_ax.clear()
        self.loss_ax.clear()
        
        # ã‚°ãƒ©ãƒ•è¨­å®šã‚’å†é©ç”¨
        self.acc_ax.set_title('ç²¾åº¦ (Accuracy)', fontsize=14, fontweight='bold')
        self.acc_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
        self.acc_ax.set_ylabel('ç²¾åº¦', fontsize=12)
        self.acc_ax.grid(True, alpha=0.3)
        self.acc_ax.set_ylim(0.5, 1.0)  # ç²¾åº¦è»¸ç¯„å›²ã‚’0.5-1.0ã«è¨­å®š
        
        self.loss_ax.set_title('æå¤± (Loss)', fontsize=14, fontweight='bold')
        self.loss_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
        self.loss_ax.set_ylabel('æå¤±', fontsize=12)
        self.loss_ax.grid(True, alpha=0.3)
        self.loss_ax.set_ylim(0.0, 0.5)  # æå¤±è»¸ç¯„å›²ã‚’0.0-0.5ã«è¨­å®š
        
        # å„ã‚¯ãƒ©ã‚¹ã®ç¾åœ¨ã¾ã§ã®ãƒ‡ãƒ¼ã‚¿ã‚’ãƒ—ãƒ­ãƒƒãƒˆ
        epochs_range = list(range(1, self.config.epochs + 1))
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']
        
        for class_idx in range(10):
            color = colors[class_idx]
            
            # ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹éƒ¨åˆ†ã®ã¿ã‚’æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã§ãƒ—ãƒ­ãƒƒãƒˆ
            train_accs = self.mc_class_data[class_idx]['train_accs']
            test_accs = self.mc_class_data[class_idx]['test_accs']
            train_losses = self.mc_class_data[class_idx]['train_losses']
            test_losses = self.mc_class_data[class_idx]['test_losses']
            
            # ãƒ‡ãƒ¼ã‚¿ãŒå…¥åŠ›æ¸ˆã¿ã®ã‚¨ãƒãƒƒã‚¯ç¯„å›²ã‚’ç‰¹å®š
            data_epochs = []
            data_train_accs = []
            data_test_accs = []
            data_train_losses = []
            data_test_losses = []
            
            for epoch_idx in range(self.config.epochs):
                if train_accs[epoch_idx] > 0:  # ãƒ‡ãƒ¼ã‚¿ãŒå…¥åŠ›æ¸ˆã¿
                    epoch_num = epoch_idx + 1
                    data_epochs.append(epoch_num)
                    data_train_accs.append(train_accs[epoch_idx])
                    data_test_accs.append(test_accs[epoch_idx])
                    data_train_losses.append(train_losses[epoch_idx])
                    data_test_losses.append(test_losses[epoch_idx])
            
            # æŠ˜ã‚Œç·šã‚°ãƒ©ãƒ•ã§ãƒ—ãƒ­ãƒƒãƒˆï¼ˆãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
            if data_epochs:
                # ç²¾åº¦ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæŠ˜ã‚Œç·šï¼‰
                self.acc_ax.plot(data_epochs, data_train_accs, 
                               color=color, marker='o', linewidth=1.5, linestyle='-', 
                               label=f'ã‚¯ãƒ©ã‚¹{class_idx}è¨“ç·´')
                self.acc_ax.plot(data_epochs, data_test_accs, 
                               color=color, marker='s', linewidth=1.5, linestyle='--',
                               label=f'ã‚¯ãƒ©ã‚¹{class_idx}ãƒ†ã‚¹ãƒˆ')
                
                # æå¤±ãƒ—ãƒ­ãƒƒãƒˆï¼ˆæŠ˜ã‚Œç·šï¼‰- è¨“ç·´ã¨ãƒ†ã‚¹ãƒˆã®ä¸¡æ–¹
                self.loss_ax.plot(data_epochs, data_train_losses, 
                                color=color, marker='o', linewidth=1.5, linestyle='-',
                                label=f'ã‚¯ãƒ©ã‚¹{class_idx}è¨“ç·´Loss')
                self.loss_ax.plot(data_epochs, data_test_losses, 
                                color=color, marker='s', linewidth=1.5, linestyle='--',
                                label=f'ã‚¯ãƒ©ã‚¹{class_idx}ãƒ†ã‚¹ãƒˆLoss')
        
        # Xè»¸ç¯„å›²è¨­å®šï¼ˆED-ANNä»•æ§˜æº–æ‹ ï¼šæœ€å°1ã€æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°ï¼‰
        self.acc_ax.set_xlim(1, self.config.epochs)
        self.loss_ax.set_xlim(1, self.config.epochs)
        
        # å‡¡ä¾‹è¨­å®šï¼ˆé‡è¤‡å›é¿ï¼‰
        handles_acc, labels_acc = self.acc_ax.get_legend_handles_labels()
        if handles_acc:
            self.acc_ax.legend(handles_acc, labels_acc, loc='lower right', fontsize=6, ncol=2)
        
        handles_loss, labels_loss = self.loss_ax.get_legend_handles_labels()
        if handles_loss:
            self.loss_ax.legend(handles_loss, labels_loss, loc='upper right', fontsize=6, ncol=2)
        
        # æç”»æ›´æ–°
        self.multiclass_fig.canvas.draw()
        self.multiclass_fig.canvas.flush_events()
    
    def finalize_overall_visualization(self, overall_accuracy: float) -> None:
        """
        å…¨å­¦ç¿’å®Œäº†æ™‚ã®çµ±åˆçµæœãƒ—ãƒ­ãƒƒãƒˆï¼ˆED-ANNä»•æ§˜æº–æ‹ ï¼‰
        
        Args:
            overall_accuracy: çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®æœ€çµ‚ç²¾åº¦
        """
        if not hasattr(self, 'mc_class_data'):
            return
            
        # å„ã‚¨ãƒãƒƒã‚¯ã§ã®å…¨ä½“å¹³å‡ã‚’è¨ˆç®—
        for epoch_idx in range(self.config.epochs):
            epoch_train_accs = []
            epoch_test_accs = []
            epoch_train_losses = []
            epoch_test_losses = []
            
            # å…¨ã‚¯ãƒ©ã‚¹ã®è©²å½“ã‚¨ãƒãƒƒã‚¯çµæœã‚’åé›†
            for class_idx in range(10):
                epoch_train_accs.append(self.mc_class_data[class_idx]['train_accs'][epoch_idx])
                epoch_test_accs.append(self.mc_class_data[class_idx]['test_accs'][epoch_idx])
                epoch_train_losses.append(self.mc_class_data[class_idx]['train_losses'][epoch_idx])
                epoch_test_losses.append(self.mc_class_data[class_idx]['test_losses'][epoch_idx])
            
            # å¹³å‡ã‚’è¨ˆç®—ã—ã¦è¨˜éŒ²
            self.mc_overall_data['train_accs'].append(np.mean(epoch_train_accs))
            self.mc_overall_data['test_accs'].append(np.mean(epoch_test_accs))
            self.mc_overall_data['train_losses'].append(np.mean(epoch_train_losses))
            self.mc_overall_data['test_losses'].append(np.mean(epoch_test_losses))
        
        # æœ€çµ‚çš„ãªçµ±åˆçµæœã‚’å«ã‚€å®Œå…¨ãƒ—ãƒ­ãƒƒãƒˆ
        self._update_complete_plots()
        
        # ãƒ—ãƒ­ãƒƒãƒˆå®Œäº†å¾Œã€ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿ã«å¤‰æ›´
        if hasattr(self, 'multiclass_fig') and self.multiclass_fig:
            title = "ED-ANN v5.6.2 ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å­¦ç¿’é€²æ—"  # ãƒ¡ã‚¤ãƒ³ã‚¿ã‚¤ãƒˆãƒ«ã®ã¿
            self.multiclass_fig.suptitle(title, fontsize=16, fontweight='bold')
            plt.draw()  # å³åº§ã«æç”»æ›´æ–°
            plt.pause(0.1)  # ã‚¿ã‚¤ãƒˆãƒ«æ›´æ–°ã‚’è¡¨ç¤º
        
        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºã¨è‡ªå‹•çµ‚äº†æ©Ÿèƒ½
        self._handle_visualization_completion()
    
    def _handle_visualization_completion(self) -> None:
        """å¯è¦–åŒ–å®Œäº†å¾Œã®å‡¦ç†ï¼ˆè‡ªå‹•çµ‚äº†ãƒ»ãƒ¦ãƒ¼ã‚¶ãƒ¼æ“ä½œå¯¾å¿œï¼‰"""
        import matplotlib.pyplot as plt
        import time
        import sys
        
        # ã‚¿ãƒ¼ãƒŸãƒŠãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸è¡¨ç¤ºï¼ˆå‰Šé™¤è¦æ±‚ã«å¾“ã„ç°¡æ½”ã«ï¼‰
        print("ã‚°ãƒ©ãƒ•ãŒ3ç§’é–“è¡¨ç¤ºã•ã‚Œã¾ã™")
        print("ã‚°ãƒ©ãƒ•ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ã®Ã—ãƒœã‚¿ãƒ³ã§æ—©æœŸçµ‚äº†å¯èƒ½")
        
        # 3ç§’é–“å¾…æ©Ÿï¼ˆã‚°ãƒ©ãƒ•è¡¨ç¤ºç¶™ç¶šï¼‰
        start_time = time.time()
        while time.time() - start_time < 3.0:
            # ã‚°ãƒ©ãƒ•ã‚¦ã‚£ãƒ³ãƒ‰ã‚¦ãŒé–‰ã˜ã‚‰ã‚ŒãŸã‹ãƒã‚§ãƒƒã‚¯
            if not plt.get_fignums():
                break
            plt.pause(0.1)
        
        # ã‚°ãƒ©ãƒ•ã‚’é–‰ã˜ã¦ãƒ—ãƒ­ã‚°ãƒ©ãƒ çµ‚äº†
        try:
            if plt.get_fignums():
                plt.close('all')
            
            # matplotlibã®ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã‚’ç¢ºå®Ÿã«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            plt.ioff()  # ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ¢ãƒ¼ãƒ‰ã‚’ç„¡åŠ¹åŒ–
            
        except Exception as e:
            pass  # ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ç¶™ç¶š
        
        # å‰Šé™¤ï¼šçµ‚äº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã¯éè¡¨ç¤º
    
    def _update_complete_plots(self) -> None:
        """å…¨å­¦ç¿’å®Œäº†æ™‚ã®å®Œå…¨ãƒ—ãƒ­ãƒƒãƒˆï¼ˆçµ±åˆçµæœå«ã‚€ï¼‰"""
        # ã‚°ãƒ©ãƒ•ã‚’ã‚¯ãƒªã‚¢
        self.acc_ax.clear()
        self.loss_ax.clear()
        
        # ã‚°ãƒ©ãƒ•è¨­å®šã‚’å†é©ç”¨
        self.acc_ax.set_title('ç²¾åº¦ (Accuracy)', fontsize=14, fontweight='bold')
        self.acc_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
        self.acc_ax.set_ylabel('ç²¾åº¦', fontsize=12)
        self.acc_ax.grid(True, alpha=0.3)
        self.acc_ax.set_ylim(0.5, 1.0)  # ç²¾åº¦è»¸ç¯„å›²ã‚’0.5-1.0ã«è¨­å®š
        
        self.loss_ax.set_title('æå¤± (Loss)', fontsize=14, fontweight='bold')
        self.loss_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
        self.loss_ax.set_ylabel('æå¤±', fontsize=12)
        self.loss_ax.grid(True, alpha=0.3)
        self.loss_ax.set_ylim(0.0, 0.5)  # æå¤±è»¸ç¯„å›²ã‚’0.0-0.5ã«è¨­å®š
        
        epochs_range = list(range(1, self.config.epochs + 1))
        
        # 1. çµ±åˆçµæœãƒ—ãƒ­ãƒƒãƒˆï¼ˆå¤ªç·šãƒ»é»’ï¼‰
        self.acc_ax.plot(epochs_range, self.mc_overall_data['train_accs'], 
                        color='black', linewidth=3, linestyle='-', marker='o', 
                        label='çµ±åˆè¨“ç·´ç²¾åº¦')
        self.acc_ax.plot(epochs_range, self.mc_overall_data['test_accs'], 
                        color='black', linewidth=3, linestyle='--', marker='s',
                        label='çµ±åˆãƒ†ã‚¹ãƒˆç²¾åº¦')
        
        self.loss_ax.plot(epochs_range, self.mc_overall_data['train_losses'], 
                         color='black', linewidth=3, linestyle='-', marker='o',
                         label='çµ±åˆè¨“ç·´Loss')
        self.loss_ax.plot(epochs_range, self.mc_overall_data['test_losses'], 
                         color='black', linewidth=3, linestyle='--', marker='s',
                         label='çµ±åˆãƒ†ã‚¹ãƒˆLoss')
        
        # 2. å„ã‚¯ãƒ©ã‚¹çµæœãƒ—ãƒ­ãƒƒãƒˆï¼ˆç´°ç·šãƒ»ã‚«ãƒ©ãƒ¼ï¼‰
        colors = ['red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']
        
        for class_idx in range(10):
            color = colors[class_idx]
            
            train_accs = self.mc_class_data[class_idx]['train_accs']
            test_accs = self.mc_class_data[class_idx]['test_accs']
            train_losses = self.mc_class_data[class_idx]['train_losses']
            test_losses = self.mc_class_data[class_idx]['test_losses']
            
            # ç²¾åº¦ãƒ—ãƒ­ãƒƒãƒˆ
            self.acc_ax.plot(epochs_range, train_accs, 
                            color=color, linewidth=1.5, linestyle='-', 
                            label=f'ã‚¯ãƒ©ã‚¹{class_idx}è¨“ç·´')
            self.acc_ax.plot(epochs_range, test_accs, 
                            color=color, linewidth=1.5, linestyle='--')
            
            # æå¤±ãƒ—ãƒ­ãƒƒãƒˆï¼ˆè¨“ç·´ã¨ãƒ†ã‚¹ãƒˆã®ä¸¡æ–¹ï¼‰
            self.loss_ax.plot(epochs_range, train_losses, 
                             color=color, linewidth=1.5, linestyle='-',
                             label=f'ã‚¯ãƒ©ã‚¹{class_idx}è¨“ç·´Loss')
            self.loss_ax.plot(epochs_range, test_losses, 
                             color=color, linewidth=1.5, linestyle='--')
        
        # Xè»¸ç¯„å›²è¨­å®šï¼ˆED-ANNä»•æ§˜æº–æ‹ ï¼šæœ€å°1ã€æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°ï¼‰
        self.acc_ax.set_xlim(1, self.config.epochs)
        self.loss_ax.set_xlim(1, self.config.epochs)
        
        # å‡¡ä¾‹è¨­å®š
        self.acc_ax.legend(loc='lower right', fontsize=8, ncol=2)
        self.loss_ax.legend(loc='upper right', fontsize=8, ncol=2)
        
        # æç”»æ›´æ–°
        self.multiclass_fig.canvas.draw()
        self.multiclass_fig.canvas.flush_events()
    
    def setup_multiclass_visualization(self, max_epochs: int = 10) -> None:
        """ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å¯è¦–åŒ–ã®åˆæœŸè¨­å®šï¼ˆå¤–éƒ¨å‘¼ã³å‡ºã—ç”¨ï¼‰"""
        if self.visualizer:
            self._setup_multiclass_visualization(max_epochs)
    
    def _setup_multiclass_visualization(self, max_epochs: int = 10) -> None:
        """ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å¯è¦–åŒ–ã®åˆæœŸè¨­å®š"""
        import matplotlib.pyplot as plt
        
        # æ—¢å­˜ã®figureã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        if hasattr(self, 'multiclass_fig') and self.multiclass_fig is not None:
            plt.close(self.multiclass_fig)
        
        # æœ€å¤§ã‚¨ãƒãƒƒã‚¯æ•°ã‚’ä¿å­˜
        self.max_epochs = max_epochs
        
        # æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šç¢ºèªï¼ˆED-ANNä»•æ§˜æº–æ‹ ï¼‰- è­¦å‘Šãªã—ãƒ•ã‚©ãƒ³ãƒˆã®ã¿
        plt.rcParams['font.family'] = ['Noto Sans CJK JP', 'Yu Gothic', 'Noto Sans JP', 'sans-serif']
        
        self.multiclass_fig, (self.acc_ax, self.loss_ax) = plt.subplots(1, 2, figsize=(16, 8))
        self.multiclass_fig.suptitle('ED-ANN v5.6.2 ãƒãƒ«ãƒã‚¯ãƒ©ã‚¹å­¦ç¿’é€²æ—', fontsize=16, fontweight='bold')
        
        # ç²¾åº¦ã‚°ãƒ©ãƒ•è¨­å®šï¼ˆå·¦ï¼‰
        self.acc_ax.set_title('ç²¾åº¦ (Accuracy)', fontsize=14, fontweight='bold')
        self.acc_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
        self.acc_ax.set_ylabel('ç²¾åº¦', fontsize=12)
        self.acc_ax.grid(True, alpha=0.3)
        self.acc_ax.set_ylim(0.5, 1.0)  # ç²¾åº¦è»¸ç¯„å›²ã‚’0.5-1.0ã«è¨­å®š
        
        # æå¤±ã‚°ãƒ©ãƒ•è¨­å®šï¼ˆå³ï¼‰
        self.loss_ax.set_title('æå¤± (Loss)', fontsize=14, fontweight='bold')
        self.loss_ax.set_xlabel('ã‚¨ãƒãƒƒã‚¯', fontsize=12)
        self.loss_ax.set_ylabel('æå¤±', fontsize=12)
        self.loss_ax.grid(True, alpha=0.3)
        self.loss_ax.set_ylim(0.0, 0.5)  # æå¤±è»¸ç¯„å›²ã‚’0.0-0.5ã«è¨­å®š
        
        # è‰²è¨­å®šï¼ˆ11è‰²ï¼šçµ±åˆ+10ã‚¯ãƒ©ã‚¹ï¼‰
        import matplotlib.pyplot as plt
        colors = ['black', 'red', 'blue', 'green', 'orange', 'purple', 'brown', 'pink', 'gray', 'olive', 'cyan']
        self.multiclass_colors = colors
        
        # åˆæœŸçŠ¶æ…‹è¡¨ç¤ºï¼ˆç©ºã®ã‚°ãƒ©ãƒ•ã§ã€Œæº–å‚™ä¸­ã€ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼‰
        self.acc_ax.text(0.5, 0.75, '1ã‚¨ãƒãƒƒã‚¯ç›®è¨“ç·´é–‹å§‹\nã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚ã«ãƒ—ãƒ­ãƒƒãƒˆæ›´æ–°', 
                        ha='center', va='center', fontsize=12, 
                        bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.7))
        self.loss_ax.text(0.5, 0.25, '1ã‚¨ãƒãƒƒã‚¯ç›®è¨“ç·´é–‹å§‹\nã‚¨ãƒãƒƒã‚¯å®Œäº†æ™‚ã«ãƒ—ãƒ­ãƒƒãƒˆæ›´æ–°', 
                         ha='center', va='center', fontsize=12, 
                         bbox=dict(boxstyle="round,pad=0.5", facecolor="lightblue", alpha=0.7))
        
        plt.tight_layout()
        plt.ion()
        plt.show(block=False)
        
        # åˆæœŸæç”»ã‚’å¼·åˆ¶å®Ÿè¡Œ
        self.multiclass_fig.canvas.draw()
        self.multiclass_fig.canvas.flush_events()
    
    def classify_sample(self, sample: torch.Tensor) -> Dict:
        """v5.5.4æº–æ‹  - çµ±åˆåˆ†é¡ã‚·ã‚¹ãƒ†ãƒ """
        sample = sample.to(self.device)
        
        if len(sample.shape) == 3:  # å˜ä¸€ã‚µãƒ³ãƒ—ãƒ« [C, H, W]
            sample = sample.unsqueeze(0)  # ãƒãƒƒãƒæ¬¡å…ƒè¿½åŠ 
        
        confidence_scores = {}
        
        # å…¨åˆ†é¡å™¨ã§ä¿¡é ¼åº¦è¨ˆç®—
        for class_idx in range(self.config.num_classes):
            classifier = self.classifiers[class_idx]
            classifier.eval()
            
            with torch.no_grad():
                output = classifier(sample)
                confidence = output.item()
                confidence_scores[class_idx] = confidence
        
        # æœ€é«˜ä¿¡é ¼åº¦ã‚¯ãƒ©ã‚¹é¸æŠ
        predicted_class = max(confidence_scores.keys(), key=lambda k: confidence_scores[k])
        max_confidence = confidence_scores[predicted_class]
        
        return {
            'predicted_class': predicted_class,
            'confidence_scores': confidence_scores,
            'max_confidence': max_confidence
        }
    
    def evaluate_integrated_system(self, save_predictions: bool = False) -> Dict:
        """çµ±åˆã‚·ã‚¹ãƒ†ãƒ ã®è©•ä¾¡ï¼ˆäºˆæ¸¬çµæœè¨˜éŒ²æ©Ÿèƒ½ä»˜ãï¼‰"""
        print("=== çµ±åˆã‚·ã‚¹ãƒ†ãƒ è©•ä¾¡é–‹å§‹ ===")
        
        # å…¨ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§è©•ä¾¡
        test_dataset = self.dataset_manager.test_dataset
        
        class_accuracies = {}
        class_counts = defaultdict(int)
        class_correct = defaultdict(int)
        
        total_correct = 0
        total_samples = 0
        
        # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿åé›†
        all_true_labels = []
        all_predicted_labels = []
        all_predicted_probs = []
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦è¨ˆç®—
        for true_class in range(self.config.num_classes):
            class_indices = self.dataset_manager.class_indices['test'][true_class]
            test_indices = class_indices[:self.config.test_samples_per_class]
            
            for idx in test_indices:
                data, _ = test_dataset[idx]
                
                # åˆ†é¡å®Ÿè¡Œ
                result = self.classify_sample(data)
                predicted_class = result['predicted_class']
                confidence_scores = result['confidence_scores']
                
                # æ¤œè¨¼ç”¨ãƒ‡ãƒ¼ã‚¿åé›†
                if save_predictions:
                    all_true_labels.append(true_class)
                    all_predicted_labels.append(predicted_class)
                    # confidence_scoresã‚’ç¢ºç‡ã¨ã—ã¦æ­£è¦åŒ–
                    prob_list = [confidence_scores.get(i, 0.0) for i in range(10)]
                    prob_sum = sum(prob_list) if sum(prob_list) > 0 else 1.0
                    normalized_probs = [p / prob_sum for p in prob_list]
                    all_predicted_probs.append(normalized_probs)
                
                class_counts[true_class] += 1
                if predicted_class == true_class:
                    class_correct[true_class] += 1
                    total_correct += 1
                
                total_samples += 1
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ç²¾åº¦ç®—å‡º
        for class_idx in range(self.config.num_classes):
            accuracy = class_correct[class_idx] / class_counts[class_idx] if class_counts[class_idx] > 0 else 0
            class_accuracies[class_idx] = accuracy
            print(f"ã‚¯ãƒ©ã‚¹ {class_idx} ç²¾åº¦: {accuracy:.4f} ({class_correct[class_idx]}/{class_counts[class_idx]})")
        
        overall_accuracy = total_correct / total_samples if total_samples > 0 else 0
        
        # ã‚¯ãƒ©ã‚¹é–“æ ¼å·®åˆ†æ
        accuracies_list = list(class_accuracies.values())
        accuracy_range = max(accuracies_list) - min(accuracies_list)
        
        # æ¤œè¨¼ç”¨CSVä¿å­˜ï¼ˆ5ã‚¨ãƒãƒƒã‚¯ä»¥ä¸Šã®å ´åˆï¼‰
        csv_file = None
        if save_predictions and self.config.epochs >= 5:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            csv_file = save_predictions_to_csv(
                all_true_labels, all_predicted_labels, all_predicted_probs,
                "class", self.config.epochs, timestamp
            )
            
            # æ¤œè¨¼å®Ÿè¡Œ
            verification_result = verify_accuracy_from_csv(csv_file, overall_accuracy)
            print_verification_report(verification_result)
        
        results = {
            'class_accuracies': class_accuracies,
            'overall_accuracy': overall_accuracy,
            'accuracy_range': accuracy_range,
            'min_accuracy': min(accuracies_list),
            'max_accuracy': max(accuracies_list),
            'successful_classes': sum(1 for acc in accuracies_list if acc >= 0.75),  # 75%ä»¥ä¸Š
            'csv_file': csv_file
        }
        
        print(f"=== è©•ä¾¡çµæœ ===")
        print(f"å…¨ä½“ç²¾åº¦: {overall_accuracy:.4f}")
        print(f"ç²¾åº¦ç¯„å›²: {accuracy_range:.4f}")
        print(f"æˆåŠŸã‚¯ãƒ©ã‚¹æ•°: {results['successful_classes']}/10")
        
        return results

def main():
    """Phase 1 & Phase 2 æ¯”è¼ƒå®Ÿè¡Œãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description='ED-ANN v5.6.4 - ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±è¡¨ç¤ºç‰ˆ')
    parser.add_argument('--epochs', type=int, default=3, help='è¨“ç·´ã‚¨ãƒãƒƒã‚¯æ•° (default: 3)')
    parser.add_argument('--learning_rate', type=float, default=0.01, help='å­¦ç¿’ç‡ (default: 0.01)')
    parser.add_argument('--batch_size', type=int, default=32, help='ãƒãƒƒãƒã‚µã‚¤ã‚º (default: 32)')
    parser.add_argument('--hidden_size', type=int, default=64, help='éš ã‚Œå±¤ã‚µã‚¤ã‚º (default: 64)')
    parser.add_argument('--train_size', type=int, default=1000, help='ã‚¯ãƒ©ã‚¹åˆ¥è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•° (default: 1000)')
    parser.add_argument('--test_size', type=int, default=1000, help='ã‚¯ãƒ©ã‚¹åˆ¥ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•° (default: 1000)')
    parser.add_argument('--realtime', action='store_true', help='ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å­¦ç¿’è¡¨ç¤º (default: OFF)')
    parser.add_argument('--cpu', action='store_true', help='CPUå¼·åˆ¶ä½¿ç”¨ (default: è‡ªå‹•åˆ¤åˆ¥)')
    parser.add_argument('--seed', type=int, help='ã‚·ãƒ¼ãƒ‰å€¤ (ç„¡æŒ‡å®šæ™‚ã¯ãƒ©ãƒ³ãƒ€ãƒ å€¤)')
    parser.add_argument('--verbose', action='store_true', help='è©³ç´°ãƒ­ã‚°è¡¨ç¤º (default: OFF)')
    parser.add_argument('--verify', action='store_true', help='ç²¾åº¦æ¤œè¨¼æ©Ÿèƒ½(çµæœCSVæ›¸ãå‡ºã—) (default: OFF)')
    parser.add_argument('--mode', type=str, choices=['epoch', 'class', 'both'], 
                       default='epoch', help='å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰é¸æŠ (default: epoch) (epoch=ã‚¨ãƒãƒƒã‚¯å˜ä½ã€class=ã‚¯ãƒ©ã‚¹å˜ä½ã€both=æ¯”è¼ƒ)')
    
    args = parser.parse_args()
    
    # ã‚·ãƒ¼ãƒ‰å€¤ã®æ±ºå®šï¼ˆ--seedãŒæŒ‡å®šã•ã‚Œãªã‘ã‚Œã°ãƒ©ãƒ³ãƒ€ãƒ å€¤ã‚’ç”Ÿæˆï¼‰
    if args.seed is None:
        import random
        import time
        random_seed = int(time.time() * 1000) % 10000  # ç¾åœ¨æ™‚åˆ»ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ
        print(f"ğŸ² ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰: {random_seed} (è‡ªå‹•ç”Ÿæˆ)")
    else:
        random_seed = args.seed
        print(f"ğŸ¯ ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰: {random_seed} (æŒ‡å®šå€¤)")
    
    # verboseã‚ªãƒ—ã‚·ãƒ§ãƒ³ã«åŸºã¥ã„ã¦ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–
    global logger, VERBOSE_MODE
    logger = setup_logging(verbose=args.verbose)
    VERBOSE_MODE = args.verbose
    
    # å­¦ç¿’é–‹å§‹ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
    print("ğŸš€ ED-ANN v5.6.4 - ãƒ¢ãƒ‡ãƒ«ãƒ»ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±è¡¨ç¤ºç‰ˆ")
    
    # ç²¾åº¦æ¤œè¨¼æ©Ÿèƒ½ä½¿ç”¨æ™‚ã®ã‚¨ãƒãƒƒã‚¯æ•°ãƒã‚§ãƒƒã‚¯
    if args.verify and args.epochs < 5:
        print("\nâŒ ã‚¨ãƒ©ãƒ¼: ç²¾åº¦æ¤œè¨¼æ©Ÿèƒ½(--verify)ã‚’ä½¿ç”¨ã™ã‚‹å ´åˆã¯5ã‚¨ãƒãƒƒã‚¯ä»¥ä¸Šã®ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
        print(f"   ç¾åœ¨æŒ‡å®šã•ã‚ŒãŸã‚¨ãƒãƒƒã‚¯æ•°: {args.epochs}")
        print(f"   å¿…è¦ãªã‚¨ãƒãƒƒã‚¯æ•°: 5ä»¥ä¸Š")
        print("\nğŸ’¡ ãƒ’ãƒ³ãƒˆ: ä»¥ä¸‹ã®ã‚ˆã†ã«ã‚¨ãƒãƒƒã‚¯æ•°ã‚’æŒ‡å®šã—ã¦ãã ã•ã„:")
        print(f"   python {sys.argv[0]} --epochs 5 --verify")
        return
    
    # è¨­å®šä½œæˆ
    config = TrainingConfig(
        epochs=args.epochs,
        learning_rate=args.learning_rate,
        batch_size=args.batch_size,
        hidden_size=args.hidden_size,
        train_samples_per_class=args.train_size,
        test_samples_per_class=args.test_size,
        realtime=args.realtime,
        force_cpu=args.cpu,
        random_seed=random_seed,
        verbose=args.verbose,
        verify=args.verify
    )
    
    # ãƒ¢ãƒ‡ãƒ«æ§‹é€ ã¨ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æƒ…å ±ã‚’è¡¨ç¤º
    display_model_summary(config.hidden_size)
    display_hyperparameters()
    
    results = {}
    
    # ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’
    if args.mode in ['class', 'both']:
        print("\n" + "="*60)
        print("ğŸ¯ ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’å®Ÿè¡Œ")
        print("="*60)
        
        trainer_p1 = RestoredTrainer(config)
        trainer_p1.initialize_classifiers()
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ï¼ˆPhase 1ã®ã¿ï¼‰
        if config.realtime and args.mode == 'phase1':
            trainer_p1.setup_multiclass_visualization(config.epochs)
        
        # ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’å®Ÿè¡Œ
        training_results = {}
        for class_idx in range(config.num_classes):
            print(f"=== ã‚¯ãƒ©ã‚¹ {class_idx} è¨“ç·´å®Ÿè¡Œä¸­ ===") if config.verbose else None
            class_result = trainer_p1.train_classifier(class_idx)
            training_results[class_idx] = class_result
            print(f"=== ã‚¯ãƒ©ã‚¹ {class_idx} è¨“ç·´å®Œäº† ===") if config.verbose else None
        
        # ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’æœ€çµ‚è©•ä¾¡
        p1_results = trainer_p1.evaluate_integrated_system(save_predictions=config.verify)
        results['class'] = p1_results
        
        # ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–çµ‚äº†å‡¦ç†
        if config.realtime and hasattr(trainer_p1, 'multiclass_fig') and trainer_p1.multiclass_fig is not None:
            try:
                plt.close(trainer_p1.multiclass_fig)
            except:
                pass
    
    # ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’
    if args.mode in ['epoch', 'both']:
        print("\n" + "="*60)
        print("ğŸ”„ ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’å®Ÿè¡Œ")  
        print("="*60)
        
        trainer_p2 = EpochBasedTrainer(config)
        p2_results = trainer_p2.train_epoch_based()
        results['epoch'] = p2_results
        
        # Phase 2å¯è¦–åŒ–çµ‚äº†å‡¦ç†
        if config.realtime and hasattr(trainer_p2, 'multiclass_fig') and trainer_p2.multiclass_fig is not None:
            try:
                plt.close(trainer_p2.multiclass_fig)
            except:
                pass
    
    # çµæœæ¯”è¼ƒè¡¨ç¤º
    if args.mode == 'both' and len(results) == 2:
        print("\n" + "="*80)
        print("ğŸ“Š ã‚¯ãƒ©ã‚¹å˜ä½ vs ã‚¨ãƒãƒƒã‚¯å˜ä½ æ¯”è¼ƒçµæœ")
        print("="*80)
        
        p1 = results['class']
        p2 = results['epoch']
        
        print(f"{'ãƒ¡ãƒˆãƒªã‚¯ã‚¹':<20} {'ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’':<20} {'ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’':<20} {'å·®åˆ†':<15}")
        print("-" * 80)
        print(f"{'å…¨ä½“ç²¾åº¦':<20} {p1['overall_accuracy']:<20.4f} {p2['overall_accuracy']:<20.4f} {p2['overall_accuracy'] - p1['overall_accuracy']:+.4f}")
        print(f"{'ç²¾åº¦ç¯„å›²':<20} {p1['accuracy_range']:<20.4f} {p2['accuracy_range']:<20.4f} {p2['accuracy_range'] - p1['accuracy_range']:+.4f}")
        print(f"{'æˆåŠŸã‚¯ãƒ©ã‚¹æ•°':<20} {p1['successful_classes']:<20} {p2['success_classes']:<20} {p2['success_classes'] - p1['successful_classes']:+}")
        
        print("\nğŸ† æ¨å¥¨æ‰‹æ³•:", end=" ")
        if p2['overall_accuracy'] > p1['overall_accuracy']:
            print("ã‚¨ãƒãƒƒã‚¯å˜ä½å­¦ç¿’ãŒå„ªä½")
        elif p1['overall_accuracy'] > p2['overall_accuracy']:
            print("ã‚¯ãƒ©ã‚¹å˜ä½å­¦ç¿’ãŒå„ªä½")
        else:
            print("ä¸¡æ‰‹æ³•åŒç­‰æ€§èƒ½")
    
    return results

def display_model_summary(hidden_size=64, model_name: str = "ED-ANN Model"):
    """
    TensorFlow model.summary()ãƒ©ã‚¤ã‚¯ãªãƒ¢ãƒ‡ãƒ«æƒ…å ±è¡¨ç¤º
    
    Args:
        hidden_size: éš ã‚Œå±¤ã‚µã‚¤ã‚º
        model_name: ãƒ¢ãƒ‡ãƒ«å
    """
    print("\n" + "="*80)
    print(f"ğŸ“‹ {model_name} æ§‹é€ æƒ…å ±")
    print("="*80)
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®å®šç¾©ï¼ˆMNISTãƒ‡ãƒ¼ã‚¿ã«åŸºã¥ãï¼‰
    input_size = 784   # MNIST: 28x28 = 784
    output_size = 10   # MNIST: 10ã‚¯ãƒ©ã‚¹
    
    print(f"{'Layer (type)':<25} {'Output Shape':<20} {'Param #':<15}")
    print("-" * 80)
    
    # å…¥åŠ›å±¤ â†’ éš ã‚Œå±¤(1)
    input_to_hidden_params = input_size * hidden_size + hidden_size  # é‡ã¿ + ãƒã‚¤ã‚¢ã‚¹
    print(f"{'å…¥åŠ›å±¤ (Linear)':<25} {'(None, ' + str(hidden_size) + ')':<20} {input_to_hidden_params:<15,}")
    
    # éš ã‚Œå±¤(1) ã®æ´»æ€§åŒ–é–¢æ•°ï¼ˆReLUï¼‰
    # æ³¨æ„: ReLUã¯æ´»æ€§åŒ–é–¢æ•°ã§ã‚ã‚Šã€å®Ÿéš›ã®ED-ANNã§ã¯éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®æ´»æ€§åŒ–ã«ä½¿ç”¨
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã¯æŒãŸãªã„
    print(f"{'éš ã‚Œå±¤(1) (ReLU)':<25} {'(None, ' + str(hidden_size) + ')':<20} {'0':<15}")
    
    # éš ã‚Œå±¤(1) â†’ å‡ºåŠ›å±¤
    hidden_to_output_params = hidden_size * output_size + output_size  # é‡ã¿ + ãƒã‚¤ã‚¢ã‚¹
    print(f"{'å‡ºåŠ›å±¤ (Linear)':<25} {'(None, ' + str(output_size) + ')':<20} {hidden_to_output_params:<15,}")
    
    print("=" * 80)
    total_params = input_to_hidden_params + hidden_to_output_params
    print(f"Total params: {total_params:,}")
    print(f"Trainable params: {total_params:,}")
    print(f"Non-trainable params: 0")
    print("="*80)

def display_hyperparameters():
    """
    HyperParametersã‚¯ãƒ©ã‚¹ã®å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¡¨ç¤º
    """
    print("\n" + "="*80)
    print("EDæ³•ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è¨­å®š")
    print("="*80)
    
    hp = HyperParameters()
    
    print("EDæ³•ç†è«–ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿:")
    print(f"   d_plus: float = {hp.d_plus}      # ã‚¢ãƒŸãƒ³æ¿ƒåº¦å¢—åŠ é‡ï¼ˆæ­£ç­”æ™‚ã®é‡ã¿å¢—åŠ åˆ¶å¾¡ï¼‰")
    print(f"   d_minus: float = {hp.d_minus}    # ã‚¢ãƒŸãƒ³æ¿ƒåº¦æ¸›å°‘é‡ï¼ˆèª¤ç­”æ™‚ã®é‡ã¿æ¸›å°‘åˆ¶å¾¡ï¼‰")
    
    print("\nå­¦ç¿’ç‡é–¢é€£:")
    print(f"   base_learning_rate: float = {hp.base_learning_rate}    # åŸºæœ¬å­¦ç¿’ç‡ï¼ˆAdam optimizerç”¨ï¼‰")
    print(f"   ed_learning_rate: float = {hp.ed_learning_rate}     # EDæ³•å°‚ç”¨å­¦ç¿’ç‡ï¼ˆé‡ã¿æ›´æ–°åˆ¶å¾¡ï¼‰")
    
    print("\né‡ã¿æ›´æ–°åˆ¶å¾¡:")
    print(f"   weight_decay: float = {hp.weight_decay}          # é‡ã¿æ¸›è¡°ï¼ˆéå­¦ç¿’é˜²æ­¢ï¼‰")
    print(f"   momentum: float = {hp.momentum}               # ãƒ¢ãƒ¼ãƒ¡ãƒ³ã‚¿ãƒ ï¼ˆå­¦ç¿’å®‰å®šæ€§å‘ä¸Šï¼‰")
    
    print("\nEDæ³•ç‰¹æœ‰åˆ¶ç´„:")
    print(f"   preserve_sign: bool = {hp.preserve_sign}          # é‡ã¿ç¬¦å·ä¿æŒï¼ˆEDæ³•æ ¸å¿ƒåˆ¶ç´„ï¼‰")
    print(f"   abs_increase_only: bool = {hp.abs_increase_only}      # çµ¶å¯¾å€¤å¢—åŠ ã®ã¿è¨±å¯")
    
    print("\nã‚¢ãƒŸãƒ³æ¿ƒåº¦åˆ¶å¾¡:")
    print(f"   amine_threshold: float = {hp.amine_threshold}        # ã‚¢ãƒŸãƒ³æ¿ƒåº¦é–¾å€¤ï¼ˆå­¦ç¿’åˆ¤å®šåŸºæº–ï¼‰")
    print(f"   concentration_decay: float = {hp.concentration_decay}   # æ¿ƒåº¦æ¸›è¡°ç‡ï¼ˆæ™‚é–“çµŒéã«ã‚ˆã‚‹æ¸›è¡°ï¼‰")
    
    print("\nã‚¯ãƒ©ã‚¹åˆ¥å­¦ç¿’åˆ¶å¾¡:")
    print(f"   class_learning_balance: bool = {hp.class_learning_balance}  # ã‚¯ãƒ©ã‚¹é–“å­¦ç¿’ãƒãƒ©ãƒ³ã‚¹èª¿æ•´")
    print(f"   adaptive_rate: bool = {hp.adaptive_rate}         # é©å¿œçš„å­¦ç¿’ç‡ï¼ˆå®Ÿé¨“çš„æ©Ÿèƒ½ï¼‰")
    
    print("="*80)

def save_predictions_to_csv(true_labels: List[int], predicted_labels: List[int], 
                           predicted_probs: List[List[float]], mode: str, 
                           epoch: int, timestamp: str) -> str:
    """
    äºˆæ¸¬çµæœã‚’CSVå½¢å¼ã§ä¿å­˜ï¼ˆæ¤œè¨¼ç”¨ï¼‰
    
    Args:
        true_labels: æ­£è§£ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ
        predicted_labels: äºˆæ¸¬ãƒ©ãƒ™ãƒ«ã®ãƒªã‚¹ãƒˆ
        predicted_probs: å„ã‚¯ãƒ©ã‚¹ã®äºˆæ¸¬ç¢ºç‡ã®ãƒªã‚¹ãƒˆ
        mode: å­¦ç¿’ãƒ¢ãƒ¼ãƒ‰ï¼ˆepoch/classï¼‰
        epoch: ã‚¨ãƒãƒƒã‚¯æ•°
        timestamp: ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—
    
    Returns:
        ä¿å­˜ã•ã‚ŒãŸCSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
    """
    filename = f"ed_ann_predictions_{mode}_{epoch}ep_{timestamp}.csv"
    
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        writer = csv.writer(csvfile)
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œ
        header = ['sample_id', 'true_label', 'predicted_label'] + \
                [f'prob_class_{i}' for i in range(10)]
        writer.writerow(header)
        
        # ãƒ‡ãƒ¼ã‚¿è¡Œ
        for i, (true_label, pred_label, probs) in enumerate(zip(true_labels, predicted_labels, predicted_probs)):
            row = [i, true_label, pred_label] + probs
            writer.writerow(row)
    
    return filename

def verify_accuracy_from_csv(csv_file: str, displayed_accuracy: float, 
                           displayed_loss: Optional[float] = None) -> Dict:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ç²¾åº¦ã¨Lossã‚’å†è¨ˆç®—ã—ã€è¡¨ç¤ºå€¤ã¨æ¯”è¼ƒæ¤œè¨¼
    
    Args:
        csv_file: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        displayed_accuracy: è¡¨ç¤ºã•ã‚ŒãŸç²¾åº¦
        displayed_loss: è¡¨ç¤ºã•ã‚ŒãŸLossï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
    
    Returns:
        æ¤œè¨¼çµæœã®è¾æ›¸
    """
    true_labels = []
    predicted_labels = []
    predicted_probs = []
    
    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿
    with open(csv_file, 'r', encoding='utf-8') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            true_labels.append(int(row['true_label']))
            predicted_labels.append(int(row['predicted_label']))
            
            # äºˆæ¸¬ç¢ºç‡ã‚’å–å¾—
            probs = [float(row[f'prob_class_{i}']) for i in range(10)]
            predicted_probs.append(probs)
    
    # ç²¾åº¦è¨ˆç®—
    correct_predictions = sum(1 for true, pred in zip(true_labels, predicted_labels) if true == pred)
    calculated_accuracy = correct_predictions / len(true_labels)
    
    # Lossè¨ˆç®—ï¼ˆCrossEntropyLossï¼‰
    calculated_loss = 0.0
    if predicted_probs:
        for true_label, probs in zip(true_labels, predicted_probs):
            # log(softmax(probs))ã§CrossEntropyLossè¨ˆç®—
            log_prob = np.log(max(probs[true_label], 1e-10))  # æ•°å€¤å®‰å®šæ€§ã®ãŸã‚æœ€å°å€¤åˆ¶é™
            calculated_loss -= log_prob
        calculated_loss /= len(true_labels)
    
    # æ¤œè¨¼çµæœ
    accuracy_match = abs(calculated_accuracy - displayed_accuracy) < 1e-6
    loss_match = True
    if displayed_loss is not None:
        loss_match = abs(calculated_loss - displayed_loss) < 1e-6
    
    verification_result = {
        'csv_file': csv_file,
        'sample_count': len(true_labels),
        'calculated_accuracy': calculated_accuracy,
        'displayed_accuracy': displayed_accuracy,
        'accuracy_match': accuracy_match,
        'calculated_loss': calculated_loss,
        'displayed_loss': displayed_loss,
        'loss_match': loss_match,
        'verification_status': 'PASS' if accuracy_match and loss_match else 'FAIL'
    }
    
    return verification_result

def print_verification_report(verification_result: Dict):
    """
    æ¤œè¨¼çµæœãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º
    """
    print("\n" + "="*80)
    print("ğŸ” ED-ANN äºˆæ¸¬ç²¾åº¦æ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ")
    print("="*80)
    
    result = verification_result
    status_icon = "âœ…" if result['verification_status'] == 'PASS' else "âŒ"
    
    print(f"ğŸ“Š æ¤œè¨¼ãƒ•ã‚¡ã‚¤ãƒ«: {result['csv_file']}")
    print(f"ğŸ“ˆ ã‚µãƒ³ãƒ—ãƒ«æ•°: {result['sample_count']:,}")
    print(f"\nğŸ¯ ç²¾åº¦æ¤œè¨¼:")
    print(f"   è¡¨ç¤ºç²¾åº¦: {result['displayed_accuracy']:.6f}")
    print(f"   è¨ˆç®—ç²¾åº¦: {result['calculated_accuracy']:.6f}")
    print(f"   ç²¾åº¦ä¸€è‡´: {'âœ… YES' if result['accuracy_match'] else 'âŒ NO'}")
    
    if result['displayed_loss'] is not None:
        print(f"\nğŸ“‰ Lossæ¤œè¨¼:")
        print(f"   è¡¨ç¤ºLoss: {result['displayed_loss']:.6f}")
        print(f"   è¨ˆç®—Loss: {result['calculated_loss']:.6f}")
        print(f"   Lossä¸€è‡´: {'âœ… YES' if result['loss_match'] else 'âŒ NO'}")
    
    print(f"\nğŸ” æ¤œè¨¼çµæœ: {status_icon} {result['verification_status']}")
    print("="*80)

if __name__ == '__main__':
    setup_japanese_font()
    results = main()
