
#!/usr/bin/env python3
"""
ç´”æ­£EDæ³•ï¼ˆError Diffusion Learning Algorithmï¼‰æœ€å°å®Ÿè£…ç‰ˆ
Original C implementation by Isamu Kaneko (1999) - æ•™è‚²ç”¨ã‚·ãƒ³ãƒ—ãƒ«ç‰ˆ
"""

import numpy as np
import random
import math
import time
import argparse
import os
from typing import List, Tuple, Optional
from tqdm import tqdm
from modules.ed_core import EDGenuine
from modules.network_mnist import EDNetworkMNIST
from modules.data_loader import MiniBatchDataLoader

# æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆè¨­å®šï¼ˆæ–‡å­—åŒ–ã‘é˜²æ­¢ï¼‰
import matplotlib
import matplotlib.pyplot as plt
import matplotlib.font_manager as fm

# åˆ©ç”¨å¯èƒ½ãªæ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’å„ªå…ˆé †ä½ã§è¨­å®š
japanese_fonts = ['Noto Sans CJK JP', 'BIZ UDGothic', 'Noto Sans JP', 'DejaVu Sans']
available_font = None
for font_name in japanese_fonts:
    try:
        if font_name in [f.name for f in fm.fontManager.ttflist]:
            available_font = font_name
            break
    except:
        continue

if available_font:
    plt.rcParams['font.family'] = available_font
    print(f"æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆã‚’è¨­å®š: {available_font}")
else:
    print("è­¦å‘Š: æ—¥æœ¬èªãƒ•ã‚©ãƒ³ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")

class NetworkStructure:
    def __init__(self, input_size, hidden_layers, output_size):
        """
        ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ åˆæœŸåŒ–
        
        Args:
            input_size (int): å…¥åŠ›å±¤ã‚µã‚¤ã‚º (ä¾‹: 784 for MNIST)
            hidden_layers (list[int]): éš ã‚Œå±¤æ§‹é€  (ä¾‹: [256, 128, 64])
            output_size (int): å‡ºåŠ›å±¤ã‚µã‚¤ã‚º (ä¾‹: 10 for 10-class classification)
        """
        self.input_size = input_size
        self.hidden_layers = hidden_layers if isinstance(hidden_layers, list) else [hidden_layers]
        self.output_size = output_size
        
        # ed_multi.prompt.mdæº–æ‹ ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½“ç³»è¨ˆç®—
        # ä»•æ§˜: 0,1(ãƒã‚¤ã‚¢ã‚¹), 2ï½in+1(å…¥åŠ›å±¤), in+2(å‡ºåŠ›é–‹å§‹), in+3ï½all+1(éš ã‚Œå±¤)
        
        # Cå®Ÿè£…å¤‰æ•°ã®å†ç¾
        self.in_units = input_size * 2  # èˆˆå¥®æ€§ãƒ»æŠ‘åˆ¶æ€§ãƒšã‚¢ (inå¤‰æ•°ã«ç›¸å½“)
        self.hd_units = sum(self.hidden_layers)  # éš ã‚Œå±¤ãƒ¦ãƒ‹ãƒƒãƒˆç·æ•° (hdå¤‰æ•°ã«ç›¸å½“)
        self.ot_units = output_size  # å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•° (otå¤‰æ•°ã«ç›¸å½“)
        self.all_units = self.in_units + self.hd_units + self.ot_units  # ç·ãƒ¦ãƒ‹ãƒƒãƒˆæ•° (allå¤‰æ•°ã«ç›¸å½“)
        
        # ed_multi.prompt.mdä»•æ§˜æº–æ‹ ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½“ç³»
        self.bias_start = 0
        self.bias_end = 1
        self.input_start = 2
        self.input_end = 2 + self.in_units - 1  # = in+1 in C code
        self.output_pos = self.input_end + 1    # = in+2 in C code (å‡ºåŠ›å±¤é–‹å§‹ä½ç½®)
        self.hidden_start = self.output_pos + 1 # = in+3 in C code (éš ã‚Œå±¤é–‹å§‹)
        self.hidden_end = self.hidden_start + self.hd_units - 1  # = all+1 in C code
        
        # åˆ©ä¾¿æ€§ã®ãŸã‚ã®è¿½åŠ ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£
        self.total_layers = len(self.hidden_layers) + 2  # å…¥åŠ›å±¤ + éš ã‚Œå±¤æ•° + å‡ºåŠ›å±¤
        self.excitatory_input_size = self.in_units  # å¾Œæ–¹äº’æ›æ€§
        
        # å±¤åˆ¥é–‹å§‹ä½ç½®è¨ˆç®—ï¼ˆå¤šå±¤å¯¾å¿œï¼‰
        self.layer_starts = []
        self.layer_starts.append(self.input_start)  # å…¥åŠ›å±¤é–‹å§‹: 2
        
        # éš ã‚Œå±¤ã®å„å±¤é–‹å§‹ä½ç½®ã‚’è¨ˆç®—
        current_pos = self.hidden_start
        for layer_size in self.hidden_layers:
            self.layer_starts.append(current_pos)
            current_pos += layer_size
        
        self.layer_starts.append(self.output_pos)  # å‡ºåŠ›å±¤é–‹å§‹: in+2
    
    def get_layer_range(self, layer_index):
        """
        æŒ‡å®šã—ãŸå±¤ã®ãƒ¦ãƒ‹ãƒƒãƒˆç¯„å›²ã‚’å–å¾—ï¼ˆed_multi.prompt.mdä»•æ§˜æº–æ‹ ï¼‰
        
        Args:
            layer_index (int): å±¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ (0: å…¥åŠ›, 1-N: éš ã‚Œå±¤, N+1: å‡ºåŠ›)
        
        Returns:
            tuple: (start_index, end_index)
        """
        if layer_index == 0:  # å…¥åŠ›å±¤: 2 ï½ in+1
            return (self.input_start, self.input_end)
        elif layer_index <= len(self.hidden_layers):  # éš ã‚Œå±¤: in+3 ï½ all+1
            start = self.layer_starts[layer_index]
            if layer_index < len(self.hidden_layers):
                end = self.layer_starts[layer_index + 1] - 1
            else:
                end = self.hidden_end
            return (start, end)
        else:  # å‡ºåŠ›å±¤: in+2 (å˜ä¸€ä½ç½®)
            return (self.output_pos, self.output_pos)
    
    def is_single_layer(self):
        """å˜å±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return len(self.hidden_layers) == 1
    
    def is_multi_layer(self):
        """å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        return len(self.hidden_layers) > 1
    
    def calculate_amine_diffusion_coefficient(self, layer_distance):
        """
        å±¤é–“è·é›¢ã«åŸºã¥ãã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•°è¨ˆç®—
        
        Args:
            layer_distance (int): å±¤é–“è·é›¢ (1: éš£æ¥å±¤, 2: 2å±¤é›¢ã‚Œ, etc.)
        
        Returns:
            float: æ‹¡æ•£ä¿‚æ•° (u1^layer_distance)
        """
        # ed_multi.prompt.mdæº–æ‹ : è·é›¢ã«å¿œã˜ã¦æ‹¡æ•£ä¿‚æ•°ã‚’æ¸›è¡°
        base_diffusion = 1.0  # u1åŸºæœ¬å€¤
        return base_diffusion ** layer_distance
    
    def get_network_summary(self):
        """ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã‚µãƒãƒªãƒ¼å–å¾—ï¼ˆed_multi.prompt.mdä»•æ§˜æº–æ‹ ï¼‰"""
        return {
            'input_size': self.input_size,
            'hidden_layers': self.hidden_layers,
            'output_size': self.output_size,
            'total_layers': self.total_layers,
            'all_units': self.all_units,  # ä¿®æ­£: total_units â†’ all_units
            'layer_type': 'å˜å±¤' if self.is_single_layer() else f'{len(self.hidden_layers)}å±¤',
            'excitatory_input_size': self.in_units,  # ä¿®æ­£: excitatory_input_size â†’ in_units
            'index_ranges': {
                'bias': (self.bias_start, self.bias_end),
                'input': (self.input_start, self.input_end),
                'hidden': (self.hidden_start, self.hidden_end),
                'output': self.output_pos  # ä¿®æ­£: å‡ºåŠ›ã¯å˜ä¸€ä½ç½®
            },
            'ed_multi_compliance': {
                'bias_indices': '0, 1',
                'input_indices': f'2 ï½ {self.input_end}',
                'output_index': f'{self.output_pos} (in+2)',
                'hidden_indices': f'{self.hidden_start} ï½ {self.hidden_end} (in+3 ï½ all+1)'
            }
        }

# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
class HyperParams:
    """
    EDæ³•ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç®¡ç†ã‚¯ãƒ©ã‚¹
    é‡‘å­å‹‡æ°ã‚ªãƒªã‚¸ãƒŠãƒ«ä»•æ§˜ã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’ä¿æŒã—ã€å®Ÿè¡Œæ™‚å¼•æ•°ã§ã®å¤‰æ›´ã‚’å¯èƒ½ã«ã™ã‚‹
    """
    
    def __init__(self):
        """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®šï¼ˆæœ€é©åŒ–ã•ã‚ŒãŸãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ä½¿ç”¨ï¼‰"""
        # EDæ³•é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆPhase 2æœ€é©åŒ–çµæœï¼‰
        self.learning_rate = 0.3      # å­¦ç¿’ç‡ (alpha) - Phase 2æœ€é©å€¤
        self.initial_amine = 0.7      # åˆæœŸã‚¢ãƒŸãƒ³æ¿ƒåº¦ (beta) - Phase 2æœ€é©å€¤
        self.diffusion_rate = 0.5     # ã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•° (u1) - Phase 1æœ€é©å€¤
        self.sigmoid_threshold = 0.7  # ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¾å€¤ (u0) - Phase 1æœ€é©å€¤
        self.initial_weight_1 = 0.3   # é‡ã¿åˆæœŸå€¤1 - Phase 1æœ€é©å€¤
        self.initial_weight_2 = 0.5   # é‡ã¿åˆæœŸå€¤2 - Phase 1æœ€é©å€¤
        
        # å®Ÿè¡Œæ™‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.train_samples = 100      # è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°
        self.test_samples = 100       # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°
        self.epochs = 5               # ã‚¨ãƒãƒƒã‚¯æ•°ï¼ˆåŠ¹ç‡æ€§æœ€é©å€¤ï¼‰
        self.hidden_layers = [128]    # éš ã‚Œå±¤æ§‹é€  (å˜å±¤äº’æ›: [128], å¤šå±¤ä¾‹: [256,128,64])
        self.batch_size = 32          # ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚ºï¼ˆæ–°æ©Ÿèƒ½ï¼šé‡‘å­å‹‡æ°ç†è«–æ‹¡å¼µï¼‰
        self.random_seed = None       # ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ï¼ˆNoneã¯ãƒ©ãƒ³ãƒ€ãƒ ï¼‰
        self.enable_visualization = False  # ç²¾åº¦/èª¤å·®å¯è¦–åŒ–
        self.verbose = False          # è©³ç´°è¡¨ç¤º
        self.quiet_mode = False       # ç°¡æ½”å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰ï¼ˆã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒç”¨ï¼‰
        self.force_cpu = False        # CPUå¼·åˆ¶å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰
        self.fashion_mnist = False    # Fashion-MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨
        self.save_fig = None          # å›³è¡¨ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª (None: ç„¡åŠ¹, str: ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæŒ‡å®š)
    
    def parse_args(self, args=None):
        """
        argparseã«ã‚ˆã‚‹ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£æ
        ed_genuine.prompt.mdæº–æ‹ : ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã®å®Œå…¨æ€§ã‚’ä¿æŒ
        """
        parser = argparse.ArgumentParser(
            description='ç´”æ­£EDæ³•ï¼ˆError Diffusion Learning Algorithmï¼‰å®Ÿè¡Œ v0.1.8',
            formatter_class=argparse.RawDescriptionHelpFormatter,
            epilog="""
                    EDæ³•ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¬æ˜:
                        å­¦ç¿’ç‡(alpha): ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã®å­¦ç¿’å¼·åº¦ã‚’åˆ¶å¾¡
                        ã‚¢ãƒŸãƒ³æ¿ƒåº¦(beta): åˆæœŸèª¤å·®ä¿¡å·ã®å¼·åº¦
                        æ‹¡æ•£ä¿‚æ•°(u1): ã‚¢ãƒŸãƒ³ï¼ˆèª¤å·®ä¿¡å·ï¼‰ã®æ‹¡æ•£ç‡
                        ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¾å€¤(u0): æ´»æ€§åŒ–é–¢æ•°ã®æ„Ÿåº¦
  
                    Original Algorithm: é‡‘å­å‹‡ (1999)
                    Implementation: Python with ed_genuine.prompt.md compliance
                """
        )
        
        # EDæ³•é–¢é€£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¾¤ï¼ˆæ©Ÿèƒ½é †é…ç½®ï¼‰
        ed_group = parser.add_argument_group('EDæ³•ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿')
        ed_group.add_argument('--learning_rate', '--lr', type=float, default=self.learning_rate,
                             help=f'å­¦ç¿’ç‡ alpha (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.learning_rate})')
        ed_group.add_argument('--amine', '--ami', type=float, default=self.initial_amine,
                             help=f'åˆæœŸã‚¢ãƒŸãƒ³æ¿ƒåº¦ beta (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.initial_amine})')
        ed_group.add_argument('--diffusion', '--dif', type=float, default=self.diffusion_rate,
                             help=f'ã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•° u1 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.diffusion_rate})')
        ed_group.add_argument('--sigmoid', '--sig', type=float, default=self.sigmoid_threshold,
                             help=f'ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¾å€¤ u0 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.sigmoid_threshold})')
        ed_group.add_argument('--weight1', '--w1', type=float, default=self.initial_weight_1,
                             help=f'é‡ã¿åˆæœŸå€¤1 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.initial_weight_1})')
        ed_group.add_argument('--weight2', '--w2', type=float, default=self.initial_weight_2,
                             help=f'é‡ã¿åˆæœŸå€¤2 (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.initial_weight_2})')
        
        # å®Ÿè¡Œæ™‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¾¤ï¼ˆæ©Ÿèƒ½é †é…ç½®ï¼‰
        exec_group = parser.add_argument_group('å®Ÿè¡Œæ™‚è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿')
        exec_group.add_argument('--train_samples', '--train', type=int, default=self.train_samples,
                               help=f'è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.train_samples})')
        exec_group.add_argument('--test_samples', '--test', type=int, default=self.test_samples,
                               help=f'ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.test_samples})')
        exec_group.add_argument('--epochs', '--epo', type=int, default=self.epochs,
                               help=f'ã‚¨ãƒãƒƒã‚¯æ•° (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.epochs})')
        exec_group.add_argument('--hidden', '--hid', type=str, default=','.join(map(str, self.hidden_layers)),
                               help=f'éš ã‚Œå±¤æ§‹é€  (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {",".join(map(str, self.hidden_layers))}) - ã‚«ãƒ³ãƒåŒºåˆ‡ã‚ŠæŒ‡å®š (ä¾‹: 256,128,64)')
        exec_group.add_argument('--batch_size', '--batch', type=int, default=self.batch_size,
                               help=f'ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: {self.batch_size}) - é‡‘å­å‹‡æ°ç†è«–æ‹¡å¼µ')
        exec_group.add_argument('--seed', type=int, default=self.random_seed,
                               help=f'ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ãƒ©ãƒ³ãƒ€ãƒ )')
        exec_group.add_argument('--viz', action='store_true', default=self.enable_visualization,
                               help='ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–ã‚’æœ‰åŠ¹åŒ– (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡åŠ¹)')
        exec_group.add_argument('--verbose', '--v', action='store_true', default=self.verbose,
                               help='è©³ç´°è¡¨ç¤ºã‚’æœ‰åŠ¹åŒ– (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡åŠ¹)')
        exec_group.add_argument('--quiet', '--q', action='store_true', default=False,
                               help='ç°¡æ½”å‡ºåŠ›ãƒ¢ãƒ¼ãƒ‰ - ã‚°ãƒªãƒƒãƒ‰ã‚µãƒ¼ãƒç”¨ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡åŠ¹)')
        exec_group.add_argument('--cpu', action='store_true', default=self.force_cpu,
                               help='CPUå¼·åˆ¶å®Ÿè¡Œãƒ¢ãƒ¼ãƒ‰ (GPUç„¡åŠ¹åŒ–ã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ç„¡åŠ¹)')
        exec_group.add_argument('--fashion', action='store_true', default=False,
                               help='Fashion-MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½¿ç”¨ (ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: é€šå¸¸MNIST)')
        exec_group.add_argument('--save_fig', nargs='?', const='images', default=None,
                               help='å›³è¡¨ä¿å­˜ã‚’æœ‰åŠ¹åŒ– (å¼•æ•°ãªã—: ./images, å¼•æ•°ã‚ã‚Š: æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª)')
        
        # å¼•æ•°è§£æ
        parsed_args = parser.parse_args(args)
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å€¤ã®æ›´æ–°
        self.learning_rate = parsed_args.learning_rate
        self.initial_amine = parsed_args.amine
        self.diffusion_rate = parsed_args.diffusion
        self.sigmoid_threshold = parsed_args.sigmoid
        self.initial_weight_1 = parsed_args.weight1
        self.initial_weight_2 = parsed_args.weight2
        
        self.train_samples = parsed_args.train_samples
        self.test_samples = parsed_args.test_samples
        self.epochs = parsed_args.epochs
        
        # éš ã‚Œå±¤æ§‹é€ ã®è§£æï¼ˆã‚«ãƒ³ãƒåŒºåˆ‡ã‚Šæ–‡å­—åˆ—ã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›ï¼‰
        if isinstance(parsed_args.hidden, str):
            try:
                self.hidden_layers = [int(x.strip()) for x in parsed_args.hidden.split(',') if x.strip()]
                if not self.hidden_layers:
                    raise ValueError("éš ã‚Œå±¤æ§‹é€ ãŒç©ºã§ã™")
                # å…¨ã¦ã®å€¤ãŒæ­£ã®æ•´æ•°ã§ã‚ã‚‹ã“ã¨ã‚’ç¢ºèª
                if any(layer <= 0 for layer in self.hidden_layers):
                    raise ValueError("éš ã‚Œå±¤ã®ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ã¯æ­£ã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
            except ValueError as e:
                raise ValueError(f"--hidden ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã®å½¢å¼ãŒä¸æ­£ã§ã™: {e}")
        else:
            # å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®å‡¦ç†ï¼ˆintã§æŒ‡å®šã•ã‚ŒãŸå ´åˆï¼‰
            self.hidden_layers = [parsed_args.hidden]
            
        self.batch_size = parsed_args.batch_size
        self.random_seed = parsed_args.seed
        self.enable_visualization = parsed_args.viz
        self.verbose = parsed_args.verbose
        self.quiet_mode = parsed_args.quiet
        self.force_cpu = parsed_args.cpu
        self.fashion_mnist = parsed_args.fashion
        self.save_fig = getattr(parsed_args, 'save_fig', None)
        
        return parsed_args
    
    def set_random_seed(self):
        """
        ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®šï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰
        """
        if self.random_seed is not None:
            np.random.seed(self.random_seed)
            random.seed(self.random_seed)
            # NOTE: mathãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯ã‚·ãƒ¼ãƒ‰è¨­å®šã‚’ã‚µãƒãƒ¼ãƒˆã—ã¦ã„ãªã„
            if self.verbose:
                print(f"ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®š: {self.random_seed}")
        else:
            if self.verbose:
                print("ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰: æœªè¨­å®šï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰")
    
    def validate_params(self):
        """
        ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼ï¼ˆed_genuine.prompt.mdæº–æ‹ ï¼‰
        ç”Ÿç‰©å­¦çš„åˆ¶ç´„ã¨ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ åˆ¶ç´„ã®ãƒã‚§ãƒƒã‚¯
        """
        errors = []
        
        # EDæ³•ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶ç´„
        if self.learning_rate <= 0:
            errors.append("å­¦ç¿’ç‡ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        if self.initial_amine <= 0:
            errors.append("åˆæœŸã‚¢ãƒŸãƒ³æ¿ƒåº¦ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        if self.diffusion_rate <= 0:
            errors.append("ã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•°ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        if self.sigmoid_threshold <= 0:
            errors.append("ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¾å€¤ã¯æ­£ã®å€¤ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        
        # å®Ÿè¡Œæ™‚ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ¶ç´„
        if self.train_samples <= 0:
            errors.append("è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°ã¯æ­£ã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        if self.test_samples <= 0:
            errors.append("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°ã¯æ­£ã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        if self.epochs <= 0:
            errors.append("ã‚¨ãƒãƒƒã‚¯æ•°ã¯æ­£ã®æ•´æ•°ã§ã‚ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™")
        # éš ã‚Œå±¤æ§‹é€ ã®æ¤œè¨¼ã¯æ—¢ã«parse_argså†…ã§å®Ÿè¡Œæ¸ˆã¿
            
        # å®Ÿç”¨çš„åˆ¶ç´„ï¼ˆãƒ¡ãƒ¢ãƒªãƒ»è¨ˆç®—é‡ï¼‰
        if self.train_samples > 10000:
            errors.append("è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°ã¯10000ä»¥ä¸‹ã‚’æ¨å¥¨ã—ã¾ã™")
        if self.test_samples > 10000:
            errors.append("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°ã¯10000ä»¥ä¸‹ã‚’æ¨å¥¨ã—ã¾ã™")
        # éš ã‚Œå±¤ã®æœ€å¤§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ãƒã‚§ãƒƒã‚¯
        if max(self.hidden_layers) > 1000:
            errors.append(f"éš ã‚Œå±¤ã®æœ€å¤§ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³æ•°ï¼ˆ{max(self.hidden_layers)}ï¼‰ã¯1000ä»¥ä¸‹ã‚’æ¨å¥¨ã—ã¾ã™")
        
        # å¯è¦–åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
        if self.enable_visualization and self.epochs < 3:
            print("âš ï¸ --vizã‚ªãƒ—ã‚·ãƒ§ãƒ³ã¯3ã‚¨ãƒãƒƒã‚¯ä»¥ä¸Šã§ãªã„ã¨ä½¿ç”¨ã§ãã¾ã›ã‚“ã€‚")
            print("   å¯è¦–åŒ–ã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ç„¡åŠ¹ã«ã—ã¦å®Ÿè¡Œã‚’ç¶™ç¶šã—ã¾ã™ã€‚")
            self.enable_visualization = False
            
        if errors:
            raise ValueError("ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼:\n" + "\n".join(f"  - {error}" for error in errors))
    
    def display_config(self):
        """è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã®è¡¨ç¤º"""
        print("=" * 60)
        print("EDæ³•å®Ÿè¡Œè¨­å®š")
        print("=" * 60)
        print("ã€EDæ³•ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
        print(f"  å­¦ç¿’ç‡ (alpha):         {self.learning_rate:.3f}")
        print(f"  åˆæœŸã‚¢ãƒŸãƒ³æ¿ƒåº¦ (beta):  {self.initial_amine:.3f}")
        print(f"  ã‚¢ãƒŸãƒ³æ‹¡æ•£ä¿‚æ•° (u1):    {self.diffusion_rate:.3f}")
        print(f"  ã‚·ã‚°ãƒ¢ã‚¤ãƒ‰é–¾å€¤ (u0):    {self.sigmoid_threshold:.3f}")
        print(f"  é‡ã¿åˆæœŸå€¤1:            {self.initial_weight_1:.3f}")
        print(f"  é‡ã¿åˆæœŸå€¤2:            {self.initial_weight_2:.3f}")
        print()
        print("ã€å®Ÿè¡Œæ™‚è¨­å®šãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã€‘")
        print(f"  ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ:           {'Fashion-MNIST' if self.fashion_mnist else 'MNIST'}")
        print(f"  è¨“ç·´ãƒ‡ãƒ¼ã‚¿æ•°:           {self.train_samples}")
        print(f"  ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æ•°:         {self.test_samples}")
        print(f"  ã‚¨ãƒãƒƒã‚¯æ•°:             {self.epochs}")
        
        # éš ã‚Œå±¤æ§‹é€ ã®è¡¨ç¤ºï¼ˆå˜å±¤ãƒ»å¤šå±¤ã«å¯¾å¿œï¼‰
        layer_structure = " â†’ ".join(map(str, self.hidden_layers))
        layer_type = "å˜å±¤" if len(self.hidden_layers) == 1 else f"{len(self.hidden_layers)}å±¤"
        print(f"  éš ã‚Œå±¤æ§‹é€ :             {layer_structure} ({layer_type})")
        
        print(f"  ãƒŸãƒ‹ãƒãƒƒãƒã‚µã‚¤ã‚º:       {self.batch_size} {'(é€æ¬¡å‡¦ç†)' if self.batch_size == 1 else '(ãƒŸãƒ‹ãƒãƒƒãƒ)'}")
        print(f"  ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ å¯è¦–åŒ–:     {'ON' if self.enable_visualization else 'OFF'}")
        print(f"  è©³ç´°è¡¨ç¤º:               {'ON' if self.verbose else 'OFF'}")
        print(f"  å›³è¡¨ä¿å­˜:               {'ON -> ' + self.save_fig if self.save_fig else 'OFF'}")
        print("=" * 60)

# === EDæ³•åˆ†é¡å®Ÿè¡Œ ===
def run_classification(hyperparams=None):
    """MNISTåˆ†é¡å®Ÿè¡Œé–¢æ•° - æœ€å°å®Ÿè£…ç‰ˆ"""
    if hyperparams is None:
        hyperparams = HyperParams()
    
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®š
    hyperparams.set_random_seed()
    
    # ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ä½œæˆãƒ»å­¦ç¿’å®Ÿè¡Œ
    network = EDNetworkMNIST(hyperparams)
    results = network.run_classification(random_state=42)
    
    print(f"\næœ€çµ‚çµæœ: ç²¾åº¦ {results['final_accuracy']/100:.3f} ({results['final_accuracy']:.1f}%)")
    return results


def run_multilayer_classification(hyperparams, network_structure):
    """
    å¤šå±¤å¯¾å¿œå­¦ç¿’å®Ÿè¡Œï¼ˆed_multi.prompt.mdæº–æ‹  + é«˜é€ŸåŒ–çµ±ä¸€å®Ÿè£…ï¼‰
    
    ã€2025å¹´9æœˆ14æ—¥çµ±ä¸€é«˜é€ŸåŒ–å®Ÿè£…ã€‘
    ğŸš€ å˜å±¤ã¨åŒã˜EDNetworkMNIST.run_classification()ã‚’ä½¿ç”¨
    ğŸš€ é«˜é€ŸåŒ–å®Ÿè£…ã‚’å¤šå±¤ã§ã‚‚é©ç”¨ã—ã¦å­¦ç¿’æ™‚é–“ã‚’çµ±ä¸€
    ğŸš€ ed_multi.prompt.mdæº–æ‹ : Cå®Ÿè£…ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã®æ•´åˆæ€§ç¶­æŒ
    
    Args:
        hyperparams: HyperParamsã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
        network_structure: NetworkStructureã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
    Returns:
        dict: å­¦ç¿’çµæœ
    """
    # ãƒ©ãƒ³ãƒ€ãƒ ã‚·ãƒ¼ãƒ‰è¨­å®šï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰
    hyperparams.set_random_seed()
    
    print(f"ğŸ”§ å¤šå±¤EDæ³•çµ±ä¸€å®Ÿè£…é–‹å§‹: {network_structure.get_network_summary()}")
    print(f"ğŸ¯ å˜å±¤ã¨åŒã˜é«˜é€ŸåŒ–å®Ÿè£…ã‚’ä½¿ç”¨: EDNetworkMNIST.run_classification()")
    print(f"ğŸ“Š å¤šå±¤æ§‹é€ : å…¥åŠ›{network_structure.input_size} â†’ {'â†’'.join(map(str, network_structure.hidden_layers))} â†’ å‡ºåŠ›{network_structure.output_size}")
    
    # ğŸš€ çµ±ä¸€é«˜é€ŸåŒ–: å˜å±¤ã¨åŒã˜EDNetworkMNISTã‚’ä½¿ç”¨
    network = EDNetworkMNIST(hyperparams)
    
    print(f"âœ… å¤šå±¤ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯çµ±ä¸€å®Ÿè£…æº–å‚™å®Œäº†")
    print(f"   ğŸš€ é«˜é€ŸåŒ–å®Ÿè£…é©ç”¨: train_epoch_with_buffer/minibatchä½¿ç”¨")
    print(f"   ğŸ“Š æ§‹é€ : {network_structure.get_network_summary()}")
    
    # ğŸš€ é«˜é€ŸåŒ–ã•ã‚ŒãŸåˆ†é¡å®Ÿè¡Œï¼ˆå˜å±¤ã¨åŒã˜å®Ÿè£…ï¼‰
    results = network.run_classification(random_state=42)
    
    # é‡ã¿ä¿å­˜ã®ãŸã‚ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’çµæœã«è¿½åŠ 
    results['network_instance'] = network
    
    # çµæœã«å¤šå±¤æƒ…å ±ã‚’è¿½åŠ 
    results['multilayer_mode'] = True
    results['truly_multilayer'] = True
    results['algorithm_used'] = 'EDNetworkMNIST_unified'
    results['ed_multi_compliance'] = True
    results['network_structure'] = network_structure.get_network_summary()
    results['performance_unified'] = True  # å˜å±¤ã¨åŒã˜æ€§èƒ½
    
    print(f"âœ… å¤šå±¤å­¦ç¿’çµ±ä¸€å®Ÿè£…å®Œäº†")
    print(f"ğŸ“Š æœ€çµ‚ç²¾åº¦: {results.get('final_accuracy', 0)/100:.3f} ({results.get('final_accuracy', 0):.1f}%) (çµ±ä¸€é«˜é€ŸåŒ–ç‰ˆ)")
    print(f"ğŸ“ˆ æœ€é«˜ç²¾åº¦: {results.get('peak_accuracy', 0)/100:.3f} ({results.get('peak_accuracy', 0):.1f}%) (çµ±ä¸€é«˜é€ŸåŒ–ç‰ˆ)")
    print(f"ğŸš€ ä½¿ç”¨å®Ÿè£…: EDNetworkMNISTï¼ˆå˜å±¤ã¨åŒã˜é«˜é€ŸåŒ–ï¼‰")
    
    return results




def main():
    """
    ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•° - MNIST/Fashion-MNISTåˆ†é¡å°‚ç”¨ç‰ˆ
    
    ã€v0.1.8å®Ÿè¡Œä»•æ§˜ã€‘
    - MNIST/Fashion-MNISTãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå¯¾å¿œ
    - 28Ã—28ç”»åƒãƒ‘ã‚¿ãƒ¼ãƒ³ï¼ˆ784æ¬¡å…ƒï¼‰ã€10ã‚¯ãƒ©ã‚¹åˆ†é¡
    - ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³åˆ¶å¾¡å¯¾å¿œ
    - æ··åŒè¡Œåˆ—å¯è¦–åŒ–æ©Ÿèƒ½å®Œå…¨å¯¾å¿œ
    - ä»Šå¾Œã®é–‹ç™ºãƒ™ãƒ¼ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«ã¨ã—ã¦æœ€é©åŒ–
    
    ã€ed_genuine.prompt.mdæº–æ‹ å®Ÿè£…ã€‘
    - ç‹¬ç«‹å‡ºåŠ›ãƒ‹ãƒ¥ãƒ¼ãƒ­ãƒ³ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¿æŒ
    - ã‚¢ãƒŸãƒ³æ‹¡æ•£å­¦ç¿’åˆ¶å¾¡ç¶™æ‰¿
    - é‡‘å­å‹‡æ°ã‚ªãƒªã‚¸ãƒŠãƒ«ä»•æ§˜å®Œå…¨æº–æ‹ 
    """
    pass  # ãƒ¡ã‚¤ãƒ³ãƒ­ã‚¸ãƒƒã‚¯ã¯if __name__ == "__main__"ã§å®Ÿè¡Œ


if __name__ == "__main__":
    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿è§£æ
    hyperparams = HyperParams()
    
    try:
        # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°è§£æ
        args = hyperparams.parse_args()
        
        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿å¦¥å½“æ€§æ¤œè¨¼
        hyperparams.validate_params()
        
        # è¨­å®šè¡¨ç¤º
        if not hyperparams.quiet_mode:
            hyperparams.display_config()
        
        # ã‚·ãƒ³ãƒ—ãƒ«ãªå­¦ç¿’å®Ÿè¡Œ
        results = run_classification(hyperparams)
        
        # å®Ÿè¡Œçµæœè¡¨ç¤º
        if results and hyperparams.verbose:
            print(f"âœ… å­¦ç¿’å®Œäº†: ç²¾åº¦ {results.get('test_accuracy', 0):.1f}%")
            
    except ValueError as e:
        print(f"âŒ ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚¨ãƒ©ãƒ¼: {e}")
        exit(1)
    except KeyboardInterrupt:
        print("\nâš ï¸ å®Ÿè¡ŒãŒä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        exit(0)


